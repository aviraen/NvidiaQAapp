content,urls,titles
,https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html,release notes
release notes 1 cuda  update 1 release notes v125 pdf archive nvidia cuda toolkit release notes the release notes for the cuda toolkit 1 cuda  update 1 release notes the release notes for the nvidia cuda toolkit can be found online at httpsdocsnvidiacomcudacudatoolkitreleasenotesindexhtml  note the release notes have been reorganized into two major sections the general cuda release notes and the cuda libraries release notes including historical information for 12x releases  cuda toolkit major component versions cuda components starting with cuda 11 the various components in the toolkit are versioned independently for cuda  update 1 the table below indicates the versions table 1 cuda  update 1 component versions component name version information supported architectures supported platforms cuda c core compute libraries thrust 0 x8664 arm64sbsa aarch64jetson linux windows cub 0 libcu 0 cooperative groups 82 cuda compatibility 36505571 aarch64jetson linux cuda runtime cudart 82 x8664 arm64sbsa aarch64jetson linux windows wsl cuobjdump 39 x8664 arm64sbsa aarch64jetson linux windows cupti 82 x8664 arm64sbsa aarch64jetson linux windows wsl cuda cuxxfilt demangler 82 x8664 arm64sbsa aarch64jetson linux windows cuda demo suite 82 x8664 linux windows cuda gdb 82 x8664 arm64sbsa aarch64jetson linux wsl cuda nsight eclipse plugin 82 x8664 linux cuda nvcc 82 x8664 arm64sbsa aarch64jetson linux windows wsl cuda nvdisasm 39 x8664 arm64sbsa aarch64jetson linux windows cuda nvml headers 82 x8664 arm64sbsa aarch64jetson linux windows wsl cuda nvprof 82 x8664 linux windows cuda nvprune 82 x8664 arm64sbsa aarch64jetson linux windows wsl cuda nvrtc 82 x8664 arm64sbsa aarch64jetson linux windows wsl nvtx 82 x8664 arm64sbsa aarch64jetson linux windows wsl cuda nvvp 82 x8664 linux windows cuda opencl 39 x8664 linux windows cuda profiler api 39 x8664 arm64sbsa aarch64jetson linux windows wsl cuda compute sanitizer api 81 x8664 arm64sbsa aarch64jetson linux windows wsl cuda cublas  x8664 arm64sbsa aarch64jetson linux windows wsl cudla 82 aarch64jetson linux cuda cufft  x8664 arm64sbsa aarch64jetson linux windows wsl cuda cufile  x8664 arm64sbsa aarch64jetson linux cuda curand  x8664 arm64sbsa aarch64jetson linux windows wsl cuda cusolver  x8664 arm64sbsa aarch64jetson linux windows wsl cuda cusparse  x8664 arm64sbsa aarch64jetson linux windows wsl cuda npp  x8664 arm64sbsa aarch64jetson linux windows wsl cuda nvfatbin 82 x8664 arm64sbsa aarch64jetson linux windows wsl cuda nvjitlink 82 x8664 arm64sbsa aarch64jetson linux windows wsl cuda nvjpeg  x8664 arm64sbsa aarch64jetson linux windows wsl nsight compute  x8664 arm64sbsa aarch64jetson linux windows wsl windows 11 nsight systems  x8664 arm64sbsa linux windows wsl nsight visual studio edition vse  x8664 windows windows nvidiafs 1 6 x8664 arm64sbsa aarch64jetson linux visual studio integration 82 x8664 windows windows nvidia linux driver 06 x8664 arm64sbsa linux nvidia windows driver  x8664 windows windows wsl cuda driver running a cuda application requires the system with at least one cuda capable gpu and a driver that is compatible with the cuda toolkit see table 3  for more information various gpu products that are cuda capable visit httpsdevelopernvidiacomcudagpus  each release of the cuda toolkit requires a minimum version of the cuda driver the cuda driver is backward compatible meaning that applications compiled against a particular version of the cuda will continue to work on subsequent later driver releases more information on compatibility can be found at httpsdocsnvidiacomcudacudacbestpracticesguideindexhtmlcudacompatibilityandupgrades  note starting with cuda  the toolkit components are individually versioned and the toolkit itself is versioned as shown in the table below the minimum required driver version for cuda minor version compatibility is shown below cuda minor version compatibility is described in detail in httpsdocsnvidiacomdeploycudacompatibilityindexhtml table 2 cuda toolkit and minimum required driver version for cuda minor version compatibility cuda toolkit minimum required driver version for cuda minor version compatibility linux x8664 driver version windows x8664 driver version cuda 12x 13  cuda x cuda x cuda x cuda x cuda x cuda x cuda x cuda x 02  cuda  3 06  using a minimum required version that is different from toolkit driver version could be allowed in compatibility mode please read the cuda compatibility guide for details cuda  was released with an earlier driver version but by upgrading to tesla recommended drivers 02 linux  windows minor version compatibility is possible across the cuda 11x family of toolkits the version of the development nvidia gpu driver packaged in each cuda toolkit release is shown below table 3 cuda toolkit and corresponding driver versions cuda toolkit toolkit driver version linux x8664 driver version windows x8664 driver version cuda  update 1 06  cuda  ga 02  cuda  update 1 15  cuda  ga 14  cuda  update 1 08  cuda  ga 06  cuda  update 2 05  cuda  update 1 09  cuda  ga 03  cuda  update 1 02  cuda  ga 02  cuda  update 1 12  cuda  ga 13  cuda  ga 05  cuda  update 1 07  cuda  ga 04  cuda  update 2 03  cuda  update 1 03  cuda  ga 01  cuda  update 2 05  cuda  update 1 05  cuda  ga 05  cuda  update 4 01  cuda  update 3 01  cuda  update 2 02  cuda  update 1 02  cuda 0 ga 01  cuda 1 update 1 01  cuda 0 ga 01  cuda 2 update 2 03  cuda 1 update 1 03  cuda 0 ga 03  cuda 1 update 1   cuda  ga   cuda 3 update 1 06  cuda 2 ga 05  cuda 1 rc 06  cuda 89   cuda  105 general release and updates   cuda 130   cuda  148 update 1   cuda  88   cuda  85   cuda  76   cuda  61 ga2   cuda  44   cuda  16   cuda,https://docs.nvidia.com/cuda/cuda-features-archive/index.html,cuda features archive
cuda features archive 1 cuda  features v125 pdf archive nvidia cuda features archive the list of cuda features by release 1 cuda  features  compiler 1 vs2022 support cuda  officially supports the latest vs2022 as host compiler a separate nsight visual studio installer 1 must be downloaded from here  a future cuda release will have the nsight visual studio installer with vs2022 support integrated into it 2 new instructions in public ptx new instructions for bit mask creationbmsk and sign extensionszext are added to the public ptx isa you can find documentation for these instructions in the ptx isa guide bmsk and szext  3 unused kernel optimization in cuda  unused kernel pruning was introduced with the potential benefits of reducing binary size and improving performance through more efficient optimizations this was an optin feature but in  this feature is enabled by default as mentioned in the  blog there is an optout flag that can be used in case it becomes necessary for debug purposes or for other special situations nvcc rdctrue usercu testliba o user xnvlink ignorehostinfo 4 new archnative option in addition to the archall and archallmajor options added in cuda  nvcc introduced arch native in cuda  update 1 this archnative option is a convenient way for users to let nvcc determine the right target architecture to compile the cuda device code to based on the gpu installed on the system this can be particularly helpful for testing when applications are run on the same system they are compiled in 5 generate ptx from nvlink using the following command line device linker nvlink will produce ptx as an output in addition to cubin nvcc dlto dlink ptx device linking by nvlink is the final stage in the cuda compilation process applications that have multiple source translation units have to be compiled in separate compilation mode lto introduced in cuda  allowed nvlink to perform optimizations at device link time instead of at compile time so that separately compiled applications with several translation units can be optimized to the same level as whole program compilations with a single translation unit however without the option to output ptx applications that cared about forward compatibility of device code could not benefit from link time optimization or had to constrain the device code to a single source file with the option for nvlink that performs lto to generate the output in ptx customer applications that require forward compatibility across gpu architectures can span across multiple files and can also take advantage of link time optimization 6 bullseye support nvcc compiled source code now works with the code coverage tool bullseye the code coverage is only for the cpu or the host functions code coverage for device function is not supported through bullseye 7 int128 developer tool support in  cuda c support for 128 bit was added in  developer tools support the datatype as well with the latest version of libcu int 128 data datype is supported by math functions 2 notices  notice this document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality condition or quality of a product nvidia corporation nvidia makes no representations or warranties expressed or implied as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein nvidia shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use this document is not a commitment to develop release or deliver any material defined below code or functionality nvidia reserves the right to make corrections modifications enhancements improvements and any other changes to this document at any time without notice customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete nvidia products are sold subject to the nvidia standard terms and conditions of sale supplied at the time of order acknowledgement unless otherwise agreed in an individual sales agreement signed by authorized representatives of nvidia and customer terms of sale nvidia hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the nvidia product referenced in this document no contractual obligations are formed either directly or indirectly by this document nvidia products are not designed authorized or warranted to be suitable for use in medical military aircraft space or life support equipment nor in applications where failure or malfunction of the nvidia product can reasonably be expected to result in personal injury death or property or environmental damage nvidia accepts no liability for inclusion andor use of nvidia products in such equipment or applications and therefore such inclusion andor use is at customers own risk nvidia makes no representation or warranty that products based on this document will be suitable for any specified use testing of all parameters of each product is not necessarily performed by nvidia it is customers sole responsibility to evaluate and determine the applicability of any information contained in this document ensure the product is suitable and fit for the application planned by customer and perform the necessary testing for the application in order to avoid a default of the application or the product weaknesses in customers product designs may affect the quality and reliability of the nvidia product and may result in additional or different conditions andor requirements beyond those contained in this document nvidia accepts no liability related to any default damage costs or problem which may be based on or attributable to i the use of the nvidia product in any manner that is contrary to this document or ii customer product designs no license either expressed or implied is granted under any nvidia patent right copyright or other nvidia intellectual property right under this document information published by nvidia regarding thirdparty products or,https://docs.nvidia.com/cuda/eula/index.html,eula
eula 1 license agreement for nvidia software development kits v125 pdf archive end user license agreement nvidia software license agreement and cuda supplement to software license agreement last updated october 8 2021 the cuda toolkit end user license agreement applies to the nvidia cuda toolkit the nvidia cuda samples the nvidia display driver nvidia nsight tools visual studio edition and the associated documentation on cuda apis programming model and development tools if you do not agree with the terms and conditions of the license agreement then do not download or use the software last updated october 8 2021 preface the software license agreement in chapter 1 and the supplement in chapter 2 contain license terms and conditions that govern the use of nvidia cuda toolkit by accepting this agreement you agree to comply with all the terms and conditions applicable to the products included herein nvidia driver description this package contains the operating system driver and fundamental system software components for nvidia gpus nvidia cuda toolkit description the nvidia cuda toolkit provides commandline and graphical tools for building debugging and optimizing the performance of applications accelerated by nvidia gpus runtime and math libraries and documentation including programming guides user manuals and api references default install location of cuda toolkit windows platform programfilesnvidia gpu computing toolkitcudav linux platform usrlocalcuda mac platform developernvidiacuda nvidia cuda samples description cuda samples are now located in httpsgithubcomnvidiacudasamples  which includes instructions for obtaining building and running the samples they are no longer included in the cuda toolkit nvidia nsight visual studio edition windows only description nvidia nsight development platform visual studio edition is a development environment integrated into microsoft visual studio that provides tools for debugging profiling analyzing and optimizing your gpu computing and graphics applications default install location of nsight visual studio edition windows platform programfilesx86nvidia corporationnsight visual studio edition  1 license agreement for nvidia software development kits important noticeread before downloading installing copying or using the licensed software this license agreement including exhibits attached agreement is a legal agreement between you and nvidia corporation nvidia and governs your use of a nvidia software development kit sdk each sdk has its own set of software and materials but here is a description of the types of items that may be included in a sdk source code header files apis data sets and assets examples include images textures models scenes videos native api inputoutput files binary software sample code libraries utility programs programming code and documentation this agreement can be accepted only by an adult of legal age of majority in the country in which the sdk is used if you are entering into this agreement on behalf of a company or other legal entity you represent that you have the legal authority to bind the entity to this agreement in which case you will mean the entity you represent if you dont have the required age or authority to accept this agreement or if you dont accept all the terms and conditions of this agreement do not download install or use the sdk you agree to use the sdk only for purposes that are permitted by a this agreement and b any applicable law regulation or generally accepted practices or guidelines in the relevant jurisdictions  license 1 license grant subject to the terms of this agreement nvidia hereby grants you a nonexclusive nontransferable license without the right to sublicense except as expressly provided in this agreement to install and use the sdk modify and create derivative works of sample source code delivered in the sdk and distribute those portions of the sdk that are identified in this agreement as distributable as incorporated in object code format into a software application that meets the distribution requirements indicated in this agreement 2 distribution requirements these are the distribution requirements for you to exercise the distribution grant your application must have material additional functionality beyond the included portions of the sdk the distributable portions of the sdk shall only be accessed by your application the following notice shall be included in modifications and derivative works of sample source code distributed this software contains source code provided by nvidia corporation unless a developer tool is identified in this agreement as distributable it is delivered for your internal use only the terms under which you distribute your application must be consistent with the terms of this agreement including without limitation terms relating to the license grant and license restrictions and protection of nvidias intellectual property rights additionally you agree that you will protect the privacy security and legal rights of your application users you agree to notify nvidia in writing of any known or suspected distribution or use of the sdk not in compliance with the requirements of this agreement and to enforce the terms of your agreements with respect to distributed sdk 3 authorized users you may allow employees and contractors of your entity or of your subsidiaryies to access and use the sdk from your secure network to perform work on your behalf if you are an academic institution you may allow users enrolled or employed by the academic institution to access and use the sdk from your secure network you are responsible for the compliance with the terms of this agreement by your authorized users if you become aware that your authorized users didnt follow the terms of this agreement you agree to take reasonable steps to resolve the noncompliance and prevent new occurrences 4 prerelease sdk the sdk versions identified as alpha beta preview or otherwise as prerelease may not be fully functional may contain errors or design flaws and may have reduced or different security privacy accessibility availability and reliability standards relative to commercial versions of nvidia software and materials use of a prerelease sdk may result in unexpected results loss of data project delays or other unpredictable damage or loss you may use a prerelease sdk at your own risk understanding that prerelease sdks are not intended for use in production,https://docs.nvidia.com/cuda/cuda-quick-start-guide/index.html,quick start guide
quick start guide 1 introduction v125 pdf archive cuda quick start guide minimal firststeps instructions to get cuda running on a standard system 1 introduction this guide covers the basic instructions needed to install cuda and verify that a cuda application can run on each supported platform these instructions are intended to be used on a clean installation of a supported platform for questions which are not answered in this document please refer to the windows installation guide and linux installation guide  the cuda installation packages can be found on the cuda downloads page  2 windows when installing cuda on windows you can choose between the network installer and the local installer the network installer allows you to download only the files you need the local installer is a standalone installer with a large initial download for more details refer to the windows installation guide   network installer perform the following steps to install cuda and verify the installation launch the downloaded installer package read and accept the eula select next to download and install all components once the download completes the installation will begin automatically once the installation completes click next to acknowledge the nsight visual studio edition installation summary click close to close the installer navigate to the samples nbody directory in httpsgithubcomnvidiacudasamplestreemastersamples5domainspecificnbody  open the nbody visual studio solution file for the version of visual studio you have installed for example nbodyvs2019sln  open the build menu within visual studio and click build solution  navigate to the cuda samples build directory and run the nbody sample note run samples by navigating to the executables location otherwise it will fail to locate dependent resources  local installer perform the following steps to install cuda and verify the installation launch the downloaded installer package read and accept the eula select next to install all components once the installation completes click next to acknowledge the nsight visual studio edition installation summary click close to close the installer navigate to the samples nbody directory in httpsgithubcomnvidiacudasamplestreemastersamples5domainspecificnbody  open the nbody visual studio solution file for the version of visual studio you have installed open the build menu within visual studio and click build solution  navigate to the cuda samples build directory and run the nbody sample note run samples by navigating to the executables location otherwise it will fail to locate dependent resources  pip wheels windows nvidia provides python wheels for installing cuda through pip primarily for using cuda with python these packages are intended for runtime use and do not currently include developer tools these can be installed separately please note that with this installation method cuda installation environment is managed via pip and additional care must be taken to set up your host environment to use cuda outside the pip environment prerequisites to install wheels you must first install the nvidiapyindex package which is required in order to set up your pip installation to fetch additional python modules from the nvidia ngc pypi repo if your pip and setuptools python modules are not uptodate then use the following command to upgrade these python modules if these python modules are outofdate then the commands which follow later in this section may fail py m pip install upgrade setuptools pip wheel you should now be able to install the nvidiapyindex module py m pip install nvidiapyindex if your project is using a requirementstxt file then you can add the following line to your requirementstxt file as an alternative to installing the nvidiapyindex package extraindexurl httpspypingcnvidiacom procedure install the cuda runtime package py m pip install nvidiacudaruntimecu12 optionally install additional packages as listed below using the following command py m pip install nvidialibrary metapackages the following metapackages will install the latest version of the named component on windows for the indicated cuda version cu12 should be read as cuda12 nvidiacudaruntimecu12 nvidiacudacupticu12 nvidiacudanvcccu12 nvidianvmldevcu12 nvidiacudanvrtccu12 nvidianvtxcu12 nvidiacudasanitizerapicu12 nvidiacublascu12 nvidiacufftcu12 nvidiacurandcu12 nvidiacusolvercu12 nvidiacusparsecu12 nvidianppcu12 nvidianvjpegcu12 these metapackages install the following packages nvidianvmldevcu125 nvidiacudanvcccu125 nvidiacudaruntimecu125 nvidiacudacupticu125 nvidiacublascu125 nvidiacudasanitizerapicu125 nvidianvtxcu125 nvidiacudanvrtccu125 nvidianppcu125 nvidiacusparsecu125 nvidiacusolvercu125 nvidiacurandcu125 nvidiacufftcu125 nvidianvjpegcu125  conda the conda packages are available at httpsanacondaorgnvidia  installation to perform a basic install of all cuda toolkit components using conda run the following command conda install cuda c nvidia uninstallation to uninstall the cuda toolkit using conda run the following command conda remove cuda 3 linux cuda on linux can be installed using an rpm debian runfile or conda package depending on the platform being installed on  linux x8664 for development on the x8664 architecture in some cases x8664 systems may act as host platforms targeting other architectures see the linux installation guide for more details 1 redhat centos when installing cuda on redhat or centos you can choose between the runfile installer and the rpm installer the runfile installer is only available as a local installer the rpm installer is available as both a local installer and a network installer the network installer allows you to download only the files you need the local installer is a standalone installer with a large initial download in the case of the rpm installers the instructions for the local and network variants are the same for more details refer to the linux installation guide   rpm installer perform the following steps to install cuda and verify the installation install epel to satisfy the dkms dependency by following the instructions at epels website  enable optional repos on rhel 8 linux only execute the following steps to enable optional repositories on x8664 workstation subscriptionmanager repos enablerhel8forx8664appstreamrpms subscriptionmanager repos enablerhel8forx8664baseosrpms subscriptionmanager repos enablecodereadybuilderforrhel8x8664rpms install the repository metadata clean the yum cache and install cuda sudo rpm install cudarepodistroversionarchitecturerpm sudo rpm erase gpgpubkey7fa2af80 sudo yum clean expirecache sudo yum install cuda reboot the system to load the nvidia drivers sudo reboot set up the development environment by modifying the path and ldlibrarypath variables export pathusrlocalcuda125binpathpath export ldlibrarypathusrlocalcuda125lib64 ldlibrarypathldlibrarypath install a writable copy of the samples from httpsgithubcomnvidiacudasamples  then build and run the,https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html,installation guide windows
installation guide windows 1 introduction v125 pdf archive cuda installation guide for microsoft windows the installation instructions for the cuda toolkit on microsoft windows systems 1 introduction cuda is a parallel computing platform and programming model invented by nvidia it enables dramatic increases in computing performance by harnessing the power of the graphics processing unit gpu cuda was developed with several design goals in mind provide a small set of extensions to standard programming languages like c that enable a straightforward implementation of parallel algorithms with cuda cc programmers can focus on the task of parallelization of the algorithms rather than spending time on their implementation support heterogeneous computation where applications use both the cpu and gpu serial portions of applications are run on the cpu and parallel portions are offloaded to the gpu as such cuda can be incrementally applied to existing applications the cpu and gpu are treated as separate devices that have their own memory spaces this configuration also allows simultaneous computation on the cpu and gpu without contention for memory resources cudacapable gpus have hundreds of cores that can collectively run thousands of computing threads these cores have shared resources including a register file and a shared memory the onchip shared memory allows parallel tasks running on these cores to share data without sending it over the system memory bus this guide will show you how to install and check the correct operation of the cuda development tools  system requirements to use cuda on your system you will need the following installed a cudacapable gpu a supported version of linux with a gcc compiler and toolchain nvidia cuda toolkit available at httpsdevelopernvidiacomcudadownloads supported microsoft windows operating systems microsoft windows 11 21h2 microsoft windows 11 22h2sv2 microsoft windows 11 23h2 microsoft windows 10 21h2 microsoft windows 10 22h2 microsoft windows server 2022 table 1 windows compiler support in cuda  compiler ide native x8664 crosscompilation 32bit on 64bit c dialect msvc version 193x visual studio 2022 17x yes not supported c14 default c17 c20 msvc version 192x visual studio 2019 16x yes c14 default c17 msvc version 191x visual studio 2017 15x rtw and all updates yes c14 default c17 support for visual studio 2015 is deprecated in release  support for visual studio 2017 is deprecated in release  32bit compilation native and crosscompilation is removed from cuda  and later toolkit use the cuda toolkit from earlier releases for 32bit compilation cuda driver will continue to support running 32bit application binaries on geforce gpus until ada ada will be the last architecture with driver support for 32bit applications hopper does not support 32bit applications support for running x86 32bit applications on x8664 windows is limited to use with cuda driver cuda runtime cudart cuda math library mathh  about this document this document is intended for readers familiar with microsoft windows operating systems and the microsoft visual studio environment you do not need previous experience with cuda or experience with parallel computation 2 installing cuda development tools basic instructions can be found in the quick start guide  read on for more detailed instructions the setup of cuda development tools on a system running the appropriate version of windows consists of a few simple steps verify the system has a cudacapable gpu download the nvidia cuda toolkit install the nvidia cuda toolkit test that the installed software runs correctly and communicates with the hardware  verify you have a cudacapable gpu you can verify that you have a cudacapable gpu through the display adapters section in the windows device manager  here you will find the vendor name and model of your graphics cards if you have an nvidia card that is listed in httpsdevelopernvidiacomcudagpus  that gpu is cudacapable the release notes for the cuda toolkit also contain a list of supported products the windows device manager can be opened via the following steps open a run window from the start menu run control name microsoftdevicemanager  download the nvidia cuda toolkit the nvidia cuda toolkit is available at httpsdevelopernvidiacomcudadownloads  choose the platform you are using and one of the following installer formats network installer a minimal installer which later downloads packages required for installation only the packages selected during the selection phase of the installer are downloaded this installer is useful for users who want to minimize download time full installer an installer which contains all the components of the cuda toolkit and does not require any further download this installer is useful for systems which lack network access and for enterprise deployment the cuda toolkit installs the cuda driver and tools needed to create build and run a cuda application as well as libraries header files and other resources download verification the download can be verified by comparing the md5 checksum posted at httpsdeveloperdownloadnvidiacomcomputecuda1251docssidebarmd5sumtxt with that of the downloaded file if either of the checksums differ the downloaded file is corrupt and needs to be downloaded again  install the cuda software before installing the toolkit you should read the release notes as they provide details on installation and software functionality note the driver and toolkit must be installed for cuda to function if you have not installed a standalone driver install the driver from the nvidia cuda toolkit note the installation may fail if windows update starts after the installation has begun wait until windows update is complete and then try the installation again graphical installation install the cuda software by executing the cuda installer and following the onscreen prompts silent installation the installer can be executed in silent mode by executing the package with the s flag additional parameters can be passed which will install specific subpackages instead of all packages see the table below for a list of all the subpackage names table 2 possible subpackage names subpackage name subpackage description toolkit subpackages defaults to cprogram filesnvidia gpu computing toolkitcudav125 cudaprofilerapi125 cuda profiler api cudart125 cuda runtime libraries cuobjdump125 extracts information from cubin files cupti125 the cuda,https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html,installation guide linux
installation guide for linux 1 introduction v125 pdf archive nvidia cuda installation guide for linux the installation instructions for the cuda toolkit on linux 1 introduction cuda is a parallel computing platform and programming model invented by nvidia  it enables dramatic increases in computing performance by harnessing the power of the graphics processing unit gpu cuda was developed with several design goals in mind provide a small set of extensions to standard programming languages like c that enable a straightforward implementation of parallel algorithms with cuda cc programmers can focus on the task of parallelization of the algorithms rather than spending time on their implementation support heterogeneous computation where applications use both the cpu and gpu serial portions of applications are run on the cpu and parallel portions are offloaded to the gpu as such cuda can be incrementally applied to existing applications the cpu and gpu are treated as separate devices that have their own memory spaces this configuration also allows simultaneous computation on the cpu and gpu without contention for memory resources cudacapable gpus have hundreds of cores that can collectively run thousands of computing threads these cores have shared resources including a register file and a shared memory the onchip shared memory allows parallel tasks running on these cores to share data without sending it over the system memory bus this guide will show you how to install and check the correct operation of the cuda development tools  system requirements to use nvidia cuda on your system you will need the following installed cudacapable gpu a supported version of linux with a gcc compiler and toolchain cuda toolkit available at httpsdevelopernvidiacomcudadownloads the cuda development environment relies on tight integration with the host development environment including the host compiler and c runtime libraries and is therefore only supported on distribution versions that have been qualified for this cuda toolkit release the following table lists the supported linux distributions please review the footnotes associated with the table table 1 native linux distribution support in cuda  update 1 distribution kernel 1 default gcc glibc x8664 rhel 9y y 4 0427 1  rhel 8y y 10 0553 0  opensuse leap 15y y 5 21150500 0  rocky linux 8y y10 0553 0  rocky linux 9y y4 0427 1  suse sles 15y y 5 21150500 0  ubuntu  lts 031 0  ubuntu z z 4 lts 027 0  ubuntu z z 6 lts 067 0  debian 12x x5 761 0  debian 11y y9 2092 1  debian 10z z13 021 0  fedora 39 6300 1  kylinos v10 sp2 v2101ky10 0  amazon linux 2023  1  arm64 sbsa rhel 9y y 4 0427 1  rhel 8y y 10 0553 0  suse sles 15y y 5 21150500 0  ubuntu  lts 031 0  ubuntu  lts z 5 lts 0102 0  ubuntu z z 5 lts 0174 0  arm64 sbsa jetson dgpu 06 lts rel35 jp 5x 192tegra 0  4 lts rel36 jp6x 136tegra 0  aarch64 jetson igpu l4t ubuntu  rel36 jp6x 80tegra 0 5 the following notes apply to the kernel versions supported by cuda for specific kernel versions supported on red hat enterprise linux rhel visit httpsaccessredhatcomarticles3078  a list of kernel versions including the release dates for suse linux enterprise server sles is available at httpswwwsusecomsupportkbdocid000019587  l4t provides a linux kernel and a sample root filesystem derived from ubuntu  for more details visit httpsdevelopernvidiacomembeddedjetsonlinux   os support policy cuda support for ubuntu x ubuntu x rhel 8x rhel 9x rocky linux 8x rocky linux 9x suse sles 15x and opensuse leap 15x will be until the standard eoss as defined for each os please refer to the support lifecycle for these oses to know their support timelines cuda supports the latest fedora release version for fedora release timelines visit httpsdocsfedoraprojectorgenusreleases  cuda supports a single kylinos release version for details visit httpswwwkylinoscn  refer to the support lifecycle for these supported oses to know their support timelines and plan to move to newer releases accordingly  host compiler support policy in order to compile the cpu host code in the cuda source the cuda compiler nvcc requires a compatible host compiler to be installed on the system the version of the host compiler supported on linux platforms is tabulated as below nvcc performs a version check on the host compilers major version and so newer minor versions of the compilers listed below will be supported but major versions falling outside the range will not be supported table 2 supported compilers distribution gcc clang nvhpc xlc armcc icc x8664 6x  7x   no no  arm64 sbsa 6x  7x   no 1 no for gcc and clang the preceding table indicates the minimum version and the latest version supported if you are on a linux distribution that may use an older version of gcc toolchain as default than what is listed above it is recommended to upgrade to a newer toolchain cuda  or later toolkit newer gcc toolchains are available with the red hat developer toolset for example for platforms that ship a compiler version older than gcc 6 by default linking to static or dynamic libraries that are shipped with the cuda toolkit is not supported we only support libstdc gccs implementation for all the supported host compilers for the platforms listed above 1 supported c dialects nvcc and nvrtc cuda runtime compiler support the following c dialect c11 c14 c17 c20 on supported host compilers the default c dialect of nvcc is determined by the default dialect of the host compiler used for compilation refer to host compiler documentation and the cuda programming guide for more details on language support c20 is supported with the following flavors of host compiler in both host and device code gcc clang nvhpc arm cc 10x 11x 22x 22x  about,https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html,programming guide
cuda c programming guide 1 introduction v125 pdf archive cuda c programming guide the programming guide to the cuda model and interface changes from version  added section asynchronous data copies using tensor memory access tma  added unified memory programming guide supporting grace hopper with address translation service ats and heterogeneous memory management hmm on x86 1 introduction  the benefits of using gpus the graphics processing unit gpu 1 provides much higher instruction throughput and memory bandwidth than the cpu within a similar price and power envelope many applications leverage these higher capabilities to run faster on the gpu than on the cpu see gpu applications  other computing devices like fpgas are also very energy efficient but offer much less programming flexibility than gpus this difference in capabilities between the gpu and the cpu exists because they are designed with different goals in mind while the cpu is designed to excel at executing a sequence of operations called a thread  as fast as possible and can execute a few tens of these threads in parallel the gpu is designed to excel at executing thousands of them in parallel amortizing the slower singlethread performance to achieve greater throughput the gpu is specialized for highly parallel computations and therefore designed such that more transistors are devoted to data processing rather than data caching and flow control the schematic figure 1 shows an example distribution of chip resources for a cpu versus a gpu figure 1 the gpu devotes more transistors to data processing devoting more transistors to data processing for example floatingpoint computations is beneficial for highly parallel computations the gpu can hide memory access latencies with computation instead of relying on large data caches and complex flow control to avoid long memory access latencies both of which are expensive in terms of transistors in general an application has a mix of parallel parts and sequential parts so systems are designed with a mix of gpus and cpus in order to maximize overall performance applications with a high degree of parallelism can exploit this massively parallel nature of the gpu to achieve higher performance than on the cpu  cuda a generalpurpose parallel computing platform and programming model in november 2006 nvidia introduced cuda  a general purpose parallel computing platform and programming model that leverages the parallel compute engine in nvidia gpus to solve many complex computational problems in a more efficient way than on a cpu cuda comes with a software environment that allows developers to use c as a highlevel programming language as illustrated by figure 2  other languages application programming interfaces or directivesbased approaches are supported such as fortran directcompute openacc figure 2 gpu computing applications cuda is designed to support various languages and application programming interfaces  a scalable programming model the advent of multicore cpus and manycore gpus means that mainstream processor chips are now parallel systems the challenge is to develop application software that transparently scales its parallelism to leverage the increasing number of processor cores much as 3d graphics applications transparently scale their parallelism to manycore gpus with widely varying numbers of cores the cuda parallel programming model is designed to overcome this challenge while maintaining a low learning curve for programmers familiar with standard programming languages such as c at its core are three key abstractions a hierarchy of thread groups shared memories and barrier synchronization that are simply exposed to the programmer as a minimal set of language extensions these abstractions provide finegrained data parallelism and thread parallelism nested within coarsegrained data parallelism and task parallelism they guide the programmer to partition the problem into coarse subproblems that can be solved independently in parallel by blocks of threads and each subproblem into finer pieces that can be solved cooperatively in parallel by all threads within the block this decomposition preserves language expressivity by allowing threads to cooperate when solving each subproblem and at the same time enables automatic scalability indeed each block of threads can be scheduled on any of the available multiprocessors within a gpu in any order concurrently or sequentially so that a compiled cuda program can execute on any number of multiprocessors as illustrated by figure 3  and only the runtime system needs to know the physical multiprocessor count this scalable programming model allows the gpu architecture to span a wide market range by simply scaling the number of multiprocessors and memory partitions from the highperformance enthusiast geforce gpus and professional quadro and tesla computing products to a variety of inexpensive mainstream geforce gpus see cudaenabled gpus for a list of all cudaenabled gpus figure 3 automatic scalability note a gpu is built around an array of streaming multiprocessors sms see hardware implementation for more details a multithreaded program is partitioned into blocks of threads that execute independently from each other so that a gpu with more multiprocessors will automatically execute the program in less time than a gpu with fewer multiprocessors  document structure this document is organized into the following sections introduction is a general introduction to cuda programming model outlines the cuda programming model programming interface describes the programming interface hardware implementation describes the hardware implementation performance guidelines gives some guidance on how to achieve maximum performance cudaenabled gpus lists all cudaenabled devices c language extensions is a detailed description of all extensions to the c language cooperative groups describes synchronization primitives for various groups of cuda threads cuda dynamic parallelism describes how to launch and synchronize one kernel from another virtual memory management describes how to manage the unified virtual address space stream ordered memory allocator describes how applications can order memory allocation and deallocation graph memory nodes describes how graphs can create and own memory allocations mathematical functions lists the mathematical functions supported in cuda c language support lists the c features supported in device code texture fetching gives more details on texture fetching compute capabilities gives the technical specifications of various devices as well as more architectural,https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html,best practices guide
cuda c best practices guide 1 preface v125 pdf archive cuda c best practices guide the programming guide to using the cuda toolkit to obtain the best performance from nvidia gpus 1 preface  what is this document this best practices guide is a manual to help developers obtain the best performance from nvidia cuda gpus it presents established parallelization and optimization techniques and explains coding metaphors and idioms that can greatly simplify programming for cudacapable gpu architectures while the contents can be used as a reference manual you should be aware that some topics are revisited in different contexts as various programming and configuration topics are explored as a result it is recommended that firsttime readers proceed through the guide sequentially this approach will greatly improve your understanding of effective programming practices and enable you to better use the guide for reference later  who should read this guide the discussions in this guide all use the c programming language so you should be comfortable reading c code this guide refers to and relies on several other documents that you should have at your disposal for reference all of which are available at no cost from the cuda website httpsdocsnvidiacomcuda  the following documents are especially important resources cuda installation guide cuda c programming guide cuda toolkit reference manual in particular the optimization section of this guide assumes that you have already successfully downloaded and installed the cuda toolkit if not please refer to the relevant cuda installation guide for your platform and that you have a basic familiarity with the cuda c programming language and environment if not please refer to the cuda c programming guide  assess parallelize optimize deploy this guide introduces the assess parallelize optimize deployapod design cycle for applications with the goal of helping application developers to rapidly identify the portions of their code that would most readily benefit from gpu acceleration rapidly realize that benefit and begin leveraging the resulting speedups in production as early as possible apod is a cyclical process initial speedups can be achieved tested and deployed with only minimal initial investment of time at which point the cycle can begin again by identifying further optimization opportunities seeing additional speedups and then deploying the even faster versions of the application into production 1 assess for an existing project the first step is to assess the application to locate the parts of the code that are responsible for the bulk of the execution time armed with this knowledge the developer can evaluate these bottlenecks for parallelization and start to investigate gpu acceleration by understanding the endusers requirements and constraints and by applying amdahls and gustafsons laws the developer can determine the upper bound of performance improvement from acceleration of the identified portions of the application 2 parallelize having identified the hotspots and having done the basic exercises to set goals and expectations the developer needs to parallelize the code depending on the original code this can be as simple as calling into an existing gpuoptimized library such as cublas  cufft  or thrust  or it could be as simple as adding a few preprocessor directives as hints to a parallelizing compiler on the other hand some applications designs will require some amount of refactoring to expose their inherent parallelism as even cpu architectures will require exposing parallelism in order to improve or simply maintain the performance of sequential applications the cuda family of parallel programming languages cuda c cuda fortran etc aims to make the expression of this parallelism as simple as possible while simultaneously enabling operation on cudacapable gpus designed for maximum parallel throughput 3 optimize after each round of application parallelization is complete the developer can move to optimizing the implementation to improve performance since there are many possible optimizations that can be considered having a good understanding of the needs of the application can help to make the process as smooth as possible however as with apod as a whole program optimization is an iterative process identify an opportunity for optimization apply and test the optimization verify the speedup achieved and repeat meaning that it is not necessary for a programmer to spend large amounts of time memorizing the bulk of all possible optimization strategies prior to seeing good speedups instead strategies can be applied incrementally as they are learned optimizations can be applied at various levels from overlapping data transfers with computation all the way down to finetuning floatingpoint operation sequences the available profiling tools are invaluable for guiding this process as they can help suggest a nextbest course of action for the developers optimization efforts and provide references into the relevant portions of the optimization section of this guide 4 deploy having completed the gpu acceleration of one or more components of the application it is possible to compare the outcome with the original expectation recall that the initial assess step allowed the developer to determine an upper bound for the potential speedup attainable by accelerating given hotspots before tackling other hotspots to improve the total speedup the developer should consider taking the partially parallelized implementation and carry it through to production this is important for a number of reasons for example it allows the user to profit from their investment as early as possible the speedup may be partial but is still valuable and it minimizes risk for the developer and the user by providing an evolutionary rather than revolutionary set of changes to the application  recommendations and best practices throughout this guide specific recommendations are made regarding the design and implementation of cuda c code these recommendations are categorized by priority which is a blend of the effect of the recommendation and its scope actions that present substantial improvements for most cuda applications have the highest priority while small optimizations that affect only very specific situations are given a lower priority before implementing lower priority recommendations it is good practice to make sure all higher priority recommendations that are relevant have,https://docs.nvidia.com/cuda/maxwell-compatibility-guide/index.html,maxwell compatibility guide
maxwell compatibility guide 1 maxwell compatibility v125 pdf archive maxwell compatibility guide for cuda applications the guide to building cuda applications for gpus based on the nvidia maxwell architecture 1 maxwell compatibility  about this document this application note maxwell compatibility guide for cuda applications is intended to help developers ensure that their nvidia cuda applications will run on gpus based on the nvidia maxwell architecture this document provides guidance to developers who are already familiar with programming in cuda c and want to make sure that their software applications are compatible with maxwell  application compatibility on maxwell the nvidia cuda c compiler nvcc  can be used to generate both architecturespecific cubin files and forwardcompatible ptx versions of each kernel each cubin file targets a specific computecapability version and is forwardcompatible only with gpu architectures of the same major version number  for example cubin files that target compute capability  are supported on all computecapability 3x kepler devices but are not supported on computecapability 5x maxwell devices for this reason to ensure forward compatibility with gpu architectures introduced after the application has been released it is recommended that all applications include ptx versions of their kernels note cuda runtime applications containing both cubin and ptx code for a given architecture will automatically use the cubin by default keeping the ptx path strictly for forwardcompatibility purposes applications that already include ptx versions of their kernels should work asis on maxwellbased gpus applications that only support specific gpu architectures via cubin files however will need to be updated to provide maxwellcompatible ptx or cubins  verifying maxwell compatibility for existing applications the first step is to check that maxwellcompatible device code at least ptx is compiled in to the application the following sections show how to accomplish this for applications built with different cuda toolkit versions 1 applications using cuda toolkit  or earlier cuda applications built using cuda toolkit versions  through  are compatible with maxwell as long as they are built to include ptx versions of their kernels to test that ptx jit is working for your application you can do the following download and install the latest driver from httpswwwnvidiacomdrivers  set the environment variable cudaforceptxjit1  launch your application when starting a cuda application for the first time with the above environment flag the cuda driver will jitcompile the ptx for each cuda kernel that is used into native cubin code if you set the environment variable above and then launch your program and it works properly then you have successfully verified maxwell compatibility note be sure to unset the cudaforceptxjit environment variable when you are done testing 2 applications using cuda toolkit  or later cuda applications built using cuda toolkit  or later 1 are compatible with maxwell as long as they are built to include kernels in either maxwellnative cubin format see building applications with maxwell support or ptx format see applications using cuda toolkit  or earlier or both  building applications with maxwell support when a cuda application launches a kernel the cuda runtime determines the compute capability of each gpu in the system and uses this information to automatically find the best matching cubin or ptx version of the kernel that is available if a cubin file supporting the architecture of the target gpu is available it is used otherwise the cuda runtime will load the ptx and jitcompile that ptx to the gpus native cubin format before launching it if neither is available then the kernel launch will fail the method used to build your application with either native cubin or at least ptx support for maxwell depend on the version of the cuda toolkit used the main advantages of providing native cubins are as follows it saves the end user the time it takes to jitcompile kernels that are available only as ptx all kernels compiled into the application must have native binaries at load time or else they will be built justintime from ptx including kernels from all libraries linked to the application even if those kernels are never launched by the application especially when using large libraries this jit compilation can take a significant amount of time the cuda driver will cache the cubins generated as a result of the ptx jit so this is mostly a onetime cost for a given user but it is time best avoided whenever possible ptx jitcompiled kernels often cannot take advantage of architectural features of newer gpus meaning that nativecompiled code may be faster or of greater accuracy 1 applications using cuda toolkit  or earlier the compilers included in cuda toolkit  or earlier generate cubin files native to earlier nvidia architectures such as fermi and kepler but they cannot generate cubin files native to the maxwell architecture to allow support for maxwell and future architectures when using version  or earlier of the cuda toolkit the compiler must generate a ptx version of each kernel below are compiler settings that could be used to build mykernelcu to run on fermi or kepler devices natively and on maxwell devices via ptx jit note computexx refers to a ptx version and smxx refers to a cubin version the arch clause of the gencode commandline option to nvcc specifies the frontend compilation target and must always be a ptx version the code clause specifies the backend compilation target and can either be cubin or ptx or both only the backend target versions specified by the code clause will be retained in the resulting binary at least one must be ptx to provide maxwell compatibility windows nvccexe ccbin cvs2010vcbin xcompiler ehsc w3 nologo o2 zi mt gencodearchcompute20codesm20 gencodearchcompute30codesm30 gencodearchcompute35codesm35 gencodearchcompute35codecompute35 compile o releasemykernelcuobj mykernelcu maclinux usrlocalcudabinnvcc gencodearchcompute20codesm20 gencodearchcompute30codesm30 gencodearchcompute35codesm35 gencodearchcompute35codecompute35 o2 o mykernelo c mykernelcu alternatively you may be familiar with the simplified nvcc commandline option archsmxx  which is a shorthand equivalent to the following more explicit gencode commandline options used above archsmxx expands to the following gencodearchcomputexxcodesmxx,https://docs.nvidia.com/cuda/pascal-compatibility-guide/index.html,pascal compatibility guide
pascal compatibility guide 1 pascal compatibility v125 pdf archive pascal compatibility guide for cuda applications the guide to building cuda applications for gpus based on the nvidia pascal architecture 1 pascal compatibility  about this document this application note pascal compatibility guide for cuda applications is intended to help developers ensure that their nvidia cuda applications will run on gpus based on the nvidia pascal architecture this document provides guidance to developers who are already familiar with programming in cuda c and want to make sure that their software applications are compatible with pascal  application compatibility on pascal the nvidia cuda c compiler nvcc  can be used to generate both architecturespecific cubin files and forwardcompatible ptx versions of each kernel each cubin file targets a specific computecapability version and is forwardcompatible only with gpu architectures of the same major version number  for example cubin files that target compute capability  are supported on all computecapability 3x kepler devices but are not supported on computecapability 5x maxwell or 6x pascal devices for this reason to ensure forward compatibility with gpu architectures introduced after the application has been released it is recommended that all applications include ptx versions of their kernels note cuda runtime applications containing both cubin and ptx code for a given architecture will automatically use the cubin by default keeping the ptx path strictly for forwardcompatibility purposes applications that already include ptx versions of their kernels should work asis on pascalbased gpus applications that only support specific gpu architectures via cubin files however will need to be updated to provide pascalcompatible ptx or cubins  verifying pascal compatibility for existing applications the first step is to check that pascalcompatible device code at least ptx is compiled in to the application the following sections show how to accomplish this for applications built with different cuda toolkit versions 1 applications using cuda toolkit  or earlier cuda applications built using cuda toolkit versions  through  are compatible with pascal as long as they are built to include ptx versions of their kernels to test that ptx jit is working for your application you can do the following download and install the latest driver from httpswwwnvidiacomdrivers  set the environment variable cudaforceptxjit1  launch your application when starting a cuda application for the first time with the above environment flag the cuda driver will jitcompile the ptx for each cuda kernel that is used into native cubin code if you set the environment variable above and then launch your program and it works properly then you have successfully verified pascal compatibility note be sure to unset the cudaforceptxjit environment variable when you are done testing 2 applications using cuda toolkit  cuda applications built using cuda toolkit  are compatible with pascal as long as they are built to include kernels in either pascalnative cubin format see building applications with pascal support or ptx format see applications using cuda toolkit  or earlier or both  building applications with pascal support when a cuda application launches a kernel the cuda runtime determines the compute capability of each gpu in the system and uses this information to automatically find the best matching cubin or ptx version of the kernel that is available if a cubin file supporting the architecture of the target gpu is available it is used otherwise the cuda runtime will load the ptx and jitcompile that ptx to the gpus native cubin format before launching it if neither is available then the kernel launch will fail the method used to build your application with either native cubin or at least ptx support for pascal depend on the version of the cuda toolkit used the main advantages of providing native cubins are as follows it saves the end user the time it takes to jitcompile kernels that are available only as ptx all kernels compiled into the application must have native binaries at load time or else they will be built justintime from ptx including kernels from all libraries linked to the application even if those kernels are never launched by the application especially when using large libraries this jit compilation can take a significant amount of time the cuda driver will cache the cubins generated as a result of the ptx jit so this is mostly a onetime cost for a given user but it is time best avoided whenever possible ptx jitcompiled kernels often cannot take advantage of architectural features of newer gpus meaning that nativecompiled code may be faster or of greater accuracy 1 applications using cuda toolkit  or earlier the compilers included in cuda toolkit  or earlier generate cubin files native to earlier nvidia architectures such as kepler and maxwell but they cannot generate cubin files native to the pascal architecture to allow support for pascal and future architectures when using version  or earlier of the cuda toolkit the compiler must generate a ptx version of each kernel below are compiler settings that could be used to build mykernelcu to run on kepler or maxwell devices natively and on pascal devices via ptx jit note computexx refers to a ptx version and smxx refers to a cubin version the arch clause of the gencode commandline option to nvcc specifies the frontend compilation target and must always be a ptx version the code clause specifies the backend compilation target and can either be cubin or ptx or both only the backend target versions specified by the code clause will be retained in the resulting binary at least one must be ptx to provide pascal compatibility windows nvccexe ccbin cvs2010vcbin xcompiler ehsc w3 nologo o2 zi mt gencodearchcompute30codesm30 gencodearchcompute35codesm35 gencodearchcompute50codesm50 gencodearchcompute52codesm52 gencodearchcompute52codecompute52 compile o releasemykernelcuobj mykernelcu maclinux usrlocalcudabinnvcc gencodearchcompute30codesm30 gencodearchcompute35codesm35 gencodearchcompute50codesm50 gencodearchcompute52codesm52 gencodearchcompute52codecompute52 o2 o mykernelo c mykernelcu alternatively you may be familiar with the simplified nvcc commandline option archsmxx  which is a shorthand equivalent to the following more explicit gencode commandline options used above archsmxx expands to the following gencodearchcomputexxcodesmxx,https://docs.nvidia.com/cuda/volta-compatibility-guide/index.html,volta compatibility guide
volta compatibility guide 1 volta compatibility v125 pdf archive volta compatibility guide for cuda applications the guide to building cuda applications for gpus based on the nvidia volta architecture 1 volta compatibility  about this document this application note volta compatibility guide for cuda applications is intended to help developers ensure that their nvidia cuda applications will run on gpus based on the nvidia volta architecture this document provides guidance to developers who are already familiar with programming in cuda c and want to make sure that their software applications are compatible with volta  application compatibility on volta the nvidia cuda c compiler nvcc  can be used to generate both architecturespecific cubin files and forwardcompatible ptx versions of each kernel each cubin file targets a specific computecapability version and is forwardcompatible only with gpu architectures of the same major version number  for example cubin files that target compute capability  are supported on all computecapability 3x kepler devices but are not supported on computecapability 5x maxwell or 6x pascal devices for this reason to ensure forward compatibility with gpu architectures introduced after the application has been released it is recommended that all applications include ptx versions of their kernels note cuda runtime applications containing both cubin and ptx code for a given architecture will automatically use the cubin by default keeping the ptx path strictly for forwardcompatibility purposes applications that already include ptx versions of their kernels should work asis on voltabased gpus applications that only support specific gpu architectures via cubin files however will need to be updated to provide voltacompatible ptx or cubins  verifying volta compatibility for existing applications the first step is to check that voltacompatible device code at least ptx is compiled into the application the following sections show how to accomplish this for applications built with different cuda toolkit versions 1 applications using cuda toolkit  or earlier cuda applications built using cuda toolkit versions  through  are compatible with volta as long as they are built to include ptx versions of their kernels to test that ptx jit is working for your application you can do the following download and install the latest driver from httpwwwnvidiacomdrivers  set the environment variable cudaforceptxjit1  launch your application when starting a cuda application for the first time with the above environment flag the cuda driver will jitcompile the ptx for each cuda kernel that is used into native cubin code if you set the environment variable above and then launch your program and it works properly then you have successfully verified volta compatibility note be sure to unset the cudaforceptxjit environment variable when you are done testing 2 applications using cuda toolkit  cuda applications built using cuda toolkit  are compatible with volta as long as they are built to include kernels in either voltanative cubin format see building applications with volta support or ptx format see applications using cuda toolkit  or earlier or both  building applications with volta support when a cuda application launches a kernel the cuda runtime determines the compute capability of each gpu in the system and uses this information to automatically find the best matching cubin or ptx version of the kernel that is available if a cubin file supporting the architecture of the target gpu is available it is used otherwise the cuda runtime will load the ptx and jitcompile that ptx to the gpus native cubin format before launching it if neither is available then the kernel launch will fail the method used to build your application with either native cubin or at least ptx support for volta depend on the version of the cuda toolkit used the main advantages of providing native cubins are as follows it saves the end user the time it takes to jitcompile kernels that are available only as ptx all kernels compiled into the application must have native binaries at load time or else they will be built justintime from ptx including kernels from all libraries linked to the application even if those kernels are never launched by the application especially when using large libraries this jit compilation can take a significant amount of time the cuda driver will cache the cubins generated as a result of the ptx jit so this is mostly a onetime cost for a given user but it is time best avoided whenever possible ptx jitcompiled kernels often cannot take advantage of architectural features of newer gpus meaning that nativecompiled code may be faster or of greater accuracy 1 applications using cuda toolkit  or earlier the compilers included in cuda toolkit  or earlier generate cubin files native to earlier nvidia architectures such as maxwell and pascal but they cannot generate cubin files native to the volta architecture to allow support for volta and future architectures when using version  or earlier of the cuda toolkit the compiler must generate a ptx version of each kernel below are compiler settings that could be used to build mykernelcu to run on maxwell or pascal devices natively and on volta devices via ptx jit note computexx refers to a ptx version and smxx refers to a cubin version the arch clause of the gencode commandline option to nvcc specifies the frontend compilation target and must always be a ptx version the code clause specifies the backend compilation target and can either be cubin or ptx or both only the backend target versions specified by the code clause will be retained in the resulting binary at least one must be ptx to provide volta compatibility windows nvccexe ccbin cvs2010vcbin xcompiler ehsc w3 nologo o2 zi mt gencodearchcompute50codesm50 gencodearchcompute52codesm52 gencodearchcompute60codesm60 gencodearchcompute61codesm61 gencodearchcompute61codecompute61 compile o releasemykernelcuobj mykernelcu maclinux usrlocalcudabinnvcc gencodearchcompute50codesm50 gencodearchcompute52codesm52 gencodearchcompute60codesm60 gencodearchcompute61codesm61 gencodearchcompute61codecompute61 o2 o mykernelo c mykernelcu alternatively you may be familiar with the simplified nvcc commandline option archsmxx  which is a shorthand equivalent to the following more explicit gencode commandline options used above archsmxx expands to the following gencodearchcomputexxcodesmxx gencodearchcomputexxcodecomputexx,https://docs.nvidia.com/cuda/turing-compatibility-guide/index.html,turing compatibility guide
turing compatibility guide 1 turing compatibility v125 pdf archive turing compatibility guide for cuda applications the guide to building cuda applications for nvidia turing gpus 1 turing compatibility  about this document this application note turing compatibility guide for cuda applications is intended to help developers ensure that their nvidia cuda applications will run on gpus based on the nvidia turing architecture this document provides guidance to developers who are already familiar with programming in cuda c and want to make sure that their software applications are compatible with turing  application compatibility on turing the nvidia cuda c compiler nvcc  can be used to generate both architecturespecific cubin files and forwardcompatible ptx versions of each kernel each cubin file targets a specific computecapability version and is forwardcompatible only with gpu architectures of the same major version number  for example cubin files that target compute capability  are supported on all computecapability 3x kepler devices but are not supported on computecapability 5x maxwell or 6x pascal devices for this reason to ensure forward compatibility with gpu architectures introduced after the application has been released it is recommended that all applications include ptx versions of their kernels note cuda runtime applications containing both cubin and ptx code for a given architecture will automatically use the cubin by default keeping the ptx path strictly for forwardcompatibility purposes applications that already include ptx versions of their kernels should work asis on turingbased gpus applications that only support specific gpu architectures via cubin files however will need to be updated to provide turingcompatible ptx or cubins  compatibility between volta and turing the turing architecture is based on voltas instruction set architecture isa  extending it with new instructions as a consequence any binary that runs on volta will be able to run on turing forward compatibility but a turing binary will not be able to run on volta please note that volta kernels using more than 64kb of shared memory via the explicit optin see cuda c programming guide will not be able to launch on turing as they would exceed turings shared memory capacity most applications compiled for volta should run efficiently on turing except if the application uses heavily the tensor cores or if recompiling would allow use of new turingspecific instructions voltas tensor core instructions can only reach half of the peak performance on turing recompiling explicitly for turing is thus recommended  verifying turing compatibility for existing applications the first step is to check that turingcompatible device code at least ptx is compiled into the application the following sections show how to accomplish this for applications built with different cuda toolkit versions 1 applications using cuda toolkit  or earlier cuda applications built using cuda toolkit versions  through  are compatible with turing as long as they are built to include ptx versions of their kernels to test that ptx jit is working for your application you can do the following download and install the latest driver from httpswwwnvidiacomdrivers  set the environment variable cudaforceptxjit1  launch your application when starting a cuda application for the first time with the above environment flag the cuda driver will jitcompile the ptx for each cuda kernel that is used into native cubin code if you set the environment variable above and then launch your program and it works properly then you have successfully verified turing compatibility note be sure to unset the cudaforceptxjit environment variable when you are done testing 2 applications using cuda toolkit 9x cuda applications built using cuda toolkit 9x are compatible with turing as long as they are built to include kernels in either voltanative cubin format see compatibility between volta and turing or ptx format see applications using cuda toolkit  or earlier or both 3 applications using cuda toolkit  cuda applications built using cuda toolkit  are compatible with turing as long as they are built to include kernels in voltanative or turingnative cubin format see compatibility between volta and turing  or ptx format see applications using cuda toolkit  or earlier  or both  building applications with turing support when a cuda application launches a kernel the cuda runtime determines the compute capability of each gpu in the system and uses this information to automatically find the best matching cubin or ptx version of the kernel that is available if a cubin file supporting the architecture of the target gpu is available it is used otherwise the cuda runtime will load the ptx and jitcompile that ptx to the gpus native cubin format before launching it if neither is available then the kernel launch will fail the method used to build your application with either native cubin or at least ptx support for turing depend on the version of the cuda toolkit used the main advantages of providing native cubins are as follows it saves the end user the time it takes to jitcompile kernels that are available only as ptx all kernels compiled into the application must have native binaries at load time or else they will be built justintime from ptx including kernels from all libraries linked to the application even if those kernels are never launched by the application especially when using large libraries this jit compilation can take a significant amount of time the cuda driver will cache the cubins generated as a result of the ptx jit so this is mostly a onetime cost for a given user but it is time best avoided whenever possible ptx jitcompiled kernels often cannot take advantage of architectural features of newer gpus meaning that nativecompiled code may be faster or of greater accuracy 1 applications using cuda toolkit  or earlier the compilers included in cuda toolkit  or earlier generate cubin files native to earlier nvidia architectures such as maxwell and pascal but they cannot generate cubin files native to volta or turing architecture to allow support for volta turing and future architectures when using,https://docs.nvidia.com/cuda/ampere-compatibility-guide/index.html,nvidia ampere gpu architecture compatibility guide
ampere compatibility guide 1 nvidia ampere gpu architecture compatibility v125 pdf archive nvidia ampere gpu architecture compatibility guide for cuda applications the guide to building cuda applications for gpus based on the nvidia ampere gpu architecture 1 nvidia ampere gpu architecture compatibility  about this document this application note nvidia ampere gpu architecture compatibility guide for cuda applications is intended to help developers ensure that their nvidia cuda applications will run on the nvidia ampere architecture based gpus this document provides guidance to developers who are familiar with programming in cuda c and want to make sure that their software applications are compatible with the nvidia ampere gpu architecture  application compatibility on the nvidia ampere gpu architecture a cuda application binary with one or more gpu kernels can contain the compiled gpu code in two forms binary cubin objects and forwardcompatible ptx assembly for each kernel both cubin and ptx are generated for a certain target compute capability a cubin generated for a certain compute capability is supported to run on any gpu with the same major revision and same or higher minor revision of compute capability for example a cubin generated for compute capability  is supported to run on a gpu with compute capability  however a cubin generated for compute capability  is not supported to run on a gpu with compute capability  and a cubin generated with compute capability 7x is not supported to run on a gpu with compute capability 8x kernel can also be compiled to a ptx form at the application load time ptx is compiled to cubin and the cubin is used for kernel execution unlike cubin ptx is forwardcompatible meaning ptx is supported to run on any gpu with compute capability higher than the compute capability assumed for generation of that ptx for example ptx code generated for compute capability 7x is supported to run on compute capability 7x or any higher revision major or minor including compute capability 8x therefore although it is optional it is recommended that all applications should include ptx of the kernels to ensure forwardcompatibility to read more about cubin and ptx compatibilities see compilation with nvcc from the programming guide when a cuda application launches a kernel on a gpu the cuda runtime determines the compute capability of the gpu in the system and uses this information to find the best matching cubin or ptx version of the kernel if a cubin compatible with that gpu is present in the binary the cubin is used asis for execution otherwise the cuda runtime first generates compatible cubin by jitcompiling 1 the ptx and then the cubin is used for the execution if neither compatible cubin nor ptx is available kernel launch results in a failure application binaries that include ptx version of kernels should work asis on the nvidia ampere architecture based gpus in such cases rebuilding the application is not required however application binaries which do not include ptx only include cubins need to be rebuilt to run on the nvidia ampere architecture based gpus to know more about building compatible applications read building applications with the nvidia ampere gpu architecture support   verifying ampere compatibility for existing applications the first step towards making a cuda application compatible with the nvidia ampere gpu architecture is to check if the application binary already contains compatible gpu code at least the ptx the following sections explain how to accomplish this for an already built cuda application 1 applications built using cuda toolkit  or earlier cuda applications built using cuda toolkit versions  through  are compatible with nvidia ampere architecture based gpus as long as they are built to include ptx versions of their kernels this can be tested by forcing the ptx to jitcompile at application load time with following the steps download and install the latest driver from httpswwwnvidiacomdrivers  set the environment variable cudaforceptxjit1  launch the application with cudaforceptxjit1  gpu binary code embedded in an application binary is ignored instead ptx code for each kernel is jitcompiled to produce gpu binary code an application fails to execute if it does not include ptx this means the application is not compatible with the nvidia ampere gpu architecture and needs to be rebuilt for compatibility on the other hand if the application works properly with this environment variable set then the application is compatible with the nvidia ampere gpu architecture note be sure to unset the cudaforceptxjit environment variable after testing is done 2 applications built using cuda toolkit  cuda applications built using cuda toolkit  are compatible with the nvidia ampere gpu architecture as long as they are built to include kernels in native cubin compute capability  or ptx form or both  building applications with the nvidia ampere gpu architecture support depending on the version of the cuda toolkit used for building the application it can be built to include ptx andor native cubin for the nvidia ampere gpu architecture although it is enough to just include ptx including native cubin also has the following advantages it saves the end user the time it takes to jitcompile kernels that are available only as ptx all kernels which do not have native cubins are jitcompiled from ptx including kernels from all the libraries linked to the application even if those kernels are never launched by the application especially when using large libraries this jit compilation can take a significant amount of time the cuda driver caches the cubins generated as a result of the ptx jit so this is mostly a onetime cost for a user but it is time best avoided whenever possible ptx jitcompiled kernels often cannot take advantage of architectural features of newer gpus meaning that nativecompiled cubins may be faster or of greater accuracy 1 building applications using cuda toolkit 10x or earlier the nvcc compiler included with versions 10x   and  of the cuda toolkit can generate cubins native to,https://docs.nvidia.com/cuda/hopper-compatibility-guide/index.html,hopper compatibility guide
hopper compatibility guide 1 hopper architecture compatibility v125 pdf archive hopper compatibility guide for cuda applications the guide to building cuda applications for hopper gpus 1 hopper architecture compatibility  about this document this application note hopper architecture compatibility guide for cuda applications is intended to help developers ensure that their nvidia cuda applications will run on the nvidia hopper architecture based gpus this document provides guidance to developers who are familiar with programming in cuda c and want to make sure that their software applications are compatible with hopper architecture  application compatibility on hopper architecture a cuda application binary with one or more gpu kernels can contain the compiled gpu code in two forms binary cubin objects and forwardcompatible ptx assembly for each kernel both cubin and ptx are generated for a certain target compute capability a cubin generated for a certain compute capability is supported to run on any gpu with the same major revision and same or higher minor revision of compute capability for example a cubin generated for compute capability  is supported to run on a gpu with compute capability  however a cubin generated for compute capability  is not supported to run on a gpu with compute capability  and a cubin generated with compute capability 8x is not supported to run on a gpu with compute capability  kernel can also be compiled to a ptx form at the application load time ptx is compiled to cubin and the cubin is used for kernel execution unlike cubin ptx is forwardcompatible meaning ptx is supported to run on any gpu with compute capability higher than the compute capability assumed for generation of that ptx for example ptx code generated for compute capability 8x is supported to run on compute capability 8x or any higher revision major or minor including compute capability  therefore although it is optional it is recommended that all applications should include ptx of the kernels to ensure forwardcompatibility to read more about cubin and ptx compatibilities see compilation with nvcc from the cuda c programming guide  when a cuda application launches a kernel on a gpu the cuda runtime determines the compute capability of the gpu in the system and uses this information to find the best matching cubin or ptx version of the kernel if a cubin compatible with that gpu is present in the binary the cubin is used asis for execution otherwise the cuda runtime first generates compatible cubin by jitcompiling 1 the ptx and then the cubin is used for the execution if neither compatible cubin nor ptx is available kernel launch results in a failure application binaries that include ptx version of kernels should work asis on the hopper gpus in such cases rebuilding the application is not required however application binaries which do not include ptx only include cubins need to be rebuilt to run on the hopper gpus to know more about building compatible applications read building applications with hopper architecture support application binaries that include ptx version of kernels with architecture conditional features using sm90a or compute90a in order to take full advantage of hopper gpu architecture are not forward or backward compatible  verifying hopper compatibility for existing applications the first step towards making a cuda application compatible with hopper architecture is to check if the application binary already contains compatible gpu code at least the ptx the following sections explain how to accomplish this for an already built cuda application 1 applications built using cuda toolkit  or earlier cuda applications built using cuda toolkit versions  through  are compatible with hopper gpus as long as they are built to include ptx versions of their kernels this can be tested by forcing the ptx to jitcompile at application load time with following the steps download and install the latest driver from httpswwwnvidiacomdrivers  set the environment variable cudaforceptxjit1  launch the application with cudaforceptxjit1  gpu binary code embedded in an application binary is ignored instead ptx code for each kernel is jitcompiled to produce gpu binary code an application fails to execute if it does not include ptx this means the application is not hopper architecture compatible and needs to be rebuilt for compatibility on the other hand if the application works properly with this environment variable set then the application is hopper compatible note be sure to unset the cudaforceptxjit environment variable after testing is done 2 applications built using cuda toolkit  cuda applications built using cuda toolkit  are compatible with hopper architecture as long as they are built to include kernels in native cubin compute capability  or ptx form or both  building applications with hopper architecture support depending on the version of the cuda toolkit used for building the application it can be built to include ptx andor native cubin for the hopper architecture although it is enough to just include ptx including native cubin also has the following advantages it saves the end user the time it takes to jitcompile kernels that are available only as ptx all kernels which do not have native cubins are jitcompiled from ptx including kernels from all the libraries linked to the application even if those kernels are never launched by the application 2  especially when using large libraries this jit compilation can take a significant amount of time the cuda driver caches the cubins generated as a result of the ptx jit so this is mostly a onetime cost for a user but it is time best avoided whenever possible ptx jitcompiled kernels often cannot take advantage of architectural features of newer gpus meaning that nativecompiled cubins may be faster or of greater accuracy ptx code compiled to target architecture conditional features using sm90a or compute90a only runs on devices with compute capability  and is not backward or forward compatible 1 building applications using cuda toolkit  or earlier the nvcc compiler included with version  or earlier 7,https://docs.nvidia.com/cuda/ada-compatibility-guide/index.html,ada compatibility guide
ada compatibility guide 1 nvidia ada gpu architecture compatibility v125 pdf archive nvidia ada gpu architecture compatibility guide for cuda applications the guide to building cuda applications for nvidia ada gpus 1 nvidia ada gpu architecture compatibility  about this document this application note nvidia ada gpu architecture compatibility guide for cuda applications  is intended to help developers ensure that their nvidia cuda applications will run on the nvidia ada architecture based gpus this document provides guidance to developers who are familiar with programming in cuda c and want to make sure that their software applications are compatible with the nvidia ada gpu architecture  application compatibility on the nvidia ada gpu architecture a cuda application binary with one or more gpu kernels can contain the compiled gpu code in two forms binary cubin objects and forwardcompatible ptx assembly for each kernel both cubin and ptx are generated for a certain target compute capability a cubin generated for a certain compute capability is supported to run on any gpu with the same major revision and same or higher minor revision of compute capability for example a cubin generated for compute capability  is supported to run on a gpu with compute capability  however a cubin generated for compute capability  is not supported to run on a gpu with compute capability  and a cubin generated with compute capability 8x is not supported to run on a gpu with compute capability  kernels can also be compiled to a ptx form at the application load time ptx is compiled to cubin and the cubin is used for kernel execution unlike cubin ptx is forwardcompatible meaning ptx is supported to run on any gpu with compute capability higher than the compute capability assumed for generation of that ptx for example ptx code generated for compute capability 8x is supported to run on compute capability 8x or any higher revision major or minor including compute capability 9x therefore although it is optional it is recommended that all applications should include ptx of the kernels to ensure forwardcompatibility to read more about cubin and ptx compatibilities see compilation with nvcc from the cuda c programming guide  when a cuda application launches a kernel on a gpu the cuda runtime determines the compute capability of the gpu in the system and uses this information to find the best matching cubin or ptx version of the kernel if a cubin compatible with that gpu is present in the binary the cubin is used asis for execution otherwise the cuda runtime first generates compatible cubin by jitcompiling 1 the ptx and then the cubin is used for the execution if neither compatible cubin nor ptx is available kernel launch results in a failure application binaries that include ptx version of kernels should work asis on the nvidia ada architecture based gpus in such cases rebuilding the application is not required however application binaries that do not include ptx only include cubins need to be rebuilt to run on the nvidia ada architecture based gpus to know more about building compatible applications read building applications with the nvidia ada gpu architecture support   compatibility between ampere and ada the nvidia ada architecture is based on amperes instruction set architecture isa  extending it with new instructions as a consequence any binary that runs on ampere will be able to run on ada forward compatibility but an ada binary will not be able to run on ampere  verifying ada compatibility for existing applications the first step towards making a cuda application compatible with the nvidia ada gpu architecture is to check if the application binary already contains compatible gpu code at least the ptx the following sections explain how to accomplish this for an already built cuda application 1 applications built using cuda toolkit  or earlier cuda applications built using cuda toolkit versions  through  are compatible with nvidia ada architecture based gpus as long as they are built to include ptx versions of their kernels this can be tested by forcing the ptx to jitcompile at application load time with following the steps download and install the latest driver from httpswwwnvidiacomdrivers  set the environment variable cudaforceptxjit1  launch the application with cudaforceptxjit1  gpu binary code embedded in an application binary is ignored instead ptx code for each kernel is jitcompiled to produce gpu binary code an application fails to execute if it does not include ptx this means the application is not compatible with the nvidia ada gpu architecture and needs to be rebuilt for compatibility on the other hand if the application works properly with this environment variable set then the application is compatible with the nvidia ada gpu architecture note be sure to unset the cudaforceptxjit environment variable after testing is done 2 applications built using cuda toolkit  through  cuda applications built using cuda toolkit  through  are compatible with the nvidia ada gpu architecture as long as they are built to include kernels in amperenative cubin see compatibility between ampere and ada or ptx format see applications built using cuda toolkit  or earlier  or both 3 applications built using cuda toolkit  cuda applications built using cuda toolkit  are compatible with the nvidia ada gpu architecture as long as they are built to include kernels in amperenative or adanative cubin see compatibility between ampere and ada  or ptx format see applications built using cuda toolkit  or earlier  or both  building applications with the nvidia ada gpu architecture support depending on the version of the cuda toolkit used for building the application it can be built to include ptx andor native cubin for the nvidia ada gpu architecture although it is sufficient to just include ptx including native cubin also has the following advantages it saves the end user the time it takes to jitcompile kernels that are available only as ptx all kernels that do,https://docs.nvidia.com/cuda/maxwell-tuning-guide/index.html,maxwell tuning guide
maxwell tuning guide 1 maxwell tuning guide v125 pdf archive tuning cuda applications for maxwell the programming guide to tuning cuda applications for gpus based on the nvidia maxwell architecture 1 maxwell tuning guide  nvidia maxwell compute architecture maxwell is nvidias nextgeneration architecture for cuda compute applications maxwell retains and extends the same cuda programming model as in previous nvidia architectures such as fermi and kepler and applications that follow the best practices for those architectures should typically see speedups on the maxwell architecture without any code changes this guide summarizes the ways that an application can be finetuned to gain additional speedups by leveraging maxwell architectural features 1 maxwell introduces an allnew design for the streaming multiprocessor sm that dramatically improves energy efficiency although the kepler smx design was extremely efficient for its generation through its development nvidias gpu architects saw an opportunity for another big leap forward in architectural efficiency the maxwell sm is the realization of that vision improvements to control logic partitioning workload balancing clockgating granularity compilerbased scheduling number of instructions issued per clock cycle and many other enhancements allow the maxwell sm also called smm to far exceed kepler smx efficiency the first maxwellbased gpu is codenamed gm107 and is designed for use in powerlimited environments like notebooks and small form factor sff pcs gm107 is described in a whitepaper entitled nvidia geforce gtx 750 ti featuring firstgeneration maxwell gpu technology designed for extreme performance per watt  2 the first gpu using the secondgeneration maxwell architecture is codenamed gm204  secondgeneration maxwell gpus retain the power efficiency of the earlier generation while delivering significantly higher performance gm204 is described in a whitepaper entitled nvidia geforce gtx 980 featuring maxwell the most advanced gpu ever made  compute programming features of gm204 are similar to those of gm107 except where explicitly noted in this guide for details on the programming features discussed in this guide please refer to the cuda c programming guide   cuda best practices the performance guidelines and best practices described in the cuda c programming guide and the cuda c best practices guide apply to all cudacapable gpu architectures programmers must primarily focus on following those recommendations to achieve the best performance the highpriority recommendations from those guides are as follows find ways to parallelize sequential code minimize data transfers between the host and the device adjust kernel launch configuration to maximize device utilization ensure global memory accesses are coalesced minimize redundant accesses to global memory whenever possible avoid long sequences of diverged execution by threads within the same warp  application compatibility before addressing specific performance tuning issues covered in this guide refer to the maxwell compatibility guide for cuda applications to ensure that your application is compiled in a way that is compatible with maxwell  maxwell tuning 1 smm the maxwell streaming multiprocessor smm is similar in many respects to the kepler architectures smx the key enhancements of smm over smx are geared toward improving efficiency without requiring significant increases in available parallelism per sm from the application  occupancy the maximum number of concurrent warps per smm remains the same as in smx ie 64 and factors influencing warp occupancy remain similar or improved over smx the register file size 64k 32bit registers is the same as that of smx the maximum registers per thread 255 matches that of kepler gk110 as with kepler experimentation should be used to determine the optimum balance of register spilling vs occupancy however the maximum number of thread blocks per sm has been increased from 16 to 32 this should result in an automatic occupancy improvement for kernels with small thread blocks of 64 or fewer threads shared memory and register file resource requirements permitting such kernels would have tended to underutilize smx but less so smm shared memory capacity is increased see shared memory capacity  as such developers can expect similar or improved occupancy on smm without changes to their application at the same time warp occupancy requirements ie available parallelism for maximum device utilization are similar to or less than those of smx see instruction latencies   instruction scheduling the number of cuda cores per sm has been reduced to a power of two however with maxwells improved execution efficiency performance per sm is usually within 10 of kepler performance and the improved area efficiency of smm means cuda cores per gpu will be substantially higher vs comparable fermi or kepler chips smm retains the same number of instruction issue slots per clock and reduces arithmetic latencies compared to the kepler design as with smx each smm has four warp schedulers unlike smx however all smm core functional units are assigned to a particular scheduler with no shared units along with the selection of a poweroftwo number of cuda cores per sm which simplifies scheduling and reduces stall cycles this partitioning of sm computational resources in smm is a major component of the streamlined efficiency of smm the poweroftwo number of cuda cores per partition simplifies scheduling as each of smms warp schedulers issue to a dedicated set of cuda cores equal to the warp width each warp scheduler still has the flexibility to dualissue such as issuing a math operation to a cuda core in the same cycle as a memory operation to a loadstore unit but singleissue is now sufficient to fully utilize all cuda cores  instruction latencies another major improvement of smm is that dependent math latencies have been significantly reduced a consequence of this is a further reduction of stall cycles as the available warplevel parallelism ie occupancy on smm should be equal to or greater than that of smx see occupancy  while at the same time each math operation takes less time to complete improving utilization and throughput  instruction throughput the most significant changes to peak instruction throughputs in smm are as follows the change in number of cuda cores per sm brings with it a corresponding,https://docs.nvidia.com/cuda/pascal-tuning-guide/index.html,pascal tuning guide
pascal tuning guide 1 pascal tuning guide v125 pdf archive tuning cuda applications for pascal the programming guide to tuning cuda applications for gpus based on the nvidia pascal architecture 1 pascal tuning guide  nvidia pascal compute architecture pascal retains and extends the same cuda programming model provided by previous nvidia architectures such as maxwell and applications that follow the best practices for those architectures should typically see speedups on the pascal architecture without any code changes this guide summarizes the ways that an application can be finetuned to gain additional speedups by leveraging pascal architectural features 1 pascal architecture comprises two major variants gp100 and gp104 2 a detailed overview of the major improvements in gp100 and gp104 over earlier nvidia architectures are described in a pair of white papers entitled nvidia tesla p100 the most advanced datacenter accelerator ever built for gp100 and nvidia geforce gtx 1080 gaming perfected for gp104 for further details on the programming features discussed in this guide please refer to the cuda c programming guide  some of the pascal features described in this guide are specific to either gp100 or gp104 as noted if not specified features apply to both pascal variants  cuda best practices the performance guidelines and best practices described in the cuda c programming guide and the cuda c best practices guide apply to all cudacapable gpu architectures programmers must primarily focus on following those recommendations to achieve the best performance the highpriority recommendations from those guides are as follows find ways to parallelize sequential code minimize data transfers between the host and the device adjust kernel launch configuration to maximize device utilization ensure global memory accesses are coalesced minimize redundant accesses to global memory whenever possible avoid long sequences of diverged execution by threads within the same warp  application compatibility before addressing specific performance tuning issues covered in this guide refer to the pascal compatibility guide for cuda applications to ensure that your application is compiled in a way that is compatible with pascal  pascal tuning 1 streaming multiprocessor the pascal streaming multiprocessor sm is in many respects similar to that of maxwell pascal further improves the already excellent power efficiency provided by the maxwell architecture through both an improved 16nm finfet manufacturing process and various architectural modifications  instruction scheduling like maxwell pascal employs a poweroftwo number of cuda cores per partition this simplifies scheduling since each of the sms warp schedulers issue to a dedicated set of cuda cores equal to the warp width 32 each warp scheduler still has the flexibility to dualissue such as issuing a math operation to a cuda core in the same cycle as a memory operation to a loadstore unit but singleissue is now sufficient to fully utilize all cuda cores gp100 and gp104 designs incorporate different numbers of cuda cores per sm like maxwell each gp104 sm provides four warp schedulers managing a total of 128 singleprecision fp32 and four doubleprecision fp64 cores a gp104 processor provides up to 20 sms and the similar gp102 design provides up to 30 sms by contrast gp100 provides smaller but more numerous sms each gp100 provides up to 60 sms 3 each sm contains two warp schedulers managing a total of 64 fp32 and 32 fp64 cores the resulting 21 ratio of fp32 to fp64 cores aligns well with gp100s new datapath configuration allowing pascal to process fp64 workloads more efficiently than kepler gk210 the previous nvidia architecture to emphasize fp64 performance  occupancy the maximum number of concurrent warps per sm remains the same as in maxwell ie 64 and other factors influencing warp occupancy remain similar as well the register file size 64k 32bit registers is the same as that of maxwell the maximum registers per thread 255 matches that of maxwell as with previous architectures experimentation should be used to determine the optimum balance of register spilling vs occupancy however the maximum number of thread blocks per sm is 32 the same as maxwell shared memory capacity per sm is 64kb for gp100 and 96kb for gp104 for comparison maxwell provided 96kb and up to 112kb of shared memory respectively but each gp100 sm contains fewer cuda cores so the shared memory available per core actually increases on gp100 the maximum shared memory per block remains limited at 48kb as with prior architectures see shared memory capacity  as such developers can expect similar occupancy as on maxwell without changes to their application as a result of scheduling improvements relative to kepler warp occupancy requirements ie available parallelism needed for maximum device utilization are generally reduced 2 new arithmetic primitives  fp16 arithmetic support pascal provides improved fp16 support for applications like deep learning that are tolerant of low floatingpoint precision the half type is used to represent fp16 values on the device as with maxwell fp16 storage can be used to reduce the required memory footprint and bandwidth compared to fp32 or fp64 storage pascal also adds support for native fp16 instructions peak fp16 throughput is attained by using a paired operation to perform two fp16 instructions per core simultaneously to be eligible for the paired operation the operands must be stored in a half2 vector type gp100 and gp104 provide different fp16 throughputs gp100 designed with training deep neural networks in mind provides fp16 throughput up to 2x that of fp32 arithmetic on gp104 fp16 throughput is lower 164th that of fp32 however compensating for reduced fp16 throughput gp104 provides additional highthroughput int8 support not available in gp100  int8 dot product gp104 provides specialized instructions for twoway and fourway integer dot products these are well suited for accelerating deep learning inference workloads the dp4a intrinsic computes a dot product of four 8bit integers with accumulation into a 32bit integer similarly dp2a performs a twoelement dot product between two 16bit integers in one vector and two 8bit integers in another with accumulation into a 32bit integer both instructions offer a throughput equal to,https://docs.nvidia.com/cuda/volta-tuning-guide/index.html,volta tuning guide
volta tuning guide 1 volta tuning guide v125 pdf archive tuning cuda applications for volta the programming guide to tuning cuda applications for gpus based on the nvidia volta architecture 1 volta tuning guide  nvidia volta compute architecture volta is nvidias latest architecture for cuda compute applications volta retains and extends the same cuda programming model provided by previous nvidia architectures such as maxwell and pascal and applications that follow the best practices for those architectures should typically see speedups on the volta architecture without any code changes this guide summarizes the ways that an application can be finetuned to gain additional speedups by leveraging volta architectural features 1 volta architecture comprises a single variant gv100 a detailed overview of the major improvements in gv100 over earlier nvidia architectures is provided in a white paper entitled nvidia tesla v100 gpu architecture the worlds most advanced datacenter gpu  for further details on the programming features discussed in this guide please refer to the cuda c programming guide   cuda best practices the performance guidelines and best practices described in the cuda c programming guide and the cuda c best practices guide apply to all cudacapable gpu architectures programmers must primarily focus on following those recommendations to achieve the best performance the highpriority recommendations from those guides are as follows find ways to parallelize sequential code minimize data transfers between the host and the device adjust kernel launch configuration to maximize device utilization ensure global memory accesses are coalesced minimize redundant accesses to global memory whenever possible avoid long sequences of diverged execution by threads within the same warp  application compatibility before addressing specific performance tuning issues covered in this guide refer to the volta compatibility guide for cuda applications to ensure that your application is compiled in a way that is compatible with volta  volta tuning 1 streaming multiprocessor the volta streaming multiprocessor sm provides the following improvements over pascal  instruction scheduling each volta sm includes 4 warpscheduler units each scheduler handles a static set of warps and issues to a dedicated set of arithmetic instruction units instructions are performed over two cycles and the schedulers can issue independent instructions every cycle dependent instruction issue latency for core fma math operations are reduced to four clock cycles compared to six cycles on pascal as a result execution latencies of core math operations can be hidden by as few as 4 warps per sm assuming 4way instructionlevel parallelism ilp per warp many more warps are of course recommended to cover the much greater latency of memory transactions and controlflow operations similar to gp100 the gv100 sm provides 64 fp32 cores and 32 fp64 cores the gv100 sm additionally includes 64 int32 cores and 8 mixedprecision tensor cores gv100 provides up to 84 sms  independent thread scheduling the volta architecture introduces independent thread scheduling among threads in a warp this feature enables intrawarp synchronization patterns previously unavailable and simplifies code changes when porting cpu code however independent thread scheduling can also lead to a rather different set of threads participating in the executed code than intended if the developer made assumptions about warpsynchronicity 2 of previous hardware architectures when porting existing codes to volta the following three code patterns need careful attention for more details see the cuda c programming guide  to avoid data corruption applications using warp intrinsics shfl  any  all  and ballot should transition to the new safe synchronizing counterparts with the sync suffix the new warp intrinsics take in a mask of threads that explicitly define which lanes threads of a warp must participate in the warp intrinsic applications that assume reads and writes are implicitly visible to other threads in the same warp need to insert the new syncwarp warpwide barrier synchronization instruction between steps where data is exchanged between threads via global or shared memory assumptions that code is executed in lockstep or that readswrites from separate threads are visible across a warp without synchronization are invalid applications using syncthreads or the ptx barsync and their derivatives in such a way that a barrier will not be reached by some nonexited thread in the thread block must be modified to ensure that all nonexited threads reach the barrier the racecheck and synccheck tools provided by computesanitizer can help with locating violations  occupancy the maximum number of concurrent warps per sm remains the same as in pascal ie 64 and other factors influencing warp occupancy remain similar as well the register file size is 64k 32bit registers per sm the maximum registers per thread is 255 the maximum number of thread blocks per sm is 32 shared memory capacity per sm is 96kb similar to gp104 and a 50 increase compared to gp100 overall developers can expect similar occupancy as on pascal without changes to their application  integer arithmetic unlike pascal gpus the gv100 sm includes dedicated fp32 and int32 cores this enables simultaneous execution of fp32 and int32 operations applications can now interleave pointer arithmetic with floatingpoint computations for example each iteration of a pipelined loop could update addresses and load data for the next iteration while simultaneously processing the current iteration at full fp32 throughput 2 tensor core operations each tensor core performs the following operation d axb c where a b c and d are 4x4 matrices the matrix multiply inputs a and b are fp16 matrices while the accumulation matrices c and d may be fp16 or fp32 matrices when accumulating in fp32 the fp16 multiply results in a full precision product that is then accumulated using fp32 addition with the other intermediate products for a 4x4x4 matrix multiply in practice tensor cores are used to perform much larger 2d or higher dimensional matrix operations built up from these smaller elements the volta tensor cores are exposed as warplevel matrix operations in the cuda 9 c api the api exposes specialized matrix load matrix multiply and accumulate and matrix store operations to,https://docs.nvidia.com/cuda/turing-tuning-guide/index.html,turing tuning guide
turing tuning guide 1 turing tuning guide v125 pdf archive tuning cuda applications for turing the programming guide to tuning cuda applications for gpus based on the nvidia turing architecture 1 turing tuning guide  nvidia turing compute architecture turing is nvidias latest architecture for cuda compute applications turing retains and extends the same cuda programming model provided by previous nvidia architectures such as pascal and volta and applications that follow the best practices for those architectures should typically see speedups on the turing architecture without any code changes this guide summarizes the ways that an application can be finetuned to gain additional speedups by leveraging turing architectural features 1 for further details on the programming features discussed in this guide please refer to the cuda c programming guide   cuda best practices the performance guidelines and best practices described in the cuda c programming guide and the cuda c best practices guide apply to all cudacapable gpu architectures programmers must primarily focus on following those recommendations to achieve the best performance the highpriority recommendations from those guides are as follows find ways to parallelize sequential code minimize data transfers between the host and the device adjust kernel launch configuration to maximize device utilization ensure global memory accesses are coalesced minimize redundant accesses to global memory whenever possible avoid long sequences of diverged execution by threads within the same warp  application compatibility before addressing specific performance tuning issues covered in this guide refer to the turing compatibility guide for cuda applications to ensure that your application is compiled in a way that is compatible with turing  turing tuning 1 streaming multiprocessor the turing streaming multiprocessor sm is based on the same major architecture 7x as volta and provides similar improvements over pascal  instruction scheduling each turing sm includes 4 warpscheduler units each scheduler handles a static set of warps and issues to a dedicated set of arithmetic instruction units instructions are performed over two cycles and the schedulers can issue independent instructions every cycle dependent instruction issue latency for core fma math operations is four clock cycles like volta compared to six cycles on pascal as a result execution latencies of core math operations can be hidden by as few as 4 warps per sm assuming 4way instructionlevel parallelism ilp per warp or by 16 warps per sm without any instuctionlevel parallelism like volta the turing sm provides 64 fp32 cores 64 int32 cores and 8 improved mixedprecision tensor cores turing has a lower double precision throughput than volta with only 2 fp64 cores  independent thread scheduling the turing architecture features the same independent thread scheduling introduced with volta this enables intrawarp synchronization patterns previously unavailable and simplifies code changes when porting cpu code however independent thread scheduling can also lead to a rather different set of threads participating in the executed code than intended if the developer made assumptions about warpsynchronicity 2 of previous hardware architectures when porting existing codes to volta or turing the following three code patterns need careful attention for more details see the cuda c programming guide  to avoid data corruption applications using warp intrinsics shfl  any  all  and ballot should transition to the new safe synchronizing counterparts with the sync suffix the new warp intrinsics take in a mask of threads that explicitly define which lanes threads of a warp must participate in the warp intrinsic applications that assume reads and writes are implicitly visible to other threads in the same warp need to insert the new syncwarp warpwide barrier synchronization instruction between steps where data is exchanged between threads via global or shared memory assumptions that code is executed in lockstep or that readswrites from separate threads are visible across a warp without synchronization are invalid applications using syncthreads or the ptx barsync and their derivatives in such a way that a barrier will not be reached by some nonexited thread in the thread block must be modified to ensure that all nonexited threads reach the barrier the racecheck and synccheck tools provided by computesanitizer can help with locating violations  occupancy the maximum number of concurrent warps per sm is 32 on turing versus 64 on volta other factors influencing warp occupancy remain otherwise similar the register file size is 64k 32bit registers per sm the maximum registers per thread is 255 the maximum number of thread blocks per sm is 16 shared memory capacity per sm is 64kb overall developers can expect similar occupancy as on pascal or volta without changes to their application  integer arithmetic similar to volta the turing sm includes dedicated fp32 and int32 cores this enables simultaneous execution of fp32 and int32 operations applications can interleave pointer arithmetic with floatingpoint computations for example each iteration of a pipelined loop could update addresses and load data for the next iteration while simultaneously processing the current iteration at full fp32 throughput 2 tensor core operations volta introduced tensor cores to accelerate matrix multiply operations on mixed precision floating point data turing adds acceleration for integer matrix multiply operations the tensor cores are exposed as warplevel matrix operations in the cuda 10 c api the api provides specialized matrix load matrix multiply and accumulate and matrix store operations where each warp processes a small matrix fragment allowing to efficiently use tensor cores from a cudac program in practice tensor cores are used to perform much larger 2d or higher dimensional matrix operations built up from these smaller matrix fragments each tensor core performs the matrix multiplyaccumulate d a x b c the tensor cores support half precision matrix multiplication where the matrix multiply inputs a and b are fp16 matrices while the accumulation matrices c and d may be either fp16 or fp32 matrices when accumulating in fp32 the fp16 multiply results in a full precision product that is then accumulated using fp32 addition cuda 10 supports several fragment sizes 16x16x16 32x8x16 and 8x32x16 to use the tensor cores on,https://docs.nvidia.com/cuda/ampere-tuning-guide/index.html,nvidia ampere gpu architecture tuning guide
ampere tuning guide 1 nvidia ampere gpu architecture tuning guide v125 pdf archive tuning cuda applications for nvidia ampere gpu architecture the programming guide for tuning cuda applications for gpus based on the nvidia ampere gpu architecture 1 nvidia ampere gpu architecture tuning guide  nvidia ampere gpu architecture the nvidia ampere gpu architecture is nvidias latest architecture for cuda compute applications the nvidia ampere gpu architecture retains and extends the same cuda programming model provided by previous nvidia gpu architectures such as turing and volta and applications that follow the best practices for those architectures should typically see speedups on the nvidia a100 gpu without any code changes this guide summarizes the ways that an application can be finetuned to gain additional speedups by leveraging the nvidia ampere gpu architectures features 1 for further details on the programming features discussed in this guide please refer to the cuda c programming guide   cuda best practices the performance guidelines and best practices described in the cuda c programming guide and the cuda c best practices guide apply to all cudacapable gpu architectures programmers must primarily focus on following those recommendations to achieve the best performance the highpriority recommendations from those guides are as follows find ways to parallelize sequential code minimize data transfers between the host and the device adjust kernel launch configuration to maximize device utilization ensure global memory accesses are coalesced minimize redundant accesses to global memory whenever possible avoid long sequences of diverged execution by threads within the same warp  application compatibility before addressing specific performance tuning issues covered in this guide refer to the nvidia ampere gpu architecture compatibility guide for cuda applications to ensure that your application is compiled in a way that is compatible with the nvidia ampere gpu architecture  nvidia ampere gpu architecture tuning 1 streaming multiprocessor the nvidia ampere gpu architectures streaming multiprocessor sm provides the following improvements over volta and turing  occupancy the maximum number of concurrent warps per sm remains the same as in volta ie 64 for compute capability  while for compute capability  it is 48 other factors influencing warp occupancy are the register file size is 64k 32bit registers per sm the maximum number of registers per thread is 255 the maximum number of thread blocks per sm is 32 for devices of compute capability  ie a100 gpus and 16 for gpus with compute capability  for devices of compute capability  ie a100 gpus shared memory capacity per sm is 164 kb a 71 increase compared to v100s capacity of 96 kb for gpus with compute capability  shared memory capacity per sm is 100 kb for devices of compute capability  ie a100 gpus the maximum shared memory per thread block is 163 kb for gpus with compute capability  maximum shared memory per thread block is 99 kb overall developers can expect similar occupancy as on volta without changes to their application  asynchronous data copy from global memory to shared memory the nvidia ampere gpu architecture adds hardware acceleration for copying data from global memory to shared memory these copy instructions are asynchronous with respect to computation and allow users to explicitly control overlap of compute with data movement from global memory into the sm these instructions also avoid using extra registers for memory copies and can also bypass the l1 cache this new feature is exposed via the pipeline api in cuda for more information please refer to the section on async copy in the cuda c programming guide   hardware acceleration for split arrivewait barrier the nvidia ampere gpu architecture adds hardware acceleration for a split arrivewait barrier in shared memory these barriers can be used to implement fine grained thread controls producerconsumer computation pipeline and divergence code patterns in cuda these barriers can also be used alongside the asynchronous copy for more information on the arrivewait barriers refer to the arrivewait barrier section in the cuda c programming guide   warp level support for reduction operations the nvidia ampere gpu architecture adds native support for warp wide reduction operations for 32bit signed and unsigned integer operands the warp wide reduction operations support arithmetic add  min  and max operations on 32bit signed and unsigned integers and bitwise and  or and xor operations on 32bit unsigned integers for more details on the new warp wide reduction operations refer to warp reduce functions in the cuda c programming guide   improved tensor core operations the nvidia ampere gpu architecture includes new third generation tensor cores that are more powerful than the tensor cores used in volta and turing sms the new tensor cores use a larger base matrix size and add powerful new math modes including support for fp64 tensor core using new dmma instructions support for bfloat16 tensor core through hmma instructions bfloat16 format is especially effective for dl training scenarios bfloat16 provides 8bit exponent ie same range as fp32 7bit mantissa and 1 signbit support for tf32 tensor core through hmma instructions tf32 is a new 19bit tensor core format that can be easily integrated into programs for more accurate dl training than 16bit hmma formats tf32 provides 8bit exponent 10bit mantissa and 1 signbit support for bitwise and along with bitwise xor which was introduced in turing through bmma instructions the following table presents the evolution of matrix instruction sizes and supported data types for tensor cores across different gpu architecture generations instruction gpu architecture input matrix format output accumulator format matrix instruction size mxnxk hmma 16bit precision nvidia volta architecture fp16 fp16 fp32 8x8x4 nvidia turing architecture fp16 fp16 fp32 8x8x4 16x8x8 16x8x16 nvidia ampere architecture fp16 bfloat16 fp16 fp32 bfloat16 only supports fp32 as accumulator 16x8x8 16x8x16 hmma 19bit precision nvidia volta architecture na na na nvidia turing architecture na na na nvidia ampere architecture tf32 19bits fp32 16x8x4 imma integer mma nvidia volta architecture na na na nvidia turing architecture unsigned charsigned char 8bit precision,https://docs.nvidia.com/cuda/hopper-tuning-guide/index.html,hopper tuning guide
hopper tuning guide 1 nvidia hopper tuning guide v125 pdf archive tuning cuda applications for hopper gpu architecture the programming guide for tuning cuda applications for gpus based on the hopper gpu architecture 1 nvidia hopper tuning guide  nvidia hopper gpu architecture the nvidia hopper gpu architecture is nvidias latest architecture for cuda compute applications the nvidia hopper gpu architecture retains and extends the same cuda programming model provided by previous nvidia gpu architectures such as nvidia ampere gpu architecture and nvidia turing and applications that follow the best practices for those architectures should typically see speedups on the nvidia h100 gpu without any code changes this guide summarizes the ways that an application can be finetuned to gain additional speedups by leveraging the nvidia hopper gpu architectures features 1 for further details on the programming features discussed in this guide refer to the cuda c programming guide   cuda best practices the performance guidelines and best practices described in the cuda c programming guide and the cuda c best practices guide apply to all cudacapable gpu architectures programmers must primarily focus on following those recommendations to achieve the best performance the highpriority recommendations from those guides are as follows find ways to parallelize sequential code minimize data transfers between the host and the device adjust kernel launch configuration to maximize device utilization ensure that global memory accesses are coalesced minimize redundant accesses to global memory whenever possible avoid long sequences of diverged execution by threads within the same warp  application compatibility before addressing specific performance tuning issues covered in this guide refer to the hopper compatibility guide for cuda applications to ensure that your application is compiled in a way that is compatible with nvidia hopper  nvidia hopper tuning 1 streaming multiprocessor the nvidia hopper streaming multiprocessor sm provides the following improvements over turing and nvidia ampere gpu architectures  occupancy the maximum number of concurrent warps per sm remains the same as in nvidia ampere gpu architecture that is 64 and other factors influencing warp occupancy are the register file size is 64k 32bit registers per sm the maximum number of registers per thread is 255 the maximum number of thread blocks per sm is 32 for devices of compute capability  that is h100 gpus for devices of compute capability  h100 gpus shared memory capacity per sm is 228 kb a 39 increase compared to a100s capacity of 164 kb for devices of compute capability  h100 gpus the maximum shared memory per thread block is 227 kb for applications using thread block clusters it is always recommended to compute the occupancy using cudaoccupancymaxactiveclusters and launch clusterbased kernels accordingly overall developers can expect similar occupancy as on nvidia ampere gpu architecture gpus without changes to their application  tensor memory accelerator the hopper architecture builds on top of the asynchronous copies introduced by nvidia ampere gpu architecture and provides a more sophisticated asynchronous copy engine the tensor memory accelerator tma tma allows applications to transfer 1d and up to 5d tensors between global memory and shared memory in both directions as well as between the shared memory regions of different sms in the same cluster refer to thread block clusters  additionally for writes from shared memory to global memory it allows specifying element wise reduction operations such as addminmax as well as bitwise andor for most common data types this has several advantages avoids using registers for moving data between the different memory spaces avoids using sm instructions for moving data a single thread can issue large data movement instructions to the tma unit the whole block can then continue working on other instructions while the data is in flight and only wait for the data to be consumed when actually necessary enables users to write warp specialized codes where specific warps specialize on data movement between the different memory spaces while other warps only work on local data within the sm this feature will be exposed through cudamemcpyasync along with the cudabarrier and cudapipeline for synchronizing data movement  thread block clusters nvidia hopper architecture adds a new optional level of hierarchy thread block clusters that allows for further possibilities when parallelizing applications a thread block can read from write to and perform atomics in shared memory of other thread blocks within its cluster this is known as distributed shared memory as demonstrated in the cuda c programming guide  there are applications that cannot fit required data within shared memory and must use global memory instead distributed shared memory can act as an intermediate step between these two options distributed shared memory can be used by an sm simultaneously with l2 cache accesses this can benefit applications that need to communicate data between sms by utilizing the combined bandwidth of both distributed shared memory and l2 in order to achieve best performance for accesses to distributed shared memory access patterns to those described in the cuda c best practices guide for global memory should be used specifically accesses to distributed shared memory should be coalesced and aligned to 32byte segments if possible access patterns with nonunit stride should be avoided if possible which can be achieved by using local shared memory similar to what is shown in the cuda c best practices guide for shared memory  the maximum portable cluster size supported is 8 however nvidia hopper h100 gpu allows for a nonportable cluster size of 16 by opting in launching a kernel with a nonportable cluster size requires setting the cudafuncattributenonportableclustersizeallowed function attribute using larger cluster sizes may reduce the maximum number of active blocks across the gpu refer to occupancy   improved fp32 throughput devices of compute capability  have 2x more fp32 operations per cycle per sm than devices of compute capability   dynamic programming instructions the nvidia hopper architecture adds support for new instructions to accelerate dynamic programming algorithms such as the smithwaterman algorithm for sequence alignment in bioinformatics and algorithms in,https://docs.nvidia.com/cuda/ada-tuning-guide/index.html,ada tuning guide
ada tuning guide 1 nvidia ada gpu architecture tuning guide v125 pdf archive tuning cuda applications for nvidia ada gpu architecture the programming guide for tuning cuda applications for gpus based on the nvidia ada gpu architecture 1 nvidia ada gpu architecture tuning guide  nvidia ada gpu architecture the nvidia ada gpu architecture is nvidias latest architecture for cuda compute applications the nvidia ada gpu architecture retains and extends the same cuda programming model provided by previous nvidia gpu architectures such as nvidia ampere and turing and applications that follow the best practices for those architectures should typically see speedups on the nvidia ada architecture without any code changes this guide summarizes the ways that an application can be finetuned to gain additional speedups by leveraging the nvidia ada gpu architectures features 1 for further details on the programming features discussed in this guide please refer to the cuda c programming guide   cuda best practices the performance guidelines and best practices described in the cuda c programming guide and the cuda c best practices guide apply to all cudacapable gpu architectures programmers must primarily focus on following those recommendations to achieve the best performance the highpriority recommendations from those guides are as follows find ways to parallelize sequential code minimize data transfers between the host and the device adjust kernel launch configuration to maximize device utilization ensure global memory accesses are coalesced minimize redundant accesses to global memory whenever possible avoid long sequences of diverged execution by threads within the same warp  application compatibility before addressing specific performance tuning issues covered in this guide refer to the nvidia ada gpu architecture compatibility guide for cuda applications to ensure that your application is compiled in a way that is compatible with the nvidia ada gpu architecture  nvidia ada gpu architecture tuning 1 streaming multiprocessor the nvidia ada gpu architectures streaming multiprocessor sm provides the following improvements over turing and nvidia ampere gpu architectures  occupancy the maximum number of concurrent warps per sm is 48 remaining the same compared to compute capability  gpus and other factors influencing warp occupancy are the register file size is 64k 32bit registers per sm the maximum number of registers per thread is 255 the maximum number of thread blocks per sm is 24 the shared memory capacity per sm is 100 kb the maximum shared memory per thread block is 99 kb overall developers can expect similar occupancy as on compute capability  gpus without changes to their application  improved tensor core operations the nvidia ada gpu architecture includes new ada fourth generation tensor cores featuring the hopper fp8 transformer engine  improved fp32 throughput devices of compute capability  have 2x more fp32 operations per cycle per sm than devices of compute capability  while a binary compiled for  will run asis on  it is recommended to compile explicitly for  to benefit from the increased fp32 throughput 2 memory system  increased l2 capacity the nvidia ada gpu architecture increases the capacity of the l2 cache to 98304 kb in ad102 16x larger than ga102 the nvidia ada gpu architecture allows cuda users to control the persistence of data in the l2 cache for more information on the persistence of data in the l2 cache refer to the section on managing the l2 cache in the cuda c programming guide   unified shared memoryl1texture cache nvidia ada architecture features a unified l1 cache texture cache and shared memory similar to that of the nvidia ampere architecture the combined l1 cache capacity is 128 kb in the nvidia ada gpu architecture the portion of the l1 cache dedicated to shared memory known as the carveout can be selected at runtime as in previous architectures such as nvidia ampere using cudafuncsetattribute with the attribute cudafuncattributepreferredsharedmemorycarveout  the nvidia ada gpu architecture supports shared memory capacity of 0 8 16 32 64 or 100 kb per sm cuda reserves 1 kb of shared memory per thread block hence gpus with compute capability  can address up to 99 kb of shared memory in a single thread block to maintain architectural compatibility static shared memory allocations remain limited to 48 kb and an explicit optin is also required to enable dynamic allocations above this limit see the cuda c programming guide for details like the nvidia ampere and nvidia volta gpu architectures the nvidia ada gpu architecture combines the functionality of the l1 and texture caches into a unified l1texture cache that acts as a coalescing buffer for memory accesses gathering up the data requested by the threads of a warp prior to delivery of that data to the warp another benefit of its union with shared memory similar to previous architectures is improvement in terms of both latency and bandwidth 2 revision history version  initial public release added support for compute capability  1 throughout this guide volta refers to devices of compute capability  turing refers to devices of compute capability  nvidia ampere gpu architecture refers to devices of compute capability  and  nvidia ada refers to devices of compute capability  3 notices  notice this document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality condition or quality of a product nvidia corporation nvidia makes no representations or warranties expressed or implied as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein nvidia shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use this document is not a commitment to develop release or deliver any material defined below code or functionality nvidia reserves the right to make corrections modifications enhancements improvements and any other changes to this document at any time without notice customer should obtain the latest relevant,https://docs.nvidia.com/cuda/parallel-thread-execution/index.html,ptx isa
ptx isa 1 introduction v85 pdf archive parallel thread execution isa version  the programming guide to using ptx parallel thread execution and isa instruction set architecture 1 introduction this document describes ptx a lowlevel parallel thread execution virtual machine and instruction set architecture isa ptx exposes the gpu as a dataparallel computing device   scalable dataparallel computing using gpus driven by the insatiable market demand for realtime highdefinition 3d graphics the programmable gpu has evolved into a highly parallel multithreaded manycore processor with tremendous computational horsepower and very high memory bandwidth the gpu is especially wellsuited to address problems that can be expressed as dataparallel computations the same program is executed on many data elements in parallel with high arithmetic intensity the ratio of arithmetic operations to memory operations because the same program is executed for each data element there is a lower requirement for sophisticated flow control and because it is executed on many data elements and has high arithmetic intensity the memory access latency can be hidden with calculations instead of big data caches dataparallel processing maps data elements to parallel processing threads many applications that process large data sets can use a dataparallel programming model to speed up the computations in 3d rendering large sets of pixels and vertices are mapped to parallel threads similarly image and media processing applications such as postprocessing of rendered images video encoding and decoding image scaling stereo vision and pattern recognition can map image blocks and pixels to parallel processing threads in fact many algorithms outside the field of image rendering and processing are accelerated by dataparallel processing from general signal processing or physics simulation to computational finance or computational biology ptx defines a virtual machine and isa for general purpose parallel thread execution ptx programs are translated at install time to the target hardware instruction set the ptxtogpu translator and driver enable nvidia gpus to be used as programmable parallel computers  goals of ptx ptx provides a stable programming model and instruction set for general purpose parallel programming it is designed to be efficient on nvidia gpus supporting the computation features defined by the nvidia tesla architecture high level language compilers for languages such as cuda and cc generate ptx instructions which are optimized for and translated to native targetarchitecture instructions the goals for ptx include the following provide a stable isa that spans multiple gpu generations achieve performance in compiled applications comparable to native gpu performance provide a machineindependent isa for cc and other compilers to target provide a code distribution isa for application and middleware developers provide a common sourcelevel isa for optimizing code generators and translators which map ptx to specific target machines facilitate handcoding of libraries performance kernels and architecture tests provide a scalable programming model that spans gpu sizes from a single unit to many parallel units  ptx isa version  ptx isa version  introduces the following new features adds support for mmasporderedmetadata instruction  document structure the information in this document is organized into the following chapters programming model outlines the programming model ptx machine model gives an overview of the ptx virtual machine model syntax describes the basic syntax of the ptx language state spaces types and variables describes state spaces types and variable declarations instruction operands describes instruction operands abstracting the abi describes the function and call syntax calling convention and ptx support for abstracting the application binary interface abi  instruction set describes the instruction set special registers lists special registers directives lists the assembly directives supported in ptx release notes provides release notes for ptx isa versions 2x and beyond references 7542008 ieee standard for floatingpoint arithmetic isbn 9780738157528 2008 httpieeexploreieeeorgservletopacpunumber4610933 the opencl specification version  document revision 44 june 1 2011 httpwwwkhronosorgregistryclspecsopencl11pdf cuda programming guide httpsdocsnvidiacomcudacudacprogrammingguideindexhtml cuda dynamic parallelism programming guide httpsdocsnvidiacomcudacudacprogrammingguideindexhtmlcudadynamicparallelism cuda atomicity requirements httpsnvidiagithubiocccllibcudacxxextendedapimemorymodelhtmlatomicity ptx writers guide to interoperability httpsdocsnvidiacomcudaptxwritersguidetointeroperabilityindexhtml 2 programming model  a highly multithreaded coprocessor the gpu is a compute device capable of executing a very large number of threads in parallel it operates as a coprocessor to the main cpu or host in other words dataparallel computeintensive portions of applications running on the host are offloaded onto the device more precisely a portion of an application that is executed many times but independently on different data can be isolated into a kernel function that is executed on the gpu as many different threads to that effect such a function is compiled to the ptx instruction set and the resulting kernel is translated at install time to the target gpu instruction set  thread hierarchy the batch of threads that executes a kernel is organized as a grid a grid consists of either cooperative thread arrays or clusters of cooperative thread arrays as described in this section and illustrated in figure 1 and figure 2  cooperative thread arrays ctas implement cuda thread blocks and clusters implement cuda thread block clusters 1 cooperative thread arrays the parallel thread execution ptx programming model is explicitly parallel a ptx program specifies the execution of a given thread of a parallel thread array a cooperative thread array  or cta is an array of threads that execute a kernel concurrently or in parallel threads within a cta can communicate with each other to coordinate the communication of the threads within the cta one can specify synchronization points where threads wait until all threads in the cta have arrived each thread has a unique thread identifier within the cta programs use a data parallel decomposition to partition inputs work and results across the threads of the cta each cta thread uses its thread identifier to determine its assigned role assign specific input and output positions compute addresses and select work to perform the thread identifier is a threeelement vector tid  with elements tidx  tidy  and tidz that specifies the threads position within a 1d 2d or 3d cta each thread identifier component ranges from zero up to the number of,https://docs.nvidia.com/cuda/ptx-writers-guide-to-interoperability/index.html,ptx interoperability
ptx interoperability 1 introduction v125 pdf archive ptx writers guide to interoperability the guide to writing abicompliant ptx 1 introduction this document defines the application binary interface abi for the cuda architecture when generating ptx by following the abi external developers can generate compliant ptx code that can be linked with other code ptx is a lowlevel parallelthreadexecution virtual machine and isa instruction set architecture ptx can be output from multiple tools or written directly by developers ptx is meant to be gpuarchitecture independent so that the same code can be reused for different gpu architectures for more information on ptx refer to the latest version of the ptx isa reference document  there are multiple cuda architecture families each with their own isa eg sm 5x is the maxwell family sm 6x is the pascal family this document describes the highlevel abi for all architectures programs conforming to an abi are expected to be executed on the appropriate architecture gpu and can assume that instructions from that isa are available 2 data representation  fundamental types the below table shows the native scalar ptx types that are supported any ptx producer must use these sizes and alignments in order for its ptx to be compatible with ptx generated by other producers ptx also supports native vector types which are discussed in aggregates and unions  the sizes of types are defined by the host for example pointer size and long int size are dictated by the hosts abi ptx has an addresssize directive that specifies the address size used throughout the ptx code the size of pointers is 32 bits on a 32bit host or 64 bits on a 64bit host however addresses of the local and shared memory spaces are always 32 bits in size during separate compilation we store info about the host platform in each object file the linker will fail to link object files generated for incompatible host platforms ptx type size bytes align bytes hardware representation b8 1 1 untyped byte b16 2 2 untyped halfword b32 4 4 untyped word b64 8 8 untyped doubleword s8 1 1 signed integral byte s16 2 2 signed integral halfword s32 4 4 signed integral word s64 8 8 signed integral doubleword u8 1 1 unsigned integral byte u16 2 2 unsigned integral halfword u32 4 4 unsigned integral word u64 8 8 unsigned integral doubleword f16 2 2 ieee half precision f32 4 4 ieee single precision f64 8 8 ieee double precision  aggregates and unions beyond the scalar types ptx also supports nativevector types of these scalar types with both its vector syntax and its bytearray syntax for scalar types with a size no greater than four bytes vector types with 1 2 3 and 4 elements exist for all other types only 1 and 2 element vector types exist all aggregates and unions can be supported in ptx with its bytearray syntax the following are the sizeandalignment rules for all aggregates and unions for a nonnativevector type an entire aggregate or union is aligned on the same boundary as its most strictly aligned member this rule is not followed if the alignments are defined by the input language for example in opencl builtin vector data types have their alignment set to the size of the builtin data type in bytes for a native vector type discussed at the start of this section the alignment is defined as follows for the definitions below the native vector has n elements and has an element type t for a vector with an odd number of elements its alignment is the same as its member alignoft for a vector with an even number of elements its alignment is set to number of elements times the alignment of its member nalignoft each member is assigned to the lowest available offset with the appropriate alignment this may require internal padding depending on the previous member the size of an aggregate or union if necessary is increased to make it a multiple of the alignment of the aggregate or union this may require tail padding depending on the last member  bit fields c structure and union definitions may have bit fields that define integral objects with a specified number of bits bit field type width w range signed char 1 to 8 2 w1 to 2 w1 1 unsigned char 1 to 8 0 to 2 w 1 signed short 1 to 16 2 w1 to 2 w1 1 unsigned short 1 to 16 0 to 2 w 1 signed int 1 to 32 2 w1 to 2 w1 1 unsigned int 1 to 32 0 to 2 w 1 signed long long 1 to 64 2 w1 to 2 w1 1 unsigned long long 1 to 64 0 to 2 w 1 current gpus only support littleendian memory so the below assumes littleendian layout the following are rules that apply to bit fields plain bit fields neither signed nor unsigned is specified are treated as signed when no type is provided eg signed 6 is specified the type defaults to int bit fields obey the same size and alignment rules as other structure and union members with the following modifications bit fields are allocated in memory from right to left least to more significant for little endian a bit field must entirely reside in a storage unit appropriate for its declared type a bit field should never cross its unit boundary bit fields may share a storage unit with other structure and union members including members that are not bit fields as long as there is enough space within the storage unit unnamed bit fields do not affect the alignment of a structure or union zerolength bit fields force the alignment of the following member of a structure to the next alignment boundary corresponding to the bitfield type an unnamed zerolength bit field will not force the external alignment of the structure to that boundary if an,https://docs.nvidia.com/cuda/inline-ptx-assembly/index.html,inline ptx assembly
inline ptx assembly in cuda 1 using inline ptx assembly in cuda v125 pdf archive inline ptx assembly in cuda the reference guide for inlining ptx parallel thread execution assembly statements into cuda 1 using inline ptx assembly in cuda the nvidia cuda programming environment provides a parallel thread execution ptx instruction set architecture isa for using the gpu as a dataparallel computing device for more information on the ptx isa refer to the latest version of the ptx isa reference document  this application note describes how to inline ptx assembly language statements into cuda code  assembler asm statements assembler statements asm  provide a way to insert arbitrary ptx code into your cuda program a simple example is asm membargl this inserts a ptx membargl into your generated ptx code at the point of the asm statement 1 parameters an asm statement becomes more complicated and more useful when we pass values in and out of the asm the basic syntax is as follows asm templatestring constraint output constraint input where you can have multiple input or output operands separated by commas the template string contains ptx instructions with references to the operands multiple ptx instructions can be given by separating them with semicolons a simple example is as follows asm adds32 0 1 2 r i r j  r k each n in the template string is an index into the following list of operands in text order so 0 refers to the first operand 1 to the second operand and so on since the output operands are always listed ahead of the input operands they are assigned the smallest indices this example is conceptually equivalent to the following add  s32 i  j  k note that the numbered references in the string can be in arbitrary order the following is equivalent to the above example asm adds32 0 2 1 r i r k  r j you can also repeat a reference eg asm adds32 0 1 1 r i r k is conceptually add  s32 i  k  k if there is no input operand you can drop the final colon eg asm movs32 0 2 r i if there is no output operand the colon separators are adjacent eg asm movs32 r1 0 r i if you want the in a ptx instruction then you should escape it with double  eg asm movu32 0 clock r x the above was simplified to explain the ordering of the string references in reality the operand values are passed via whatever mechanism the constraint specifies the full list of constraints will be explained later but the r constraint refers to a 32bit integer register so the earlier example asm statement asm adds32 0 1 2 r i r j  r k produces the following code sequence in the output generated by the compiler ld  s32 r1  j ld  s32 r2  k add  s32 r3  r1  r2 st  s32 i  r3 this is where the distinction between input and output operands becomes important the input operands are loaded into registers before the asm statement then the result register is stored to the output operand the modifier in r specifies that the register is written to there is also available a modifier that specifies the register is both read and written eg asm adds32 0 0 1 r i r j multiple instructions can be combined into a single asm statement basically anything legal can be put into the asm string multiple instructions can be split across multiple lines by making use of ccs implicit string concatenation both c style line end comments and classical cstyle comments can be interspersed with these strings to generate readable output in the ptx intermediate file it is best practice to terminate each instruction string except the last one with nt for example a cube routine could be written as device int cube int x int y asm reg u32 t1 nt temp reg t1 mullou32 t1 1 1 nt t1 x x mullou32 0 t1 1 y t1 x r y r x return y if an output operand is conditionally updated by the asm instructions then the modifier should be used there is an implicit use of the output operand in such a case for example device int cond int x int y 0 asm nt reg pred p nt setpeqs32 p 1 34 nt x 34 p movs32 0 1 nt set y to 1 if true conceptually y x341y r y r x return y 2 constraints there is a separate constraint letter for each ptx register type h  u16 reg r  u32 reg l  u64 reg f  f32 reg d  f64 reg example asm cvtf32s64 0 1 f x l y generates ld  s64 rd1  y cvt  f32  s64 f1  rd1 st  f32 x  f1 the constraint n may be used for immediate integer operands with a known value example asm addu32 0 0 1 r x n 42 generates add  u32 r1  r1  42 the constraint c can be used for operand of type array of const char where the array contents are known at compile time it is intended to allow customization of ptx instruction modes based on compile time computation see examples here is the specification for the c constraint c constant expression the constantexpression is evaluated during compilation and shall generate the address of a variable v  where v has static storage duration  v has type array of const char v is constantinitialized  if v is a static class member then v s initializing declaration is the declaration within the class during translation the compiler will replace a reference to the operand within the assembler template with the contents of v s initializer except for the last trailing,https://docs.nvidia.com/cuda/cuda-runtime-api/index.html,cuda runtime api
1 difference between the driver and runtime apis 2 api synchronization behavior 3 stream synchronization behavior 4 graph object thread safety 5 rules for version mixing 6 modules  device management  device management deprecated  thread management deprecated  error handling  stream management  event management  external resource interoperability  execution control  execution control deprecated  occupancy  memory management  memory management deprecated  stream ordered memory allocator  unified addressing  peer device memory access  opengl interoperability  opengl interoperability deprecated  direct3d 9 interoperability  direct3d 9 interoperability deprecated  direct3d 10 interoperability  direct3d 10 interoperability deprecated  direct3d 11 interoperability  direct3d 11 interoperability deprecated  vdpau interoperability  egl interoperability  graphics interoperability  texture object management  surface object management  version management  graph management  driver entry point access  c api routines  interactions with the cuda driver api  profiler control  data types used by cuda runtime 7 data structures  cudaoccupancyb2dhelper  cudaaccesspolicywindow  cudaarraymemoryrequirements  cudaarraysparseproperties  cudaasyncnotificationinfot  cudachannelformatdesc  cudachildgraphnodeparams  cudaconditionalnodeparams  cudadeviceprop  cudaeglframe  cudaeglplanedesc  cudaeventrecordnodeparams  cudaeventwaitnodeparams  cudaextent  cudaexternalmemorybufferdesc  cudaexternalmemoryhandledesc  cudaexternalmemorymipmappedarraydesc  cudaexternalsemaphorehandledesc  cudaexternalsemaphoresignalnodeparams  cudaexternalsemaphoresignalnodeparamsv2  cudaexternalsemaphoresignalparams  cudaexternalsemaphoresignalparamsv1  cudaexternalsemaphorewaitnodeparams  cudaexternalsemaphorewaitnodeparamsv2  cudaexternalsemaphorewaitparams  cudaexternalsemaphorewaitparamsv1  cudafuncattributes  cudagraphedgedata  cudagraphexecupdateresultinfo  cudagraphinstantiateparams  cudagraphkernelnodeupdate  cudagraphnodeparams  cudahostnodeparams  cudahostnodeparamsv2  cudaipceventhandlet  cudaipcmemhandlet  cudakernelnodeparams  cudakernelnodeparamsv2  cudalaunchattribute  cudalaunchattributevalue  cudalaunchconfigt  cudalaunchmemsyncdomainmap  cudalaunchparams  cudamemaccessdesc  cudamemallocnodeparams  cudamemallocnodeparamsv2  cudamemcpy3dparms  cudamemcpy3dpeerparms  cudamemcpynodeparams  cudamemfreenodeparams  cudamemlocation  cudamempoolprops  cudamempoolptrexportdata  cudamemsetparams  cudamemsetparamsv2  cudapitchedptr  cudapointerattributes  cudapos  cudaresourcedesc  cudaresourceviewdesc  cudatexturedesc  cuuuidst 8 data fields 9 deprecated list 1 difference between the driver and runtime apis 2 api synchronization behavior 3 stream synchronization behavior 4 graph object thread safety 5 rules for version mixing 6 modules  data types used by cuda driver  error handling  initialization  version management  device management  device management deprecated  primary context management  context management  context management deprecated  module management  module management deprecated  library management  memory management  virtual memory management  stream ordered memory allocator  multicast object management  unified addressing  stream management  event management  external resource interoperability  stream memory operations  execution control  execution control deprecated  graph management  occupancy  texture reference management deprecated  surface reference management deprecated  texture object management  surface object management  tensor map object managment  peer context memory access  graphics interoperability  driver entry point access  coredump attributes control api  green contexts  profiler control deprecated  profiler control  opengl interoperability 1 opengl interoperability deprecated  direct3d 9 interoperability 1 direct3d 9 interoperability deprecated  direct3d 10 interoperability 1 direct3d 10 interoperability deprecated  direct3d 11 interoperability 1 direct3d 11 interoperability deprecated  vdpau interoperability  egl interoperability 7 data structures  cuaccesspolicywindowv1  cuarraymapinfov1  cuasyncnotificationinfo  cuctxcigparam  cuctxcreateparams  cudaarray3ddescriptorv2  cudaarraydescriptorv2  cudaarraymemoryrequirementsv1  cudaarraysparsepropertiesv1   cudachildgraphnodeparams  cudaconditionalnodeparams  cudaeventrecordnodeparams  cudaeventwaitnodeparams  cudaextsemsignalnodeparamsv1  cudaextsemsignalnodeparamsv2  cudaextsemwaitnodeparamsv1  cudaextsemwaitnodeparamsv2  cudaexternalmemorybufferdescv1  cudaexternalmemoryhandledescv1  cudaexternalmemorymipmappedarraydescv1  cudaexternalsemaphorehandledescv1  cudaexternalsemaphoresignalparamsv1  cudaexternalsemaphorewaitparamsv1  cudagraphinstantiateparams  cudahostnodeparamsv1  cudahostnodeparamsv2  cudakernelnodeparamsv1  cudakernelnodeparamsv2  cudakernelnodeparamsv3  cudalaunchparamsv1  cudamemallocnodeparamsv1  cudamemallocnodeparamsv2  cudamemfreenodeparams  cudamemcpy2dv2  cudamemcpy3dpeerv1  cudamemcpy3dv2  cudamemcpynodeparams  cudamemsetnodeparamsv1  cudamemsetnodeparamsv2  cudapointerattributep2ptokensv1  cudaresourcedescv1  cudaresourceviewdescv1  cudatexturedescv1  cudevpropv1  cudevresource  cudevsmresource  cueglframev1  cuexecaffinityparamv1  cuexecaffinitysmcountv1  cugraphedgedata  cugraphexecupdateresultinfov1  cugraphnodeparams  cuipceventhandlev1  cuipcmemhandlev1  culaunchattribute  culaunchattributevalue  culaunchconfig  culaunchmemsyncdomainmap  cumemaccessdescv1  cumemallocationpropv1  cumemfabrichandlev1  cumemlocationv1  cumempoolpropsv1  cumempoolptrexportdatav1  cumulticastobjectpropv1  custreambatchmemopparamsv1  cutensormap 8 data fields 9 deprecated list,https://docs.nvidia.com/cuda/cuda-math-api/index.html,cuda math api
cuda math api reference manual cuda math api reference manual v125 pdf archive cuda math api reference manual cuda mathematical functions are always available in device code host implementations of the common mathematical functions are mapped in a platformspecific way to standard math library functions provided by the host compiler and respective host libm where available some functions not available with the host compilers are implemented in crtmathfunctionshpp header file for example see erfinv  other less common functions like rhypot  cylbesseli0 are only available in device code cuda math device functions are nothrow for wellformed cuda programs note that many floatingpoint and integer functions names are overloaded for different argument types for example the log function has the following prototypes double log double x float log float x float logf float x note also that due to implementation constraints certain math functions from std namespace may be callable in device code even via explicitly qualified std names however such use is discouraged since this capability is unsupported unverified undocumented not portable and may change without notice 1 fp8 intrinsics 2 half precision intrinsics 3 bfloat16 precision intrinsics 4 single precision mathematical functions 5 single precision intrinsics 6 double precision mathematical functions 7 double precision intrinsics 8 type casting intrinsics 9 integer mathematical functions 10 integer intrinsics 11 simd intrinsics 12 structs 13 notices privacy policy manage my privacy do not sell or share my data terms of service accessibility corporate policies product security contact copyright 20072024 nvidia corporation affiliates all rights reserved last updated on jul 1 2024,https://docs.nvidia.com/cuda/cublas/index.html,cublas
cublas 1 introduction v125 pdf archive cublas the api reference guide for cublas the cuda basic linear algebra subroutine library 1 introduction the cublas library is an implementation of blas basic linear algebra subprograms on top of the nvidiacuda runtime it allows the user to access the computational resources of nvidia graphics processing unit gpu the cublas library exposes four sets of apis the cublas api  which is simply called cublas api in this document starting with cuda  the cublasxt api starting with cuda  and the cublaslt api starting with cuda  the cublasdx api not shipped with the cuda toolkit to use the cublas api the application must allocate the required matrices and vectors in the gpu memory space fill them with data call the sequence of desired cublas functions and then upload the results from the gpu memory space back to the host the cublas api also provides helper functions for writing and retrieving data from the gpu to use the cublasxt api the application may have the data on the host or any of the devices involved in the computation and the library will take care of dispatching the operation to and transferring the data to one or multiple gpus present in the system depending on the user request the cublaslt is a lightweight library dedicated to general matrixtomatrix multiply gemm operations with a new flexible api this library adds flexibility in matrix data layouts input types compute types and also in choosing the algorithmic implementations and heuristics through parameter programmability after a set of options for the intended gemm operation are identified by the user these options can be used repeatedly for different inputs this is analogous to how cufft and fftw first create a plan and reuse for same size and type ffts with different input data  data layout for maximum compatibility with existing fortran environments the cublas library uses columnmajor storage and 1based indexing since c and c use rowmajor storage applications written in these languages can not use the native array semantics for twodimensional arrays instead macros or inline functions should be defined to implement matrices on top of onedimensional arrays for fortran code ported to c in mechanical fashion one may chose to retain 1based indexing to avoid the need to transform loops in this case the array index of a matrix element in row i and column j can be computed via the following macro define idx2fijld j1ldi1 here ld refers to the leading dimension of the matrix which in the case of columnmajor storage is the number of rows of the allocated matrix even if only a submatrix of it is being used for natively written c and c code one would most likely choose 0based indexing in which case the array index of a matrix element in row i and column j can be computed via the following macro define idx2cijld jldi  new and legacy cublas api starting with version  the cublas library provides a new api in addition to the existing legacy api this section discusses why a new api is provided the advantages of using it and the differences with the existing legacy api warning the legacy cublas api is deprecated and will be removed in future release the new cublas library api can be used by including the header file cublasv2h  it has the following features that the legacy cublas api does not have the handle to the cublas library context is initialized using the function and is explicitly passed to every subsequent library function call this allows the user to have more control over the library setup when using multiple host threads and multiple gpus this also allows the cublas apis to be reentrant the scalars alpha and beta can be passed by reference on the host or the device instead of only being allowed to be passed by value on the host this change allows library functions to execute asynchronously using streams even when alpha and beta are generated by a previous kernel when a library routine returns a scalar result it can be returned by reference on the host or the device instead of only being allowed to be returned by value only on the host this change allows library routines to be called asynchronously when the scalar result is generated and returned by reference on the device resulting in maximum parallelism the error status cublasstatust is returned by all cublas library function calls this change facilitates debugging and simplifies software development note that cublasstatus was renamed cublasstatust to be more consistent with other types in the cublas library the cublasalloc and cublasfree functions have been deprecated this change removes these unnecessary wrappers around cudamalloc and cudafree  respectively the function cublassetkernelstream was renamed cublassetstream to be more consistent with the other cuda libraries the legacy cublas api explained in more detail in using the cublas legacy api  can be used by including the header file cublash  since the legacy api is identical to the previously released cublas library api existing applications will work out of the box and automatically use this legacy api without any source code changes the current and the legacy cublas apis cannot be used simultaneously in a single translation unit including both cublash and cublasv2h header files will lead to compilation errors due to incompatible symbol redeclarations in general new applications should not use the legacy cublas api and existing applications should convert to using the new api if it requires sophisticated and optimal stream parallelism or if it calls cublas routines concurrently from multiple threads for the rest of the document the new cublas library api will simply be referred to as the cublas library api as mentioned earlier the interfaces to the legacy and the cublas library apis are the header file cublash and cublasv2h  respectively in addition applications using the cublas library need to link against the dso cublasso for linux the dll cublasdll for,https://docs.nvidia.com/cuda/cudla-api/index.html,cudla api
here is a list of all modules data types used by cudla driver cudla api,https://docs.nvidia.com/cuda/nvblas/index.html,nvblas
nvblas 1 introduction v125 pdf archive nvblas the user guide for nvblas dropin blas replacement multigpus accelerated 1 introduction the nvblas library is a gpuaccelerated libary that implements blas basic linear algebra subprograms it can accelerate most blas level3 routines by dynamically routing blas calls to one or more nvidia gpus present in the system when the charateristics of the call make it speed up on a gpu 2 nvblas overview the nvblas library is built on top of the cublas library using only the cublasxt api refer to the cublasxt api section of the cublas documentation for more details nvblas also requires the presence of a cpu blas lirbary on the system currently nvblas intercepts only compute intensive blas level3 calls see table below depending on the charateristics of those blas calls nvblas will redirect the calls to the gpus present in the system or to cpu that decision is based on a simple heuristic that estimates if the blas call will execute for long enough to amortize the pci transfers of the input and output data to the gpu because nvblas does not support all standard blas routines it might be necessary to associate it with an existing full blas library please refer to the usage section for more details 3 gpu accelerated routines nvblas offloads only the computeintensive blas3 routines which have the best potential for acceleration on gpus the following table shows the currently supported routines routine types operation gemm sdcz multiplication of 2 matrices syrk sdcz symmetric rankk update herk cz hermitian rankk update syr2k sdcz symmetric rank2k update her2k cz hermitian rank2k update trsm sdcz triangular solve with multiple righthand sides trmm sdcz triangular matrixmatrix multiplication symm sdcz symmetric matrixmatrix multiplication hemm cz hermitian matrixmatrix multiplication 4 blas symbols interception standard blas library implementations usually expose multiple symbols for the same routines lets say func is a blas routine name func orand func are usually defined as extern symbols some blas libraries might also expose some symbols with a proprietary appended prefix nvblas intercepts only the symbols func and func  the user needs to make sure that the application intended to be gpuaccelerated by nvblas actually calls those defined symbols any other symbols will not be intercepted and the original blas routine will be executed for those cases 5 device memory support starting with release  data can be located on any gpu device even on gpu devices that are not configured to be part of the computation when any of the data is located on a gpu the computation will be exclusively done on gpu whatever the size of the problem also this feature has to be used with caution the user has to be sure that the blas call will indeed be intercepted by nvblas otherwise it will result in a crash when the cpu blas tries to execute it 6 security precaution because the nvblas library relies on a symbols interception mechanism it is essential to make sure it has not been compromised in that regard nvblas should never be used from a process running at elevated privileges such as administrator on windows or root on linux 7 configuration because nvblas is a dropin replacement of blas it must be configured through an ascii text file that describes how many and which gpus can participate in the intercepted blas calls the configuration file is parsed at the time of the loading of the library the format of the configuration file is based on keywords optionally followed by one or more userdefined parameters at most one keyword per line is allowed blank lines or lines beginning with the character are ignored  nvblasconfigfile environment variable the location and name of the configuration file must be defined by the environment variable nvblasconfigfile  by default if nvblasconfigfile is not defined nvblas will try to open the file nvblasconf in the current directory for a safe use of nvblas the configuration file should have have restricted write permissions  configuration keywords the configuration keywords syntax is described in the following subsections 1 nvblaslogfile this keyword defines the file where nvblas should print status and error messages by default if not defined the standard error output file eg stderr will be used it is advised to define this keyword early in the configuration to capture errors in parsing that file itself 2 nvblastracelogenabled when this keyword is defined every intercepted blas calls will be logged into the nvblaslogfile this feature even though intrusive can be useful for debugging purposes 3 nvblascpublaslib this keyword defines the cpu blas dynamic library file for example so file on linux or dll on windows that nvblas should open to find the cpu blas symbols definitions this keyword must be defined for nvblas to work because cpu blas libraries are often composed of multiple files even though this keyword is set to the full path to the main file of the cpu library it might still be necessary to define the right path to find the rest of the library files in the environment of your system on linux this can be done by setting the environment variable ldlibrarypath whereas on windows this can be done by setting the environment variable path  for a safe use of nvblas the following precautions are strongly advised the cpu blas library should be located where ordinary users do not have write permissions the path specified should be absolute not relative 4 nvblasgpulist this keyword defines the list of gpus that should participate in the computation of the intercepted blas calls if not defined only gpu device 0 is used since that is normally the most computecapable gpu installed in the system this keyword can be set to a list of device numbers separated by blank characters also the following wildcard keywords are also accepted for simplicity keyword meaning all all computecapable gpus detected on the system will be used by nvblas all0 gpu device 0 and all others gpus detected that,https://docs.nvidia.com/cuda/nvjpeg/index.html,nvjpeg
nvjpeg 1 introduction v125 pdf archive nvjpeg a gpu accelerated jpeg codec library 1 introduction  nvjpeg decoder the nvjpeg library provides highperformance gpu accelerated jpeg decoding functionality for image formats commonly used in deep learning and hyperscale multimedia applications the library offers single and batched jpeg decoding capabilities which efficiently utilize the available gpu resources for optimum performance and the flexibility for users to manage the memory allocation needed for decoding the nvjpeg library enables the following functions use the jpeg image data stream as input retrieve the width and height of the image from the data stream and use this retrieved information to manage the gpu memory allocation and the decoding a dedicated api is provided for retrieving the image information from the raw jpeg image data stream note throughout this document the terms cpu and host are used synonymously similarly the terms gpu and device are synonymous the nvjpeg library supports the following jpeg options baseline and progressive jpeg decodingencoding 8 bits per pixel huffman bitstream decoding upto 4 channel jpeg bitstreams 8 and 16bit quantization tables the following chroma subsampling for the 3 color channels y cb cr y u v 444 422 420 440 411 410 features hybrid decoding using both the cpu ie host and the gpu ie device hardware acceleration for baseline jpeg decode on supported platforms  input to the library is in the host memory and the output is in the gpu memory single image and batched image decoding single phase and multiple phases decoding color space conversion userprovided memory manager for the device and pinned host memory allocations  nvjpeg encoder the encoding functions of the nvjpeg library perform gpuaccelerated compression of users image data to the jpeg bitstream user can provide input data in a number of formats and colorspaces and control the encoding process with parameters encoding functionality will allocate temporary buffers using userprovided memory allocator before calling the encoding functions the user should perform a few prerequisite steps using the helper functions described in nvjpeg encoder helper api reference   thread safety not all nvjpeg types are thread safe when using decoder apis across multiple threads the following decoder types should be instantiated separately for each thread nvjpegjpegstreamt  nvjpegjpegstatet  nvjpegbufferdevicet  nvjpegbufferpinnedt when using encoder apis across multiple threads nvjpegencoderstatet should be instantiated separately for each thread for userprovided allocators inputs to nvjpegcreateex  the user needs to ensure thread safety  multigpu support the nvjpeg states and handles are bound to the device that was set as current during their creation using these states and handles with another device set as current is undefined the user is responsible of keeping track of the current device  hardware acceleration hardware accelerated jpeg decode is available on the following gpus a100 a30 h100 platforms which support hardware accelerated jpeg decode windows linux x8664 powerpc arm64 2 jpeg decoding  using jpeg decoding the nvjpeg library provides functions for both the decoding of a single image and batched decoding of multiple images 1 single image decoding for singleimage decoding you provide the data size and a pointer to the file data and the decoded image is placed in the output buffer to use the nvjpeg library start by calling the helper functions for initialization create nvjpeg library handle with one of the helper functions nvjpegcreatesimple or nvjpegcreateex  create jpeg state with the helper function nvjpegjpegstatecreate  see nvjpeg type declarations and nvjpegjpegstatecreate  the following helper functions are available in the nvjpeg library nvjpegstatust nvjpeggetpropertylibrarypropertytype type int value deprecated nvjpegstatust nvjpegcreatenvjpegbackendt backend nvjpeghandlet handle  nvjpegdevallocator allocator nvjpegstatust nvjpegcreatesimplenvjpeghandlet handle nvjpegstatust nvjpegcreateexnvjpegbackendt backend nvjpegdevallocatort devallocator nvjpegpinnedallocatort pinnedallocator unsigned int flags nvjpeghandlet handle nvjpegstatust nvjpegdestroynvjpeghandlet handle nvjpegstatust nvjpegjpegstatecreatenvjpeghandlet handle nvjpegjpegstatet jpeghandle nvjpegstatust nvjpegjpegstatedestroynvjpegjpegstate handle other helper functions such as nvjpegset and nvjpegget can be used to configure the library functionality on perhandle basis refer to the helper api reference for more details retrieve the width and height information from the jpegencoded image by using the nvjpeggetimageinfo function below is the signature of nvjpeggetimageinfo function nvjpegstatust nvjpeggetimageinfo nvjpeghandlet handle  const unsigned char data  sizet length  int ncomponents  nvjpegchromasubsamplingt subsampling  int widths  int heights for each image to be decoded pass the jpeg data pointer and data length to the above function the nvjpeggetimageinfo function is thread safe one of the outputs of the above nvjpeggetimageinfo function is nvjpegchromasubsamplingt  this parameter is an enum type and its enumerator list is composed of the chroma subsampling property retrieved from the jpeg image see nvjpeg chroma subsampling  use the nvjpegdecode function in the nvjpeg library to decode this single jpeg image see the signature of this function below nvjpegstatust nvjpegdecode nvjpeghandlet handle  nvjpegjpegstatet jpeghandle  const unsigned char data  sizet length  nvjpegoutputformatt outputformat  nvjpegimaget destination  cudastreamt stream in the above nvjpegdecode function the parameters nvjpegoutputformatt  nvjpegimaget  and cudastreamt can be used to set the output behavior of the nvjpegdecode function you provide the cudastreamt parameter to indicate the stream to which your asynchronous tasks are submitted the nvjpegoutputformatt parameter the nvjpegoutputformatt parameter can be set to one of the outputformat settings below outputformat meaning nvjpegoutputunchanged return the decoded image planar format nvjpegoutputrgb convert to planar rgb nvjpegoutputbgr convert to planar bgr nvjpegoutputrgbi convert to interleaved rgb nvjpegoutputbgri convert to interleaved bgr nvjpegoutputy return the y component only nvjpegoutputyuv return in the yuv planar format nvjpegoutputunchangediu16 return the decoded image interleaved format for example if outputformat is set to nvjpegoutputy or nvjpegoutputrgbi  or nvjpegoutputbgri then the output is written only to channel0 of nvjpegimaget  and the other channels are not touched alternately in the case of planar output the data is written to the corresponding channels of the nvjpegimaget destination structure finally in the case of grayscale jpeg and rgb output the luminance is used to create the grayscale rgb the below table explains the combinations of the output formats and the number of,https://docs.nvidia.com/cuda/cufft/index.html,cufft
cufft 1 introduction v125 pdf archive cufft api reference the api reference guide for cufft the cuda fast fourier transform library 1 introduction this document describes cufft the nvidia cuda fast fourier transform fft product it consists of two separate libraries cufft and cufftw the cufft library is designed to provide high performance on nvidia gpus the cufftw library is provided as a porting tool to enable users of fftw to start using nvidia gpus with a minimum amount of effort the fft is a divideandconquer algorithm for efficiently computing discrete fourier transforms of complex or realvalued data sets it is one of the most important and widely used numerical algorithms in computational physics and general signal processing the cufft library provides a simple interface for computing ffts on an nvidia gpu which allows users to quickly leverage the floatingpoint power and parallelism of the gpu in a highly optimized and tested fft library the cufft product supports a wide range of fft inputs and options efficiently on nvidia gpus this version of the cufft library supports the following features algorithms highly optimized for input sizes that can be written in the form 2a times 3b times 5c times 7d  in general the smaller the prime factor the better the performance ie powers of two are fastest an oleft nlog n right algorithm for every input data size halfprecision 16bit floating point singleprecision 32bit floating point and doubleprecision 64bit floating point transforms of lower precision have higher performance complex and realvalued input and output real valued input or output require less computations and data than complex values and often have faster time to solution types supported are c2c complex input to complex output r2c real input to complex output c2r symmetric complex input to real output 1d 2d and 3d transforms execution of multiple 1d 2d and 3d transforms simultaneously these batched transforms have higher performance than single transforms inplace and outofplace transforms arbitrary intra and interdimension element strides strided layout fftw compatible data layout execution of transforms across multiple gpus streamed execution enabling asynchronous computation and data movement the cufftw library provides the fftw3 api to facilitate porting of existing fftw applications please note that starting from cuda  the minimum supported gpu architecture is sm35 see deprecated functionality  2 using the cufft api this chapter provides a general overview of the cufft library api for more complete information on specific functions see cufft api reference  users are encouraged to read this chapter before continuing with more detailed descriptions the discrete fourier transform dft maps a complexvalued vector xk time domain into its frequency domain representation given by xk sumlimitsn 0n 1xne2pi ifracknn where xk is a complexvalued vector of the same size this is known as a forward dft if the sign on the exponent of e is changed to be positive the transform is an inverse transform depending on n  different algorithms are deployed for the best performance the cufft api is modeled after fftw  which is one of the most popular and efficient cpubased fft libraries cufft provides a simple configuration mechanism called a plan that uses internal building blocks to optimize the transform for the given configuration and the particular gpu hardware selected then when the execution function is called the actual transform takes place following the plan of execution the advantage of this approach is that once the user creates a plan the library retains whatever state is needed to execute the plan multiple times without recalculation of the configuration this model works well for cufft because different kinds of ffts require different thread configurations and gpu resources and the plan interface provides a simple way of reusing configurations computing a number batch of onedimensional dfts of size nx using cufft will typically look like this define nx 256 define batch 10 define rank 1  cuffthandle plan cufftcomplex data  cudamalloc void data  sizeof cufftcomplex nx batch cufftplanmany plan  rank  nx  iembed  istride  idist  oembed  ostride  odist  cufftc2c  batch  cufftexecc2c plan  data  data  cufftforward cudadevicesynchronize  cufftdestroy plan cudafree data  accessing cufft the cufft and cufftw libraries are available as shared libraries they consist of compiled programs ready for users to incorporate into applications with the compiler and linker cufft can be downloaded from httpsdevelopernvidiacomcufft  by selecting download cuda production release users are all able to install the package containing the cuda toolkit sdk code samples and development drivers the cuda toolkit contains cufft and the samples include simplecufft  the linux release for simplecufft assumes that the root install directory is usrlocalcuda and that the locations of the products are contained there as follows modify the makefile as appropriate for your system product location and name include file nvcc compiler binnvcc cufft library lib lib64libcufftso inccuffth cufft library with xt functionality lib lib64libcufftso inccufftxth cufftw library lib lib64libcufftwso inccufftwh the most common case is for developers to modify an existing cuda routine for example filenamecu to call cufft routines in this case the include file cuffth or cufftxth should be inserted into filenamecu file and the library included in the link line a single compile and link line might appear as usrlocalcudabinnvcc options filenamecu iusrlocalcudainc lusrlocalcudalib lcufft of course there will typically be many compile lines and the compiler g may be used for linking so long as the library path is set correctly users of the fftw interface see fftw interface to cufft should include cufftwh and link with both cufft and cufftw libraries functions in the cufft and cufftw library assume that the data is in gpu visible memory this means any memory allocated by cudamalloc  cudamallochost and cudamallocmanaged or registered with cudahostregister can be used as input output or plan work area with cufft and cufftw functions for the best performance input data output data and plan work area should reside in device memory cufftw library,https://docs.nvidia.com/cuda/curand/index.html,curand
the api reference guide for curand the cuda random number generation library,https://docs.nvidia.com/cuda/cusparse/index.html,cusparse
cusparse 1 introduction v125 pdf archive cusparse the api reference guide for cusparse the cuda sparse matrix library 1 introduction the cusparse library contains a set of gpuaccelerated basic linear algebra subroutines used for handling sparse matrices that perform significantly faster than cpuonly alternatives depending on the specific operation the library targets matrices with sparsity ratios in the range between  it is implemented on top of the nvidia cuda runtime which is part of the cuda toolkit and is designed to be called from c and c see also cusparselt a highperformance cuda library for sparse matrixmatrix multiplication cusparse release notes cudatoolkitreleasenotes cusparse github samples cudalibrarysamples nvidia developer forum gpuaccelerated libraries provide feedback mathlibsfeedback nvidia  com recent cusparsecusparselt blog posts and gtc presentations exploiting nvidia ampere structured sparsity with cusparselt accelerating matrix multiplication with block sparse format and nvidia tensor cores justintime linktime optimization adoption in cusparsecufft use case overview structured sparsity in the nvidia ampere architecture and applications in search engines making the most of structured sparsity in the nvidia ampere architecture the library routines provide the following functionalities operations between a sparse vector and a dense vector sum dot product scatter gather operations between a dense matrix and a sparse vector multiplication operations between a sparse matrix and a dense vector multiplication triangular solver tridiagonal solver pentadiagonal solver operations between a sparse matrix and a dense matrix multiplication triangular solver tridiagonal solver pentadiagonal solver operations between a sparse matrix and a sparse matrix sum multiplication operations between dense matrices with output a sparse matrix multiplication sparse matrix preconditioners incomplete cholesky factorization level 0 incomplete lu factorization level 0 reordering and conversion operations between different sparse matrix storage formats  library organization and features the cusparse library is organized in two set of apis the legacy apis  inspired by the sparse blas standard provide a limited set of functionalities and will not be improved in future releases  even if standard maintenance is still ensured some routines in this category could be deprecated and removed in the shortterm a replacement will be provided for the most important of them during the deprecation process the generic apis provide the standard interface layer of cusparse  they allow computing the most common sparse linear algebra operations such as sparse matrixvector spmv and sparse matrixmatrix multiplication spmm in a flexible way the new apis have the following capabilities and features set matrix data layouts  number of batches  and storage formats for example csr coo and so on set inputoutputcompute data types this also allows mixed datatype computation  set types of sparse vectormatrix indices eg 32bit 64bit choose the algorithm for the computation guarantee external device memory for internal operations provide extensive consistency checks across input matrices and vectors this includes the validation of sizes data types layout allowed operations etc provide constant descriptors for vector and matrix inputs to support constsafe interface and guarantee that the apis do not modify their inputs  static library support starting with cuda  the cusparse library is also delivered in a static form as libcusparsestatica on linux for example to compile a small application using cusparse against the dynamic library  the following command can be used nvcc mycusparseapp  cu lcusparse o mycusparseapp whereas to compile against the static library  the following command has to be used nvcc mycusparseapp  cu lcusparsestatic o mycusparseapp it is also possible to use the native host c compiler depending on the host operating system some additional libraries like pthread or dl might be needed on the linking line the following command on linux is suggested gcc mycusparseapp  c lcusparsestatic lcudartstatic lpthread ldl i cuda toolkit path include l cuda toolkit path lib64 o mycusparseapp note that in the latter case the library cuda is not needed the cuda runtime will try to open explicitly the cuda library if needed in the case of a system which does not have the cuda driver installed this allows the application to gracefully manage this issue and potentially run if a cpuonly path is available  library dependencies starting with cuda  cusparse will depend on nvjitlink library for jit justintime lto linktimeoptimization capabilities refer to the cusparsespmmop apis for more information if the user links to the dynamic library  the environment variables for loading the libraries at runtime such as ldlibrarypath on linux and path on windows must include the path where libnvjitlinkso is located if it is in the same directory as cusparse the user doesnt need to take any action if linking to the static library  the user needs to link with lnvjitlink and set the environment variables for loading the libraries at compiletime librarypathpath accordingly 2 using the cusparse api this chapter describes how to use the cusparse library api it is not a reference for the cusparse api data types and functions that is provided in subsequent chapters  apis usage notes the cusparse library allows developers to access the computational resources of the nvidia graphics processing unit gpu the cusparse apis assume that input and output data vectors and matrices reside in gpu device memory  the input and output scalars eg alpha and beta can be passed by reference on the host or the device instead of only being allowed to be passed by value on the host this allows library functions to execute asynchronously using streams even when they are generated by a previous kernel resulting in maximum parallelism the handle to the cusparse library context is initialized using the function and is explicitly passed to every subsequent library function call this allows the user to have more control over the library setup when using multiple host threads and multiple gpus the error status cusparsestatust is returned by all cusparse library function calls it is the responsibility of the developer to allocate memory and to copy data between gpu memory and cpu memory using standard cuda runtime api routines such as cudamalloc,https://docs.nvidia.com/cuda/npp/index.html,npp
npp nvidia 2d image and signal processing performance primitives npp v2305 pdf archive nvidia 2d image and signal processing performance primitives npp indices and search index search page contents what is npp  files header files library files library organization supported nvidia hardware general conventions memory management scratch buffer and host pointer function naming integer result scaling rounding modes rounding mode parameter image processing conventions function naming image data line step parameter names for image data passing sourceimage data sourceimage pointer sourcebatchimages pointer sourceplanarimage pointer array sourceplanarimage pointer sourceimage line step sourceplanarimage line step array sourceplanarimage line step passing destinationimage data destinationimage pointer destinationbatchimages pointer destinationplanarimage pointer array destinationplanarimage pointer destinationimage line step destinationplanarimage line step passing inplace image data inplace image pointer inplaceimage line step passing maskimage data maskimage pointer maskimage line step passing channelofinterest data channelofinterest number image data alignment requirements image data related error codes regionofinterest roi roi related error codes masked operation channelofinterest api selectchannel sourceimage pointer selectchannel sourceimage selectchannel destinationimage pointer sourceimage sampling pointwise operations neighborhood operations masksize parameter anchorpoint parameter sampling beyond image boundaries signal processing conventions signal data parameter names for signal data source signal pointer destination signal pointer inplace signal pointer signal data alignment requirements signal data related error codes signal length length related error codes data types structs enums and constants image arithmetic and logical operations arithmetic operations arithmetic operations addc mulc mulcscale subc divc absdiffc add addsquare addproduct addweighted mul mulscale sub div divround abs absdiff sqr sqrt ln exp logical operations logical operations andc orc xorc rshiftc lshiftc and or xor not image alpha composition operations alphacompc alphacomp image color conversion functions color processing functions color to gray conversion color debayer color gamma correction complement color key colortwist colortwistbatch colorlut colorlutlinear colorlutcubic colorluttrilinear colorlutpalette color sampling format conversion functions ycbcr420toycbcr411 ycbcr422toycbcr422 ycbcr422toycrcb422 ycbcr422tocbycr422 cbycr422toycbcr411 ycbcr422toycbcr420 ycrcb420toycbcr422 ycbcr422toycrcb420 ycbcr422toycbcr411 ycrcb422toycbcr422 ycrcb422toycbcr420 ycrcb422toycbcr411 cbycr422toycbcr422 cbycr422toycbcr420 cbycr422toycrcb420 ycbcr420toycbcr420 ycbcr420toycbcr422 ycbcr420tocbycr422 ycbcr420toycrcb420 ycrcb420tocbycr422 ycrcb420toycbycr420 ycrcb420toycbycr411 ycbcr411toycbcr411 ycbcr411toycbcr422 ycbcr411toycrcb422 ycbcr411toycbcr420 ycbcr411toycrcb420 nv12toyuv420 color model conversion functions rgbtoyuv bgrtoyuv yuvtorgb yuvtorgbbatch yuvtorgbbatchadvanced yuvtobgr yuvtobgrbatch yuvtobgrbatchadvanced rgbtoyuv422 yuv422torgb yuv422torgbbatch yuv422torgbbatchadvanced yuv422tobgrbatch yuv422tobgrbatchadvanced rgbtoyuv420 yuv420torgb yuv420torgbbatch yuv420torgbbatchadvanced nv12torgb nv21torgb bgrtoyuv420 yuv420tobgr yuv420tobgrbatch yuv420tobgrbatchadvanced nv12tobgr nv21tobgr rgbtoycbcr ycbcrtorgb ycbcrtorgbbatch ycbcrtorgbbatchadvanced ycbcrtobgr ycbcrtobgrbatch ycbcrtobgrbatchadvanced ycbcrtobgr709csc rgbtoycbcr422 ycbcr422torgb ycbcr422torgbbatch ycbcr422torgbbatchadvanced rgbtoycrcb422 ycrcb422torgb ycbcr422tobgr ycbcr422tobgrbatch ycbcr422tobgrbatchadvanced rgbtocbycr422 cbycr422torgb bgrtocbycr422 bgrtocbycr422 709hdtv cbycr422tobgr cbycr422tobgr 709hdtv rgbtoycbcr420 ycbcr420torgb ycbcr420torgbbatch ycbcr420torgbbatchadvanced rgbtoycrcb420 ycrcb420torgb bgrtoycbcr420 bgrtoycbcr420 709csc bgrtoycbcr420 709hdtv bgrtoycrcb420 709csc ycbcr420tobgr ycbcr420tobgrbatch ycbcr420tobgrbatchadvanced ycbcr420tobgr 709csc ycbcr420tobgr 709hdtv bgrtoycrcb420 bgrtoycbcr411 bgrtoycbcr ycbcr411tobgr ycbcr411torgb rgbtoxyz xyztorgb rgbtoluv luvtorgb bgrtolab labtobgr rgbtoycc ycctorgb yccktocmykjpeg cmykorycckjpegtorgb ycckjpegorcmyktobgr rgbtohls hlstorgb bgrtohls hlstobgr rbgtohsv hsvtorgb jpeg color conversion image data exchange and initialization functions set common parameters for nppiset functions masked set common parameters for nppisetcxm functions channel set common parameters for nppisetcxc functions copy common parameters for nppicopy functions masked copy common parameters for nppicopycxm functions channel copy common parameters for nppicopycxc functions extract channel copy common parameters for nppicopycxc1 functions insert channel copy common parameters for nppicopyc1cx functions packed to planar channel copy common parameters for nppicopycxpx functions planar to packed channel copy common parameters for nppicopypxcx functions copy constant border common parameters for nppicopyconstborder functions copy replicate border common parameters for nppicopyreplicateborder functions copy wrap border common parameters for nppicopywrapborder functions copy subpixel common parameters for nppicopysubpix functions convert bit depth convert to increased bit depth common parameters for nppiconvert to increased bit depth functions convert to decreased bit depth common parameters for nppiconvert to decreased bit depth functions scale bit depth scale to higher bit depth common parameters for nppiscale to higher bit depth functions scale to lower bit depth common parameters for nppiscale to lower bit depth functions duplicate channel common parameters for nppidup functions transpose common parameters for nppitranspose functions swap channels image filtering functions image 1d linear filters 1dlinearfilter image filter column filtercolumn image filter column border filtercolumnborder image filter column 32f filtercolumn32f image filter column border 32f filtercolumnborder32f image filter row filterrow image filter row border filterrowborder image filter row 32f filterrow32f image filter row border 32f filterrowborder32f image filter 1d window sum 1d window sum image filter 1d window column sum 1d window column sum image filter 1d window row sum 1d window row sum image filter 1d window sum border 1d window sum with border control image filter 1d window column sum border 1d window column sum border image filter 1d window row sum border 1d window row sum border image convolution convolution image filter filter image filter 32f filter32f image filter border filterborder image filter border 32f filterborder32f 2d fixed linear filters 2d fixed linear filters image filter box filterbox image filter box border filterboxborder image filter box border advanced filterboxborderadvanced image filter threshold adaptive box border filterthresholdadaptiveboxborder rank filters rank filters image filter max filtermax image filter max border filtermaxborder image filter min filtermin image filter min border filterminborder image filter median filtermedian image filter median border filtermedianborder fixed filters fixed filters image filter prewitt filterprewitt image filter prewitt border filterprewittborder image filter scharr filterscharr image filter scharr border filterscharrborder image filter sobel filtersobel image filter sobel border filtersobelborder image filter roberts filterroberts image filter roberts border filterrobertsborder image filter laplace filterlaplace image filter laplace border filterlaplaceborder image filter gauss filtergauss image filter gauss advanced filtergaussadvanced image filter gauss border filtergaussborder image filter advanced gauss border filtergaussadvancedborder image filter gauss pyramid layer down border filtergausspyramidlayerdownborder image filter gauss pyramid layer up border filtergausspyramidlayerupborder image filter bilateral gauss border filterbilateralgaussborder image filter high pass filterhighpass image filter high pass border filterhighpassborder image filter low pass filterlowpass image filter low pass border filterlowpassborder image filter sharpen filtersharpen image filter sharpen border filtersharpenborder image filter unsharp border filterunsharpborder image filter wiener border filterwienerborder image filter gradient vector prewitt border gradientvectorprewittborder image filter gradient vector scharr border gradientvectorscharrborder image filter gradient vector sobel border gradientvectorsobelborder computer vision filtering functions computer vision image filter distance transform filterdistancetransform image filter harris corners border filterharriscornersborder image filter hough line filterhoughline image filter histogram of oriented gradients border histogramoforientedgradientsborder image filter flood fill floodfill flood fill,https://docs.nvidia.com/cuda/nvjitlink/index.html,nvjitlink
nvjitlink 1 introduction v125 archive nvjitlink the user guide to nvjitlink library 1 introduction the jit link apis are a set of apis which can be used at runtime to link together gpu devide code the apis accept inputs in multiple formats either host objects host libraries fatbins device cubins ptx or ltoir the output is a linked cubin that can be loaded by cumoduleloaddata and cumoduleloaddataex of the cuda driver api link time optimization can also be performed when given ltoir or higher level formats that include ltoir if an input does not contain gpu assembly code it is first compiled and then linked the functionality in this library is similar to the culink apis in the cuda driver with the following advantages support for link time optimization allow users to use runtime linking with the latest toolkit version that is supported as part of cuda toolkit release this support may not be available in the cuda driver apis if the application is running with an older driver installed in the system refer to cuda compatibility for more details the clients get fine grain control and can specify lowlevel compiler options during linking 2 getting started  system requirements the jit link library requires the following system configuration posix threads support for nonwindows platform gpu any gpu with cuda compute capability  or higher cuda toolkit and driver  installation the jit link library is part of the cuda toolkit release and the components are organized as follows in the cuda toolkit installation directory on windows includenvjitlinkh libx64nvjitlinkdll libx64nvjitlinkstaticlib docpdfnvjitlinkuserguidepdf on linux includenvjitlinkh lib64libnvjitlinkso lib64libnvjitlinkstatica docpdfnvjitlinkuserguidepdf 3 user interface this chapter presents the jit link apis basic usage of the api is explained in basic usage  error codes linking supported link options  error codes enumerations nvjitlinkresult the enumerated type nvjitlinkresult defines api call result codes 1 enumerations enum nvjitlinkresult the enumerated type nvjitlinkresult defines api call result codes nvjitlink apis return nvjitlinkresult codes to indicate the result values enumerator nvjitlinksuccess enumerator nvjitlinkerrorunrecognizedoption enumerator nvjitlinkerrormissingarch enumerator nvjitlinkerrorinvalidinput enumerator nvjitlinkerrorptxcompile enumerator nvjitlinkerrornvvmcompile enumerator nvjitlinkerrorinternal enumerator nvjitlinkerrorthreadpool enumerator nvjitlinkerrorunrecognizedinput  linking enumerations nvjitlinkinputtype the enumerated type nvjitlinkinputtype defines the kind of inputs that can be passed to nvjitlinkadd apis functions nvjitlinkresult nvjitlinkadddata nvjitlinkhandle handle nvjitlinkinputtype inputtype const void data sizet size const char name nvjitlinkadddata adds data image to the link nvjitlinkresult nvjitlinkaddfile nvjitlinkhandle handle nvjitlinkinputtype inputtype const char filename nvjitlinkaddfile reads data from file and links it in nvjitlinkresult nvjitlinkcomplete nvjitlinkhandle handle nvjitlinkcomplete does the actual link nvjitlinkresult nvjitlinkcreate nvjitlinkhandle handle uint32t numoptions const char options nvjitlinkcreate creates an instance of nvjitlinkhandle with the given input options and sets the output parameter handle  nvjitlinkresult nvjitlinkdestroy nvjitlinkhandle handle nvjitlinkdestroy frees the memory associated with the given handle and sets it to null nvjitlinkresult nvjitlinkgeterrorlog nvjitlinkhandle handle char log nvjitlinkgeterrorlog puts any error messages in the log nvjitlinkresult nvjitlinkgeterrorlogsize nvjitlinkhandle handle sizet size nvjitlinkgeterrorlogsize gets the size of the error log nvjitlinkresult nvjitlinkgetinfolog nvjitlinkhandle handle char log nvjitlinkgetinfolog puts any info messages in the log nvjitlinkresult nvjitlinkgetinfologsize nvjitlinkhandle handle sizet size nvjitlinkgetinfologsize gets the size of the info log nvjitlinkresult nvjitlinkgetlinkedcubin nvjitlinkhandle handle void cubin nvjitlinkgetlinkedcubin gets the linked cubin nvjitlinkresult nvjitlinkgetlinkedcubinsize nvjitlinkhandle handle sizet size nvjitlinkgetlinkedcubinsize gets the size of the linked cubin nvjitlinkresult nvjitlinkgetlinkedptx nvjitlinkhandle handle char ptx nvjitlinkgetlinkedptx gets the linked ptx nvjitlinkresult nvjitlinkgetlinkedptxsize nvjitlinkhandle handle sizet size nvjitlinkgetlinkedptxsize gets the size of the linked ptx nvjitlinkresult nvjitlinkversion unsigned int major unsigned int minor nvjitlinkversion returns the current version of nvjitlink typedefs nvjitlinkhandle nvjitlinkhandle is the unit of linking and an opaque handle for a program 1 enumerations enum nvjitlinkinputtype the enumerated type nvjitlinkinputtype defines the kind of inputs that can be passed to nvjitlinkadd apis values enumerator nvjitlinkinputnone enumerator nvjitlinkinputcubin enumerator nvjitlinkinputptx enumerator nvjitlinkinputltoir enumerator nvjitlinkinputfatbin enumerator nvjitlinkinputobject enumerator nvjitlinkinputlibrary enumerator nvjitlinkinputany 2 functions static inline nvjitlinkresult nvjitlinkadddata nvjitlinkhandle handle  nvjitlinkinputtype inputtype  const void data  sizet size  const char name nvjitlinkadddata adds data image to the link parameters handle in nvjitlink handle inputtype in kind of input data in pointer to data image in memory size in size of the data name in name of input object returns nvjitlinksuccess nvjitlinkerrorinvalidinput nvjitlinkerrorinternal static inline nvjitlinkresult nvjitlinkaddfile nvjitlinkhandle handle  nvjitlinkinputtype inputtype  const char filename nvjitlinkaddfile reads data from file and links it in parameters handle in nvjitlink handle inputtype in kind of input filename in name of file returns nvjitlinksuccess nvjitlinkerrorinvalidinput nvjitlinkerrorinternal static inline nvjitlinkresult nvjitlinkcomplete nvjitlinkhandle handle nvjitlinkcomplete does the actual link parameters handle in nvjitlink handle returns nvjitlinksuccess nvjitlinkerrorinvalidinput nvjitlinkerrorinternal static inline nvjitlinkresult nvjitlinkcreate nvjitlinkhandle handle  uint32t numoptions  const char options nvjitlinkcreate creates an instance of nvjitlinkhandle with the given input options and sets the output parameter handle  it supports options listed in supported link options  see also nvjitlinkdestroy parameters handle out address of nvjitlink handle numoptions in number of options passed options in array of size numoptions of option strings returns nvjitlinksuccess nvjitlinkerrorunrecognizedoption nvjitlinkerrormissingarch nvjitlinkerrorinvalidinput nvjitlinkerrorinternal static inline nvjitlinkresult nvjitlinkdestroy nvjitlinkhandle handle nvjitlinkdestroy frees the memory associated with the given handle and sets it to null see also nvjitlinkcreate parameters handle in address of nvjitlink handle returns nvjitlinksuccess nvjitlinkerrorinvalidinput nvjitlinkerrorinternal static inline nvjitlinkresult nvjitlinkgeterrorlog nvjitlinkhandle handle  char log nvjitlinkgeterrorlog puts any error messages in the log user is responsible for allocating enough space to hold the log  see also nvjitlinkgeterrorlogsize parameters handle in nvjitlink handle log out the error log returns nvjitlinksuccess nvjitlinkerrorinvalidinput nvjitlinkerrorinternal static inline nvjitlinkresult nvjitlinkgeterrorlogsize nvjitlinkhandle handle  sizet size nvjitlinkgeterrorlogsize gets the size of the error log see also nvjitlinkgeterrorlog parameters handle in nvjitlink handle size out size of the error log returns nvjitlinksuccess nvjitlinkerrorinvalidinput nvjitlinkerrorinternal static inline nvjitlinkresult nvjitlinkgetinfolog nvjitlinkhandle handle  char log nvjitlinkgetinfolog puts any info messages in the log user is responsible for allocating enough space to hold the log  see also nvjitlinkgetinfologsize parameters handle in nvjitlink handle log out the info log returns nvjitlinksuccess nvjitlinkerrorinvalidinput nvjitlinkerrorinternal static inline nvjitlinkresult nvjitlinkgetinfologsize nvjitlinkhandle handle,https://docs.nvidia.com/cuda/nvfatbin/index.html,nvfatbin
nvfatbin 1 introduction v125 archive nvfatbin the user guide to nvfatbin library 1 introduction the fatbin creator apis are a set of apis which can be used at runtime to combine multiple cuda objects into one cuda fat binary fatbin the apis accept inputs in multiple formats either device cubins ptx or ltoir the output is a fatbin that can be loaded by cumoduleloaddata of the cuda driver api the functionality in this library is similar to the fatbinary offline tool in the cuda toolkit with the following advantages support for runtime fatbin creation the clients get fine grain control over the input process supports direct input from memory rather than requiring inputs be written to files 2 getting started  system requirements the fatbin creator library requires no special system configuration it does not require a gpu  installation the fatbin creator library is part of the cuda toolkit release and the components are organized as follows in the cuda toolkit installation directory on windows includenvfatbinh libx64nvfatbindll libx64nvfatbinstaticlib docpdfnvfatbinuserguidepdf on linux includenvfatbinh lib64libnvfatbinso lib64libnvfatbinstatica docpdfnvfatbinuserguidepdf 3 user interface this chapter presents the fatbin creator apis basic usage of the api is explained in basic usage  error codes creation supported options  error codes enumerations nvfatbinresult the enumerated type nvfatbinresult defines api call result codes functions const char nvfatbingeterrorstring nvfatbinresult result nvfatbingeterrorstring returns an error description string for each error code 1 enumerations enum nvfatbinresult the enumerated type nvfatbinresult defines api call result codes nvfatbin apis return nvfatbinresult codes to indicate the result values enumerator nvfatbinsuccess enumerator nvfatbinerrorinternal enumerator nvfatbinerrorelfarchmismatch enumerator nvfatbinerrorelfsizemismatch enumerator nvfatbinerrormissingptxversion enumerator nvfatbinerrornullpointer enumerator nvfatbinerrorcompressionfailed enumerator nvfatbinerrorcompressedsizeexceeded enumerator nvfatbinerrorunrecognizedoption enumerator nvfatbinerrorinvalidarch enumerator nvfatbinerrorinvalidnvvm enumerator nvfatbinerroremptyinput enumerator nvfatbinerrormissingptxarch enumerator nvfatbinerrorptxarchmismatch enumerator nvfatbinerrormissingfatbin enumerator nvfatbinerrorinvalidindex enumerator nvfatbinerroridentifierreuse 2 functions const char nvfatbingeterrorstring nvfatbinresult result nvfatbingeterrorstring returns an error description string for each error code parameters result in error code returns nullptr if result is nvfatbinsuccess a string if result is not nvfatbinsuccess  fatbinary creation functions nvfatbinresult nvfatbinaddcubin nvfatbinhandle handle const void code sizet size const char arch const char identifier nvfatbinaddcubin adds a cuda binary to the fatbinary nvfatbinresult nvfatbinaddindex nvfatbinhandle handle const void code sizet size const char identifier nvfatbinaddindex adds an index file to the fatbinary nvfatbinresult nvfatbinaddltoir nvfatbinhandle handle const void code sizet size const char arch const char identifier const char optionscmdline nvfatbinaddltoir adds ltoir to the fatbinary nvfatbinresult nvfatbinaddptx nvfatbinhandle handle const char code sizet size const char arch const char identifier const char optionscmdline nvfatbinaddptx adds ptx to the fatbinary nvfatbinresult nvfatbinaddreloc nvfatbinhandle handle const void code sizet size nvfatbinaddreloc adds relocatable ptx entries from a host object to the fatbinary nvfatbinresult nvfatbincreate nvfatbinhandle handleindirect const char options sizet optionscount nvfatbincreate creates a new handle nvfatbinresult nvfatbindestroy nvfatbinhandle handleindirect nvfatbindestroy destroys the handle nvfatbinresult nvfatbinget nvfatbinhandle handle void buffer nvfatbinget returns the completed fatbinary nvfatbinresult nvfatbinsize nvfatbinhandle handle sizet size nvfatbinsize returns the fatbinarys size nvfatbinresult nvfatbinversion unsigned int major unsigned int minor nvfatbinversion returns the current version of nvfatbin typedefs nvfatbinhandle nvfatbinhandle is the unit of fatbin creation and an opaque handle for a program 1 functions nvfatbinresult nvfatbinaddcubin nvfatbinhandle handle  const void code  sizet size  const char arch  const char identifier nvfatbinaddcubin adds a cuda binary to the fatbinary user is responsible for making sure all strings are wellformed parameters handle in nvfatbin handle code in the cubin size in the size of the cubin arch in the architecture that this cubin is for identifier in name of the cubin useful when extracting the fatbin with tools like cuobjdump returns nvfatbinsuccess nvfatbinerrorinvalidarch nvfatbinerrorelfarchmismatch nvfatbinerrorelfsizemismatch nvfatbinerrorcompressionfailed nvfatbinerrorunrecognizedoption nvfatbinerrorcompressedsizeexceeded nvfatbinerroremptyinput nvfatbinerrorinternal nvfatbinresult nvfatbinaddindex nvfatbinhandle handle  const void code  sizet size  const char identifier nvfatbinaddindex adds an index file to the fatbinary user is responsible for making sure all strings are wellformed parameters handle in nvfatbin handle code in the index size in the size of the index identifier in name of the index useful when extracting the fatbin with tools like cuobjdump returns nvfatbinsuccess nvfatbinerrorinvalidindex nvfatbinerrorcompressionfailed nvfatbinerrorunrecognizedoption nvfatbinerrorcompressedsizeexceeded nvfatbinerroremptyinput nvfatbinerrorinternal nvfatbinresult nvfatbinaddltoir nvfatbinhandle handle  const void code  sizet size  const char arch  const char identifier  const char optionscmdline nvfatbinaddltoir adds ltoir to the fatbinary user is responsible for making sure all strings are wellformed parameters handle in nvfatbin handle code in the ltoir code size in the size of the ltoir code arch in the architecture that this ltoir is for identifier in name of the ltoir useful when extracting the fatbin with tools like cuobjdump optionscmdline in options used during jit compilation returns nvfatbinsuccess nvfatbinerrornullpointer nvfatbinerrorinvalidarch nvfatbinerrorcompressionfailed nvfatbinerrorunrecognizedoption nvfatbinerrorcompressedsizeexceeded nvfatbinerroremptyinput nvfatbinerrorinternal nvfatbinresult nvfatbinaddptx nvfatbinhandle handle  const char code  sizet size  const char arch  const char identifier  const char optionscmdline nvfatbinaddptx adds ptx to the fatbinary user is responsible for making sure all string are wellformed the size should be inclusive of the terminating null character 0 if the final character is not 0 one will be added automatically but in doing so the code will be copied if it hasnt already been copied parameters handle in nvfatbin handle code in the ptx code size in the size of the ptx code arch in the architecture that this ptx is for identifier in name of the ptx useful when extracting the fatbin with tools like cuobjdump optionscmdline in options used during jit compilation returns nvfatbinsuccess nvfatbinerrornullpointer nvfatbinerrorinvalidarch nvfatbinerrorptxarchmismatch nvfatbinerrorcompressionfailed nvfatbinerrorunrecognizedoption nvfatbinerrorcompressedsizeexceeded nvfatbinerroremptyinput nvfatbinerrormissingptxversion nvfatbinerrormissingptxarch nvfatbinerrorinternal nvfatbinresult nvfatbinaddreloc nvfatbinhandle handle  const void code  sizet size nvfatbinaddreloc adds relocatable ptx entries from a host object to the fatbinary note that each relocatable ptx source must have a unique identifier the identifiers are taken from the objects entries this is enforced as only one entry per sm of each unique identifier note also that handle options are ignored for this operation instead the host objects options are copied over from each of its entries parameters handle in nvfatbin handle code in the host object image,https://docs.nvidia.com/cuda/nvrtc/index.html,nvrtc runtime compilation
nvrtc 1 introduction v125 pdf archive nvrtc the user guide for the nvrtc library 1 introduction nvrtc is a runtime compilation library for cuda c it accepts cuda c source code in character string form and creates handles that can be used to obtain the ptx the ptx string generated by nvrtc can be loaded by cumoduleloaddata and cumoduleloaddataex  and linked with other modules by using the nvjitlink library or using culinkadddata of the cuda driver api this facility can often provide optimizations and performance not possible in a purely offline static compilation in the absence of nvrtc or any runtime compilation support in cuda users needed to spawn a separate process to execute nvcc at runtime if they wished to implement runtime compilation in their applications or libraries and unfortunately this approach has the following drawbacks the compilation overhead tends to be higher than necessary end users are required to install nvcc and related tools which make it complicated to distribute applications that use runtime compilation nvrtc addresses these issues by providing a library interface that eliminates overhead associated with spawning separate processes disk ioand so on while keeping application deployment simple 2 getting started  system requirements nvrtc requires the following system configuration operating system linux x8664 linux ppc64le linux aarch64 or windows x8664 gpu any gpu with cuda compute capability  or higher cuda toolkit and driver  installation nvrtc is part of the cuda toolkit release and the components are organized as follows in the cuda toolkit installation directory on windows includenvrtch binnvrtc64major release versionminor release version0dll binnvrtcbuiltins64major release versionminor release versiondll libx64nvrtclib libx64nvrtcstaticlib libx64nvrtcbuiltinsstaticlib docpdfnvrtcuserguidepdf on linux includenvrtch lib64libnvrtcso lib64libnvrtcsomajor release versionminor release version lib64libnvrtcsomajor release versionminor release versionbuild version lib64libnvrtcbuiltinsso lib64libnvrtcbuiltinssomajor release versionminor release version lib64libnvrtcbuiltinssomajor release versionminor release versionbuild version lib64libnvrtcstatica lib64libnvrtcbuiltinsstatica docpdfnvrtcuserguidepdf 3 user interface this chapter presents the api of nvrtc basic usage of the api is explained in basic usage  error handling general information query compilation supported compile options host helper  error handling nvrtc defines the following enumeration type and function for api call error handling enumerations nvrtcresult the enumerated type nvrtcresult defines api call result codes functions const char nvrtcgeterrorstring nvrtcresult result nvrtcgeterrorstring is a helper function that returns a string describing the given nvrtcresult code eg nvrtcsuccess to nvrtcsuccess  1 enumerations enum nvrtcresult the enumerated type nvrtcresult defines api call result codes nvrtc api functions return nvrtcresult to indicate the call result values enumerator nvrtcsuccess enumerator nvrtcerroroutofmemory enumerator nvrtcerrorprogramcreationfailure enumerator nvrtcerrorinvalidinput enumerator nvrtcerrorinvalidprogram enumerator nvrtcerrorinvalidoption enumerator nvrtcerrorcompilation enumerator nvrtcerrorbuiltinoperationfailure enumerator nvrtcerrornonameexpressionsaftercompilation enumerator nvrtcerrornolowerednamesbeforecompilation enumerator nvrtcerrornameexpressionnotvalid enumerator nvrtcerrorinternalerror enumerator nvrtcerrortimefilewritefailed 2 functions const char nvrtcgeterrorstring nvrtcresult result nvrtcgeterrorstring is a helper function that returns a string describing the given nvrtcresult code eg nvrtcsuccess to nvrtcsuccess  for unrecognized enumeration values it returns nvrtcerror unknown  parameters result in cuda runtime compilation api result code returns message string for the given nvrtcresult code  general information query nvrtc defines the following function for general information query functions nvrtcresult nvrtcgetnumsupportedarchs int numarchs nvrtcgetnumsupportedarchs sets the output parameter numarchs with the number of architectures supported by nvrtc nvrtcresult nvrtcgetsupportedarchs int supportedarchs nvrtcgetsupportedarchs populates the array passed via the output parameter supportedarchs with the architectures supported by nvrtc nvrtcresult nvrtcversion int major int minor nvrtcversion sets the output parameters major and minor with the cuda runtime compilation version number 1 functions nvrtcresult nvrtcgetnumsupportedarchs int numarchs nvrtcgetnumsupportedarchs sets the output parameter numarchs with the number of architectures supported by nvrtc this can then be used to pass an array to nvrtcgetsupportedarchs to get the supported architectures see nvrtcgetsupportedarchs parameters numarchs out number of supported architectures returns nvrtcsuccess nvrtcerrorinvalidinput nvrtcresult nvrtcgetsupportedarchs int supportedarchs nvrtcgetsupportedarchs populates the array passed via the output parameter supportedarchs with the architectures supported by nvrtc the array is sorted in the ascending order the size of the array to be passed can be determined using nvrtcgetnumsupportedarchs  see nvrtcgetnumsupportedarchs parameters supportedarchs out sorted array of supported architectures returns nvrtcsuccess nvrtcerrorinvalidinput nvrtcresult nvrtcversion int major  int minor nvrtcversion sets the output parameters major and minor with the cuda runtime compilation version number parameters major out cuda runtime compilation major version number minor out cuda runtime compilation minor version number returns nvrtcsuccess nvrtcerrorinvalidinput  compilation nvrtc defines the following type and functions for actual compilation functions nvrtcresult nvrtcaddnameexpression nvrtcprogram prog const char const nameexpression nvrtcaddnameexpression notes the given name expression denoting the address of a global function or device constant variable nvrtcresult nvrtccompileprogram nvrtcprogram prog int numoptions const char const options nvrtccompileprogram compiles the given program nvrtcresult nvrtccreateprogram nvrtcprogram prog const char src const char name int numheaders const char const headers const char const includenames nvrtccreateprogram creates an instance of nvrtcprogram with the given input parameters and sets the output parameter prog with it nvrtcresult nvrtcdestroyprogram nvrtcprogram prog nvrtcdestroyprogram destroys the given program nvrtcresult nvrtcgetcubin nvrtcprogram prog char cubin nvrtcgetcubin stores the cubin generated by the previous compilation of prog in the memory pointed by cubin  nvrtcresult nvrtcgetcubinsize nvrtcprogram prog sizet cubinsizeret nvrtcgetcubinsize sets the value of cubinsizeret with the size of the cubin generated by the previous compilation of prog  nvrtcresult nvrtcgetltoir nvrtcprogram prog char ltoir nvrtcgetltoir stores the lto ir generated by the previous compilation of prog in the memory pointed by ltoir  nvrtcresult nvrtcgetltoirsize nvrtcprogram prog sizet ltoirsizeret nvrtcgetltoirsize sets the value of ltoirsizeret with the size of the lto ir generated by the previous compilation of prog  nvrtcresult nvrtcgetloweredname nvrtcprogram prog const char const nameexpression const char loweredname nvrtcgetloweredname extracts the lowered mangled name for a global function or device constant variable and updates loweredname to point to it nvrtcresult nvrtcgetnvvm nvrtcprogram prog char nvvm deprecation notice this function will be removed in a future release nvrtcresult nvrtcgetnvvmsize nvrtcprogram prog sizet nvvmsizeret deprecation notice this function will be removed in a future release nvrtcresult nvrtcgetoptixir nvrtcprogram prog char optixir nvrtcgetoptixir stores the optix ir generated by the previous compilation of prog in the memory pointed by optixir  nvrtcresult,https://docs.nvidia.com/cuda/cusolver/index.html,cusolver
cusolver 1 introduction v125 pdf archive cusolver api reference the api reference guide for cusolver a gpu accelerated library for decompositions and linear system solutions for both dense and sparse matrices 1 introduction the cusolver library is a highlevel package based on the cublas and cusparse libraries it consists of two modules corresponding to two sets of api the cusolver api on a single gpu the cusolvermg api on a single node multigpu each of these can be used independently or in concert with other toolkit libraries to simplify the notation cusolver denotes single gpu api and cusolvermg denotes multigpu api the intent of cusolver is to provide useful lapacklike features such as common matrix factorization and triangular solve routines for dense matrices a sparse leastsquares solver and an eigenvalue solver in addition cusolver provides a new refactorization library useful for solving sequences of matrices with a shared sparsity pattern cusolver combines three separate components under a single umbrella the first part of cusolver is called cusolverdn and deals with dense matrix factorization and solve routines such as lu qr svd and ldlt as well as useful utilities such as matrix and vector permutations next cusolversp provides a new set of sparse routines based on a sparse qr factorization not all matrices have a good sparsity pattern for parallelism in factorization so the cusolversp library also provides a cpu path to handle those sequentiallike matrices for those matrices with abundant parallelism the gpu path will deliver higher performance the library is designed to be called from c and c the final part is cusolverrf a sparse refactorization package that can provide very good performance when solving a sequence of matrices where only the coefficients are changed but the sparsity pattern remains the same the gpu path of the cusolver library assumes data is already in the device memory it is the responsibility of the developer to allocate memory and to copy data between gpu memory and cpu memory using standard cuda runtime api routines such as cudamalloc  cudafree  cudamemcpy  and cudamemcpyasync  cusolvermg is gpuaccelerated scalapack by now cusolvermg supports 1d column block cyclic layout and provides symmetric eigenvalue solver note the cusolver library requires hardware with a cuda compute capability cc of  or higher please see the cuda c programming guide for a list of the compute capabilities corresponding to all nvidia gpus  cusolverdn dense lapack the cusolverdn library was designed to solve dense linear systems of the form ax b where the coefficient matrix ain rnxn  righthandside vector bin rn and solution vector xin rn the cusolverdn library provides qr factorization and lu with partial pivoting to handle a general matrix a  which may be nonsymmetric cholesky factorization is also provided for symmetrichermitian matrices for symmetric indefinite matrices we provide bunchkaufman ldl factorization the cusolverdn library also provides a helpful bidiagonalization routine and singular value decomposition svd the cusolverdn library targets computationallyintensive and popular routines in lapack and provides an api compatible with lapack the user can accelerate these timeconsuming routines with cusolverdn and keep others in lapack without a major change to existing code  cusolversp sparse lapack the cusolversp library was mainly designed to a solve sparse linear system ax b and the leastsquares problem x argminaz b where sparse matrix ain rmxn  righthandside vector bin rm and solution vector xin rn  for a linear system we require mn  the core algorithm is based on sparse qr factorization the matrix a is accepted in csr format if matrix a is symmetrichermitian the user has to provide a full matrix ie fill missing lower or upper part if matrix a is symmetric positive definite and the user only needs to solve ax b  cholesky factorization can work and the user only needs to provide the lower triangular part of a  on top of the linear and leastsquares solvers the cusolversp library provides a simple eigenvalue solver based on shiftinverse power method and a function to count the number of eigenvalues contained in a box in the complex plane  cusolverrf refactorization the cusolverrf library was designed to accelerate solution of sets of linear systems by fast refactorization when given new coefficients in the same sparsity pattern aixi fi where a sequence of coefficient matrices aiin rnxn  righthandsides fiin rn and solutions xiin rn are given for i1k  the cusolverrf library is applicable when the sparsity pattern of the coefficient matrices ai as well as the reordering to minimize fillin and the pivoting used during the lu factorization remain the same across these linear systems in that case the first linear system i1 requires a full lu factorization while the subsequent linear systems i2k require only the lu refactorization the later can be performed using the cusolverrf library notice that because the sparsity pattern of the coefficient matrices the reordering and pivoting remain the same the sparsity pattern of the resulting triangular factors li and ui also remains the same therefore the real difference between the full lu factorization and lu refactorization is that the required memory is known ahead of time  naming conventions the cusolverdn library provides two different apis legacy and generic  the functions in the legacy api are available for data types float  double  cucomplex  and cudoublecomplex  the naming convention for the legacy api is as follows cusolverdn t operation where t can be s  d  c  z  or x  corresponding to the data types float  double  cucomplex  cudoublecomplex  and the generic type respectively operation can be cholesky factorization potrf  lu with partial pivoting getrf  qr factorization geqrf and bunchkaufman factorization sytrf  the functions in the generic api provide a single entry point for each routine and support for 64bit integers to define matrix and vector dimensions the naming convention for the generic api is dataagnostic and is as follows cusolverdn operation where,https://docs.nvidia.com/cuda/ptx-compiler-api/index.html,ptx compiler apis
ptx compiler api 1 introduction v125 archive ptx compiler apis the user guide to ptx compiler apis 1 introduction the ptx compiler apis are a set of apis which can be used to compile a ptx program into gpu assembly code the apis accept ptx programs in character string form and create handles to the compiler that can be used to obtain the gpu assembly code the gpu assembly code string generated by the apis can be loaded by cumoduleloaddata and cumoduleloaddataex  and linked with other modules by culinkadddata or nvjitlinkadddata api from nvjitlink of the cuda driver api the main use cases for these ptx compiler apis are with cuda driver apis compilation and loading are tied together ptx compiler apis decouple the two operations this allows applications to perform early compilation and caching of the gpu assembly code ptx compiler apis allow users to use runtime compilation for the latest ptx version that is supported as part of cuda toolkit release this support may not be available in the ptx jit compiler present in the cuda driver if the application is running with an older driver installed in the system refer to cuda compatibility for more details with ptx compiler apis clients can implement a custom caching mechanism with the compiled gpu assembly with cuda driver there is no control over caching of the jit compilation results the clients get fine grain control and can specify the compiler options during compilation 2 getting started  system requirements ptx compiler library requires the following system configuration posix threads support for nonwindows platform gpu any gpu with cuda compute capability  or higher cuda toolkit and driver  installation ptx compiler library is part of the cuda toolkit release and the components are organized as follows in the cuda toolkit installation directory on windows includenvptxcompilerh libx64nvptxcompilerstaticlib docpdfptxcompilerapiuserguidepdf on linux includenvptxcompilerh lib64libnvptxcompilerstatica docpdfptxcompilerapiuserguidepdf 3 thread safety all ptx compiler api functions are thread safe and may be invoked by multiple threads concurrently 4 user interface this chapter presents the ptx compiler apis basic usage of the api is explained in basic usage ptxcompiler handle error codes api versioning compilation apis  ptxcompiler handle typedefs nvptxcompilerhandle nvptxcompilerhandle represents a handle to the ptx compiler 1 typedefs typedef struct nvptxcompiler nvptxcompilerhandle nvptxcompilerhandle represents a handle to the ptx compiler to compile a ptx program string an instance of nvptxcompiler must be created and the handle to it must be obtained using the api nvptxcompilercreate  then the compilation can be done using the api nvptxcompilercompile   error codes enumerations nvptxcompileresult the nvptxcompiler apis return the nvptxcompileresult codes to indicate the call result 1 enumerations enum nvptxcompileresult the nvptxcompiler apis return the nvptxcompileresult codes to indicate the call result values enumerator nvptxcompilesuccess enumerator nvptxcompileerrorinvalidcompilerhandle enumerator nvptxcompileerrorinvalidinput enumerator nvptxcompileerrorcompilationfailure enumerator nvptxcompileerrorinternal enumerator nvptxcompileerroroutofmemory enumerator nvptxcompileerrorcompilerinvocationincomplete enumerator nvptxcompileerrorunsupportedptxversion enumerator nvptxcompileerrorunsupporteddevsidesync  api versioning the ptx compiler apis are versioned so that any new features or api changes can be done by bumping up the api version functions nvptxcompileresult nvptxcompilergetversion unsigned int major unsigned int minor queries the current major and minor version of ptx compiler apis being used 1 functions nvptxcompileresult nvptxcompilergetversion unsigned int major  unsigned int minor queries the current major and minor version of ptx compiler apis being used note the version of ptx compiler apis follows the cuda toolkit versioning the ptx isa version supported by a ptx compiler api version is listed here  parameters major out major version of the ptx compiler apis minor out minor version of the ptx compiler apis returns nvptxcompilesuccess nvptxcompileerrorinternal  compilation apis functions nvptxcompileresult nvptxcompilercompile nvptxcompilerhandle compiler int numcompileoptions const char const compileoptions compile a ptx program with the given compiler options nvptxcompileresult nvptxcompilercreate nvptxcompilerhandle compiler sizet ptxcodelen const char ptxcode obtains the handle to an instance of the ptx compiler initialized with the given ptx program ptxcode  nvptxcompileresult nvptxcompilerdestroy nvptxcompilerhandle compiler destroys and cleans the already created ptx compiler nvptxcompileresult nvptxcompilergetcompiledprogram nvptxcompilerhandle compiler void binaryimage obtains the image of the compiled program nvptxcompileresult nvptxcompilergetcompiledprogramsize nvptxcompilerhandle compiler sizet binaryimagesize obtains the size of the image of the compiled program nvptxcompileresult nvptxcompilergeterrorlog nvptxcompilerhandle compiler char errorlog query the error message that was seen previously for the handle nvptxcompileresult nvptxcompilergeterrorlogsize nvptxcompilerhandle compiler sizet errorlogsize query the size of the error message that was seen previously for the handle nvptxcompileresult nvptxcompilergetinfolog nvptxcompilerhandle compiler char infolog query the information message that was seen previously for the handle nvptxcompileresult nvptxcompilergetinfologsize nvptxcompilerhandle compiler sizet infologsize query the size of the information message that was seen previously for the handle 1 functions nvptxcompileresult nvptxcompilercompile nvptxcompilerhandle compiler  int numcompileoptions  const char const compileoptions compile a ptx program with the given compiler options note 8212gpuname arch is a mandatory option parameters compiler inout a handle to ptx compiler initialized with the ptx program which is to be compiled the compiled program can be accessed using the handle numcompileoptions in length of the array compileoptions compileoptions in compiler options with which compilation should be done the compiler options string is a null terminated character array a valid list of compiler options is at link  returns nvptxcompilesuccess nvptxcompileerroroutofmemory nvptxcompileerrorinternal nvptxcompileerrorinvalidprogramhandle nvptxcompileerrorcompilationfailure nvptxcompileerrorunsupportedptxversion nvptxcompileerrorunsupporteddevsidesync nvptxcompileresult nvptxcompilercreate nvptxcompilerhandle compiler  sizet ptxcodelen  const char ptxcode obtains the handle to an instance of the ptx compiler initialized with the given ptx program ptxcode  parameters compiler out returns a handle to ptx compiler initialized with the ptx program ptxcode ptxcodelen in size of the ptx program ptxcode passed as string ptxcode in the ptx program which is to be compiled passed as string returns nvptxcompilesuccess nvptxcompileerroroutofmemory nvptxcompileerrorinternal nvptxcompileresult nvptxcompilerdestroy nvptxcompilerhandle compiler destroys and cleans the already created ptx compiler parameters compiler in a handle to the ptx compiler which is to be destroyed returns nvptxcompilesuccess nvptxcompileerroroutofmemory nvptxcompileerrorinternal nvptxcompileerrorinvalidprogramhandle nvptxcompileresult nvptxcompilergetcompiledprogram nvptxcompilerhandle compiler  void binaryimage obtains the image of the compiled program note nvptxcompilercompile api should be invoked for the handle before calling this api otherwise nvptxcompileerrorcompilerinvocationincomplete is returned parameters compiler,https://docs.nvidia.com/cuda/demo-suite/index.html,cuda demo suite
cuda demo suite 1 introduction v125 pdf archive cuda demo suite the reference guide for the cuda demo suite 1 introduction the cuda demo suite contains prebuilt applications which use cuda these applications demonstrate the capabilities and details of nvidia gpus 2 demos below are the demos within the demo suite  devicequery this application enumerates the properties of the cuda devices present in the system and displays them in a human readable format  vectoradd this application is a very basic demo that implements element by element vector addition  bandwidthtest this application provides the memcopy bandwidth of the gpu and memcpy bandwidth across pcie this application is capable of measuring device to device copy bandwidth host to device copy bandwidth for pageable and pagelocked memory and device to host copy bandwidth for pageable and pagelocked memory arguments usage bandwidthtest option test the bandwidth for device to host host to device and device to device transfers example measure the bandwidth of device to host pinned memory copies in the range 1024 bytes to 102400 bytes in 1024 byte increments bandwidthtest memorypinned moderange start1024 end102400 increment1024 dtoh options explanation help display this help menu csv print results as a csv devicedeviceno all 012n specify the device device to be used compute cumulative bandwidth on all the devices specify any particular device to be used memorymemmode pageable pinned specify which memory mode to use pageable memory nonpageable system memory modemode quick range shmoo specify the mode to use performs a quick measurement measures a userspecified range of values performs an intense shmoo of a large range of values htod measure host to device transfers dtoh measure device to host transfers dtod measure device to device transfers wc allocate pinned memory as writecombined cputiming force cpubased timing always range mode options startsize endsize incrementsize starting transfer size in bytes ending transfer size in bytes increment size in bytes  busgrind provides detailed statistics about peertopeer memory bandwidth amongst gpus present in the system as well as pinned unpinned memory bandwidth arguments options explanation h print usage p 01 enable or disable pinned memory tests default on u 01 enable or disable unpinned memory tests default off e 01 enable or disable p2p enabled memory tests default off d 01 enable or disable p2p disabled memory tests default off a enable all tests n disable all tests order of parameters matters examples busgrind n p 1 e 1 run all pinned and p2p tests busgrind n u 1 runs only unpinned tests busgrind a runs all tests pinned unpinned p2p enabled p2p disabled  nbody this demo does an efficient allpairs simulation of a gravitational nbody simulation in cuda it scales the nbody simulation across multiple gpus in a single pc if available adding numbodiesnumofbodies to the command line will allow users to set of bodies for simulation adding numdevicesn to the command line option will cause the sample to use n devices if available for simulation in this mode the position and velocity data for all bodies are read from system memory using zero copy rather than from device memory for a small number of devices 4 or fewer and a large enough number of bodies bandwidth is not a bottleneck so we can achieve strong scaling across these devices arguments options explanation fullscreen run nbody simulation in fullscreen mode fp64 use double precision floating point values for simulation hostmem stores simulation data in host memory benchmark run benchmark to measure performance numbodiesn number of bodies 1 to run in simulation deviced where d012 for the cuda device to use numdevicesi where inumber of cuda devices 0 to use for simulation compare compares simulation results running once on the default gpu and once on the cpu cpu run nbody simulation on the cpu tipsyfilebin load a tipsy model file for simulation  oceanfft this is a graphical demo which simulates an ocean height field using the cufft library and renders the result using opengl the following keys can be used to control the output keys function w toggle wireframe  randomfog this is a graphical demo which does pseudo and quasi random numbers visualization produced by curand on creation randomfog generates 200000 random coordinates in spherical coordinate space radius angle rho angle theta with curands xorwow algorithm the coordinates are normalized for a uniform distribution through the sphere the x axis is drawn with blue in the negative direction and yellow positive the y axis is drawn with green in the negative direction and magenta positive the z axis is drawn with red in the negative direction and cyan positive the following keys can be used to control the output keys function s generate new set of random nos and display as spherical coordinates sphere e generate new set of random nos and display on a spherical surface shell b generate new set of random nos and display as cartesian coordinates cubebox p generate new set of random nos and display on a cartesian plane plane i l j rotate the negative zaxis up right down and left respectively a toggle autorotation t toggle 10x zoom z toggle axes display x select xorwow generator default c select sobol generator v select scrambled sobol generator r reset xorwow ie reset to initial seed and regenerate increment the number of sobol dimensions and regenerate reset the number of sobol dimensions to 1 and regenerate increment the number of displayed points by 8000 max 200000 decrement the number of displayed points by 8000 down to min 8000 qesc quit the application 3 notices  notice this document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality condition or quality of a product nvidia corporation nvidia makes no representations or warranties expressed or implied as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein nvidia shall have no liability for the consequences or use of,https://docs.nvidia.com/cuda/wsl-user-guide/index.html,cuda on wsl
cuda on wsl 1 nvidia gpu accelerated computing on wsl 2 v125 pdf archive cuda on wsl user guide the guide for using nvidia cuda on windows subsystem for linux 1 nvidia gpu accelerated computing on wsl 2 wsl or windows subsystem for linux is a windows feature that enables users to run native linux applications containers and commandline tools directly on windows 11 and later os builds cuda support in this user guide is specifically for wsl 2 which is the second generation of wsl that offers the following benefits linux applications can run as is in wsl 2 wsl 2 is characteristically a vm with a linux wsl kernel in it that provides full compatibility with mainstream linux kernel allowing support for native linux applications including popular linux distros faster file system support and thats more performant wsl 2 is tightly integrated with the microsoft windows operating system which allows it to run linux applications alongside and even interop with other windows desktop and modern store apps for the rest of this user guide wsl and wsl 2 may be used interchangeably typically developers working across both linux and windows environments have a very disruptive workflow they either have to use different systems for linux and windows or dual boot ie install linux and windows in separate partitions on the same or different hard disks on the system and boot to the os of choice in both cases developers have to stop all the work and then switch the system or reboot also this has historically restricted the development of seamless well integrated tools and software systems across two dominant ecosystems wsl enables users to have a seamless transition across the two environments without the need for a resource intensive traditional virtual machine and to improve productivity and develop using tools and integrate their workflow more importantly wsl 2 enables applications that were hitherto only available on linux to be available on windows wsl 2 support for gpu allows for these applications to benefit from gpu accelerated computing and expands the domain of applications that can be developed on wsl 2 with nvidia cuda support for wsl 2 developers can leverage nvidia gpu accelerated computing technology for data science machine learning and inference on windows through wsl gpu acceleration also serves to bring down the performance overhead of running an application inside a wsl like environment close to nearnative by being able to pipeline more parallel work on the gpu with less cpu intervention nvidia driver support for wsl 2 includes not only cuda but also directx and direct ml support for some helpful examples see httpsdocsmicrosoftcomenuswindowswin32direct3d12gputensorflowwsl  wsl 2 is a key enabler in making gpu acceleration to be seamlessly shared between windows and linux applications on the same system a reality this offers flexibility and versatility while also serving to open up gpu accelerated computing by making it more accessible figure 1 illustration of the possibilities with nvidia cuda software stack on wsl 2 this document describes a workflow for getting started with running cuda applications or containers in a wsl 2 environment  nvidia compute software support on wsl 2 this table captures the readiness and suggested software versions for nvidia software stack for wsl 2 package suggested versions installation nvidia windows driver x86 use the latest windows x86 production driver r495 and later windows will have cuda support for wsl 2 nvidiasmi will have a limited feature set on wsl 2 legacy cuda ipc apis are support from r510 windows x86 drivers can be directly downloaded from httpswwwnvidiacomdownloadindexaspx for wsl 2 support on pascal or later gpus docker support supported nvidia container toolkit minimum versions v2 with libnvidiacontainer 1 cli and docker desktop supported refer to httpsdocsnvidiacomaienterprisedeploymentguidevmware010dockerhtml  cuda toolkit and cuda developer tools preview support compute sanitizer pascal and later nsight systems cli and cupti trace volta and later developer tools debuggers pascal and later using driver r535 developer tools profilers volta and later using windows 10 os build 19044 with driver r545 or using windows 11 with driver r525 latest linux cuda toolkit package wslubuntu from 12x releases can be downloaded from httpsdevelopernvidiacomcudadownloads  rapids  or later  experimental support for single gpu httpsdocsrapidsainoticesrgn0024 nccl  or later  refer to the nccl installation guide for linux x86  2 getting started with cuda on wsl 2 to get started with running cuda on wsl complete these steps in order  step 1 install nvidia driver for gpu support install nvidia geforce game ready or nvidia rtx quadro windows 11 display driver on your system with a compatible geforce or nvidia rtxquadro card from httpswwwnvidiacomdownloadindexaspx  refer to the system requirements in the appendix note this is the only driver you need to install do not install any linux display driver in wsl  step 2 install wsl 2 launch your preferred windows terminal command prompt powershell and install wsl wslexe install ensure you have the latest wsl kernel wslexe update  step 3 set up a linux development environment from a windows terminal enter wsl c wslexe the default distro is ubuntu to update the distro to your favorite distro from the command line and to review other wsl commands refer to the following resources httpsdocsmicrosoftcomenuswindowswslinstall httpsdocsmicrosoftcomenuswindowswslbasiccommands from this point you should be able to run any existing linux application which requires cuda do not install any driver within the wsl environment for building a cuda application you will need cuda toolkit read the next section for further information 3 cuda support for wsl 2 the latest nvidia windows gpu driver will fully support wsl 2 with cuda support in the driver existing applications compiled elsewhere on a linux system for the same target gpu can run unmodified within the wsl environment to compile new cuda applications a cuda toolkit for linux x86 is needed cuda toolkit support for wsl is still in preview stage as developer tools such as profilers are not available yet however cuda application development is,https://docs.nvidia.com/cuda/eflow-users-guide/index.html,cuda on eflow
eflow users guide 1 eflow users guide v125 pdf archive eflow users guide describes how cuda and nvidia gpu accelerated cloud native applications can be deployed on eflow enabled windows devices 1 eflow users guide  introduction azure iot edge for linux on windows otherwise referred to as eflow is a microsoft technology for the deployment of linux ai containers on windows edge devices this document details how nvidia cuda and nvidia gpu accelerated cloud native applications can be deployed on such eflowenabled windows devices eflow has the following components the windows host os with virtualization enabled a linux virtual machine iot edge runtime iot edge modules or otherwise any dockercompatible containerized application runs on mobycontainerd gpuaccelerated iot edge modules support for geforce rtx gpus is based on the gpu paravirtualization that was foundational to cuda on windows subsystem on linux so cuda and compute support for eflow comes by virtue of existing cuda support on wsl 2 cuda on wsl 2 boosted the productivity of cuda developers by enabling them to build develop and containerize gpu accelerated nvidia aiml linux applications on windows desktop computers before deployment on linux instances on the cloud but eflow is aimed at deployment for ai at the edge a containerized nvidia gpu accelerated linux application that is either hosted on azure iot hub or ngc registry can be seamlessly deployed at the edge such as a retail service center or hospitals these edge deployments are typically it managed devices entrenched with windows devices for manageability but the advent of aiml use cases in this space seek the convergence for linux and windows applications not only to coexist but also seamlessly communicate on the same device because cuda support on eflow is predominantly based on wsl 2 refer to the software support limitations and known issues sections in the cuda on wsl 2 document to stay abreast of the scope of nvidia software support available on eflow as well any additional prerequisites for eflow are covered in this document the following sections details installation of eflow prerequisites for outofthebox cuda support followed by sample instructions for running an existing gpu accelerated container on eflow  setup and installation follow the microsoft eflow documentation page for various installation options suiting your needs for uptodate installation instructions visit httpakamsazeflowinstall  for details on the eflow powershell api visit httpakamsazeflowpowershell  for quick setup we have included the steps for installation through powershell in the following sections 1 driver installation on the target windows device first install an nvidia geforce or nvidia rtx gpu windows driver that is compatible with the nvidia gpu on your device eflow vm supports deploying containerized cuda applications and hence only the driver must be installed on the host system cuda toolkit cannot be installed directly within eflow nvidiaprovided cuda containers from the ngc registry can be deployed directly if you are preparing a cuda docker container ensure that the necessary toolchains are installed because eflow is based on wsl the restrictions of the software stack for a hybrid linux on windows environment apply and not all of the nvidia software stack is supported refer to the users guide of the sdk that you are interested in to determine support 2 installation of eflow in an elevated powershell prompt perform the following enable hyperv enablewindowsoptionalfeature online featurename microsofthyperv all path online true restartneeded false set execution policy and verify setexecutionpolicy executionpolicy allsigned force getexecutionpolicy allsigned download and install eflow msipath iopathcombineenvtemp azureiotedgemsi progresspreference silentlycontinue invokewebrequest httpsakamsazeflowmsi14ltsx64 outfile msipath startprocess wait msiexec argumentlist iiopathcombineenvtemp azureiotedgemsiqn determine host os configuration geteflowhostconfiguration formatlist freephysicalmemoryinmb 35502 numberoflogicalprocessors 64 64 diskinfo drivec freesizeingb798 gpuinfo count1 supportedpassthroughtypessystemobject namenvidia rtx a2000 deploy eflow deploying eflow will set up the eflow runtime and virtual machine by default eflow only reserves 1024mb of system memory for use for the workloads and that is insufficient to support gpu accelerated configurations for gpu acceleration you will have to reserve system memory explicitly at eflow deployment otherwise there will not be sufficient system memory for your containerized applications to run in order to prevent out of memory errors reserve memory explicitly as required see example below refer to command line argument options available for deploying eflow in the official documentation for more details 3 prerequisites for cuda support x86 64bit support only geforce rtx gpu products windows 1011 pro enterprise iot enterprise windows 10 users must use the november 2021 update build  or higher deployeflow only allocates 1024 mb memory by default set it to a larger value to prevent oom issue check ms documents for more details at httpslearnmicrosoftcomenusazureiotedgereferenceiotedgeforlinuxonwindowsfunctionsdeployeflow  other prerequisites specific to the platform also apply refer to httpslearnmicrosoftcomenusazureiotedgegpuaccelerationviewiotedge14   connecting to the eflow vm geteflowvmaddr 10132022 114116 querying ip and mac addresses from virtual machine ipp11490eflow virtual machine mac 00155db240c7 virtual machine ip  retrieved directly from virtual machine 00155db240c7  connecteflowvm  running nvidiasmi  running gpuaccelerated containers let us run an nbody simulation containerized cuda sample from ngc but this time inside eflow iotedgeuseripp11490eflow sudo docker run gpus all env nvidiadisablerequire1 nvcrionvidiak8scudasamplenbody nbody gpu benchmark unable to find image nvcrionvidiak8scudasamplenbody locally nbody pulling from nvidiak8scudasample 22c5ef60a68e pull complete 1939e4248814 pull complete 548afb82c856 pull complete a424d45fd86f pull complete 207b64ab7ce6 pull complete f65423f1b49b pull complete 2b60900a3ea5 pull complete e9bff09d04df pull complete edc14edf1b04 pull complete 1f37f461c076 pull complete 9026fb14bf88 pull complete digest sha25659261e419d6d48a772aad5bb213f9f1588fcdb042b115ceb7166c89a51f03363 status downloaded newer image for nvcrionvidiak8scudasamplenbody run nbody benchmark numbodiesnumbodies to measure performance fullscreen run nbody simulation in fullscreen mode fp64 use double precision floating point values for simulation hostmem stores simulation data in host memory benchmark run benchmark to measure performance numbodiesn number of bodies 1 to run in simulation deviced where d012 for the cuda device to use numdevicesi where inumber of cuda devices 0 to use for simulation compare compares simulation results running once on the default gpu and once on the cpu cpu run nbody simulation on the cpu tipsyfilebin load a tipsy model file for simulation note the cuda samples are,https://docs.nvidia.com/cuda/debugger-api/index.html,debugger api
the api reference guide for the cuda debugger,https://docs.nvidia.com/cuda/gpudirect-rdma/index.html,gpudirect rdma
gpudirect rdma 1 overview v125 pdf archive developing a linux kernel module using gpudirect rdma the api reference guide for enabling gpudirect rdma connections to nvidia gpus 1 overview gpudirect rdma is a technology introduced in keplerclass gpus and cuda  that enables a direct path for data exchange between the gpu and a thirdparty peer device using standard features of pci express examples of thirdparty devices are network interfaces video acquisition devices storage adapters gpudirect rdma is available on both tesla and quadro gpus a number of limitations can apply the most important being that the two devices must share the same upstream pci express root complex some of the limitations depend on the platform used and could be lifted in currentfuture products a few straightforward changes must be made to device drivers to enable this functionality with a wide range of hardware devices this document introduces the technology and describes the steps necessary to enable an gpudirect rdma connection to nvidia gpus on linux gpudirect rdma within the linux device driver model  how gpudirect rdma works when setting up gpudirect rdma communication between two peers all physical addresses are the same from the pci express devices point of view within this physical address space are linear windows called pci bars each device has six bar registers at most so it can have up to six active 32bit bar regions 64bit bars consume two bar registers the pci express device issues reads and writes to a peer devices bar addresses in the same way that they are issued to system memory traditionally resources like bar windows are mapped to user or kernel address space using the cpus mmu as memory mapped io mmio addresses however because current operating systems dont have sufficient mechanisms for exchanging mmio regions between drivers the nvidia kernel driver exports functions to perform the necessary address translations and mappings to add gpudirect rdma support to a device driver a small amount of address mapping code within the kernel driver must be modified this code typically resides near existing calls to getuserpages  the apis and control flow involved with gpudirect rdma are very similar to those used with standard dma transfers see supported systems and pci bar sizes for more hardware details  standard dma transfer first we outline a standard dma transfer initiated from userspace in this scenario the following components are present userspace program userspace communication library kernel driver for the device interested in doing dma transfers the general sequence is as follows the userspace program requests a transfer via the userspace communication library this operation takes a pointer to data a virtual address and a size in bytes the communication library must make sure the memory region corresponding to the virtual address and size is ready for the transfer if this is not the case already it has to be handled by the kernel driver next step the kernel driver receives the virtual address and size from the userspace communication library it then asks the kernel to translate the virtual address range to a list of physical pages and make sure they are ready to be transferred to or from we will refer to this operation as pinning the memory the kernel driver uses the list of pages to program the physical devices dma engines the communication library initiates the transfer after the transfer is done the communication library should eventually clean up any resources used to pin the memory we will refer to this operation as unpinning the memory  gpudirect rdma transfers for the communication to support gpudirect rdma transfers some changes to the sequence above have to be introduced first of all two new components are present userspace cuda library nvidia kernel driver as described in basics of uva cuda memory management  programs using the cuda library have their address space split between gpu and cpu virtual addresses and the communication library has to implement two separate paths for them the userspace cuda library provides a function that lets the communication library distinguish between cpu and gpu addresses moreover for gpu addresses it returns additional metadata that is required to uniquely identify the gpu memory represented by the address see userspace api for details the difference between the paths for cpu and gpu addresses is in how the memory is pinned and unpinned for cpu memory this is handled by builtin linux kernel functions getuserpages and putpage  however in the gpu memory case the pinning and unpinning has to be handled by functions provided by the nvidia kernel driver see pinning gpu memory and unpinning gpu memory for details some hardware caveats are explained in supported systems and pci bar sizes   changes in cuda  in this section we briefly list the changes that are available in cuda  cuda peertopeer tokens are no longer mandatory for memory buffers owned by the calling process which is typical tokens can be replaced by zero 0 in the kernelmode function nvidiap2pgetpages  this new feature is meant to make it easier for existing third party software stacks to adopt rdma for gpudirect as a consequence of the change above a new api cupointersetattribute has been introduced this api must be used to register any buffer for which no peertopeer tokens are used it is necessary to ensure correct synchronization behavior of the cuda api when operation on memory which may be read by rdma for gpudirect failing to use it in these cases may cause data corruption see changes in tokens usage  cupointergetattribute has been extended to return a globally unique numeric identifier which in turn can be used by lowerlevel libraries to detect buffer reallocations happening in userlevel code see userspace api  it provides an alternative method to detect reallocations when intercepting cuda allocation and deallocation apis is not possible the kernelmode memory pinning feature has been extended to work in combination with multiprocess service mps caveats as of cuda  cuda unified,https://docs.nvidia.com/cuda/vGPU/index.html,vgpu
vgpus and cuda 1 overview v125 pdf archive vgpus and cuda vgpus that support cuda this page describes the support for cuda on nvidia virtual gpu software 1 overview for more information on nvidia virtual gpu software visit httpsdocsnvidiacomgridindexhtml   vgpu the following vgpu releases support cuda cuda toolkit version vgpu software releases cuda  not supported cuda  update 1 1 17x releases cuda  17x releases cuda  update 2 2 not supported cuda  update 1 1 not supported cuda  not supported cuda  update 1 1 16x releases cuda  16x releases cuda  update 1 1 15x releases cuda  15x releases cuda  update 1 1 15x releases cuda  15x releases cuda  14x releases cuda  update 1 1 14x releases cuda  14x releases cuda  update 1 1 14x releases cuda  14x releases cuda  update 2 2 14x releases cuda  update 1 1 14x releases cuda  14x releases cuda  update 4 4 13x releases cuda  update 3 3 13x releases cuda  update 2 2 13x releases cuda  update 1 1 13x releases cuda  13x releases cuda  update 1 1 not supported cuda  not supported cuda  update 1 1 not supported cuda  12x releases cuda  update 1 1 not supported cuda  not supported cuda  11x releases cuda  10x releases cuda  update 2 243 not supported cuda  update 1 9x releases cuda  general release 105 8x releases cuda 130 7x releases cuda  148 update 1 not supported cuda  88 not supported cuda  85 6x releases cuda  176 5x releases some cuda features might not be supported by your version of nvidia virtual gpu software for details follow the link in the table to the documentation for your version  notices 1 notice this document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality condition or quality of a product nvidia corporation nvidia makes no representations or warranties expressed or implied as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein nvidia shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use this document is not a commitment to develop release or deliver any material defined below code or functionality nvidia reserves the right to make corrections modifications enhancements improvements and any other changes to this document at any time without notice customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete nvidia products are sold subject to the nvidia standard terms and conditions of sale supplied at the time of order acknowledgement unless otherwise agreed in an individual sales agreement signed by authorized representatives of nvidia and customer terms of sale nvidia hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the nvidia product referenced in this document no contractual obligations are formed either directly or indirectly by this document nvidia products are not designed authorized or warranted to be suitable for use in medical military aircraft space or life support equipment nor in applications where failure or malfunction of the nvidia product can reasonably be expected to result in personal injury death or property or environmental damage nvidia accepts no liability for inclusion andor use of nvidia products in such equipment or applications and therefore such inclusion andor use is at customers own risk nvidia makes no representation or warranty that products based on this document will be suitable for any specified use testing of all parameters of each product is not necessarily performed by nvidia it is customers sole responsibility to evaluate and determine the applicability of any information contained in this document ensure the product is suitable and fit for the application planned by customer and perform the necessary testing for the application in order to avoid a default of the application or the product weaknesses in customers product designs may affect the quality and reliability of the nvidia product and may result in additional or different conditions andor requirements beyond those contained in this document nvidia accepts no liability related to any default damage costs or problem which may be based on or attributable to i the use of the nvidia product in any manner that is contrary to this document or ii customer product designs no license either expressed or implied is granted under any nvidia patent right copyright or other nvidia intellectual property right under this document information published by nvidia regarding thirdparty products or services does not constitute a license from nvidia to use such products or services or a warranty or endorsement thereof use of such information may require a license from a third party under the patents or other intellectual property rights of the third party or a license from nvidia under the patents or other intellectual property rights of nvidia reproduction of information in this document is permissible only if approved in advance by nvidia in writing reproduced without alteration and in full compliance with all applicable export laws and regulations and accompanied by all associated conditions limitations and notices this document and all nvidia design specifications reference boards files drawings diagnostics lists and other documents together and separately materials are being provided as is nvidia makes no warranties expressed implied statutory or otherwise with respect to the materials and expressly disclaims all implied warranties of noninfringement merchantability and fitness for a particular purpose to the extent not prohibited by law in no event will nvidia be liable for any damages including without limitation any direct indirect special incidental punitive or consequential damages however caused and regardless of the theory of liability arising out of any use,https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html,nvcc
nvidia cuda compiler driver 1 introduction v125 pdf archive nvidia cuda compiler driver nvcc the documentation for nvcc  the cuda compiler driver 1 introduction  overview 1 cuda programming model the cuda toolkit targets a class of applications whose control part runs as a process on a general purpose computing device and which use one or more nvidia gpus as coprocessors for accelerating single program multiple data spmd parallel jobs such jobs are selfcontained in the sense that they can be executed and completed by a batch of gpu threads entirely without intervention by the host process thereby gaining optimal benefit from the parallel graphics hardware the gpu code is implemented as a collection of functions in a language that is essentially c but with some annotations for distinguishing them from the host code plus annotations for distinguishing different types of data memory that exists on the gpu such functions may have parameters and they can be called using a syntax that is very similar to regular c function calling but slightly extended for being able to specify the matrix of gpu threads that must execute the called function during its life time the host process may dispatch many parallel gpu tasks for more information on the cuda programming model consult the cuda c programming guide  2 cuda sources source files for cuda applications consist of a mixture of conventional c host code plus gpu device functions the cuda compilation trajectory separates the device functions from the host code compiles the device functions using the proprietary nvidia compilers and assembler compiles the host code using a c host compiler that is available and afterwards embeds the compiled gpu functions as fatbinary images in the host object file in the linking stage specific cuda runtime libraries are added for supporting remote spmd procedure calling and for providing explicit gpu manipulation such as allocation of gpu memory buffers and hostgpu data transfer 3 purpose of nvcc the compilation trajectory involves several splitting compilation preprocessing and merging steps for each cuda source file it is the purpose of nvcc  the cuda compiler driver to hide the intricate details of cuda compilation from developers it accepts a range of conventional compiler options such as for defining macros and includelibrary paths and for steering the compilation process all noncuda compilation steps are forwarded to a c host compiler that is supported by nvcc  and nvcc translates its options to appropriate host compiler command line options  supported host compilers a general purpose c host compiler is needed by nvcc in the following situations during noncuda phases except the run phase because these phases will be forwarded by nvcc to this compiler during cuda phases for several preprocessing stages and host code compilation see also the cuda compilation trajectory  nvcc assumes that the host compiler is installed with the standard method designed by the compiler provider if the host compiler installation is nonstandard the user must make sure that the environment is set appropriately and use relevant nvcc compile options the following documents provide detailed information about supported host compilers nvidia cuda installation guide for linux nvidia cuda installation guide for microsoft windows on all platforms the default host compiler executable gcc and g on linux and clexe on windows found in the current execution search path will be used unless specified otherwise with appropriate options see file and path specifications  note nvcc does not support the compilation of file paths that exceed the maximum path length limitations of the host system to support the compilation of long file paths please refer to the documentation for your system 2 compilation phases  nvcc identification macro nvcc predefines the following macros nvcc defined when compiling cccuda source files cudacc defined when compiling cuda source files cudaccrdc defined when compiling cuda source files in relocatable device code mode see nvcc options for separate compilation  cudaccewp defined when compiling cuda source files in extensible whole program mode see options for specifying behavior of compilerlinker  cudaccdebug defined when compiling cuda source files in the devicedebug mode see options for specifying behavior of compilerlinker  cudaccrelaxedconstexpr defined when the exptrelaxedconstexpr flag is specified on the command line refer to the cuda c programming guide for more details cudaccextendedlambda defined when the exptextendedlambda or extendedlambda flag is specified on the command line refer to the cuda c programming guide for more details cudaccvermajor defined with the major version number of nvcc  cudaccverminor defined with the minor version number of nvcc  cudaccverbuild defined with the build version number of nvcc  nvccdiagpragmasupport defined when the cuda frontend compiler supports diagnostic control with the nvdiagsuppress  nvdiagerror  nvdiagwarning  nvdiagdefault  nvdiagonce  and nvdiagnostic pragmas  nvcc phases a compilation phase is a logical translation step that can be selected by command line options to nvcc  a single compilation phase can still be broken up by nvcc into smaller steps but these smaller steps are just implementations of the phase they depend on seemingly arbitrary capabilities of the internal tools that nvcc uses and all of these internals may change with a new release of the cuda toolkit hence only compilation phases are stable across releases and although nvcc provides options to display the compilation steps that it executes these are for debugging purposes only and must not be copied and used in build scripts nvcc phases are selected by a combination of command line options and input file name suffixes and the execution of these phases may be modified by other command line options in phase selection the input file suffix defines the phase input while the command line option defines the required output of the phase the following paragraphs list the recognized file name suffixes and the supported compilation phases a full explanation of the nvcc command line options can be found in nvcc command options   supported input file suffixes the following table defines how nvcc interprets,https://docs.nvidia.com/cuda/cuda-gdb/index.html,cudagdb
cudagdb 1 introduction v125 pdf archive cudagdb the user manual for cudagdb the nvidia tool for debugging cuda applications on linux and qnx systems 1 introduction this document introduces cudagdb the nvidia cuda debugger for linux and qnx targets  what is cudagdb cudagdb is the nvidia tool for debugging cuda applications running on linux and qnx cudagdb is an extension to gdb the gnu project debugger the tool provides developers with a mechanism for debugging cuda applications running on actual hardware this enables developers to debug applications without the potential variations introduced by simulation and emulation environments  supported features cudagdb is designed to present the user with a seamless debugging environment that allows simultaneous debugging of both gpu and cpu code within the same application just as programming in cuda c is an extension to c programming debugging with cudagdb is a natural extension to debugging with gdb the existing gdb debugging features are inherently present for debugging the host code and additional features have been provided to support debugging cuda device code cudagdb supports debugging cc and fortran cuda applications fortran debugging support is limited to 64bit linux operating system cudagdb allows the user to set breakpoints to singlestep cuda applications and also to inspect and modify the memory and variables of any given thread running on the hardware cudagdb supports debugging all cuda applications whether they use the cuda driver api the cuda runtime api or both cudagdb supports debugging kernels that have been compiled for specific cuda architectures such as sm75 or sm80  but also supports debugging kernels compiled at runtime referred to as justintime compilation or jit compilation for short  about this document this document is the main documentation for cudagdb and is organized more as a user manual than a reference manual the rest of the document will describe how to install and use cudagdb to debug cuda kernels and how to use the new cuda commands that have been added to gdb some walkthrough examples are also provided it is assumed that the user already knows the basic gdb commands used to debug host applications 2 release notes  release updated gdb version moved from gdb  to  see gdb  changes support removal notice support for the macos host client of cudagdb has been removed support for android has been removed support for python  and  has been removed features multi build feature that supports native python and tui mode across all supported platforms the cudagdb program is now a wrapper script that calls the appropriate cudagdb binary if no supported python or libncurses is detected the wrapper will fallback to a cudagdb binary with python and tui support disabled added support for tui mode added support for python   and  added support for detecting and printing exceptions encountered in exited warps this can occur when debugging an application with optimizations enabled added new gdbmi command equivalents for info cuda managed and info cuda line fixed issues fixed issue with printing reference parameter arguments to cuda functions fixed issues resulting in crasheserrors when readingwriting fromto cuda generic memory fixed issue where breakonlaunch breakpoints were missed for back to back launches of the same kernel fixed issue with incorrectly reporting breakpoint hit events as sigtrap when breakpoint is hit in divergent thread fixed crash on qnx when cudagdbserver packets arrive outoforder better error handling when encountering an error when reading cuda disassembly better exit handling when resuming execution from a fatal cuda exception  release updated gdb version moved from gdb  to  see gdb  changes android deprecation notice support for android is deprecated it will be dropped in an upcoming release python  and  deprecation notice support for endoflife python  and  versions is deprecated it will be dropped in an upcoming release features performance enhancement which reduces the number of overall cuda debugger api calls performance enhancement when loading large cubins with device functions using a large number of gpu registers performance enhancement when single stepping over warp wide barriers added support for printing values contained within constant banks from gpu core dumps fixed issues prevented shell expansion on cloned function names when disassembling fixed crash when setting a conditional breakpoint on an unknown symbol name fixed issue with setting a watchpoint on a global pointer fixed assertion in switchtothread1 during inferior teardown fixed attach failures encountered with newer intel processors refactored the libpython layer to avoid unnecessary gdb code changes  release macos host client deprecation notice support for the macos host client of cudagdb is deprecated it will be dropped in an upcoming release features added support for printing values contained within constant banks new cudaconstbankbank offset convenience function to obtain address of offset in constant bank see const banks  performance enhancements added which reduce overhead when running applications with many cuda threads added support for cuda function pointers fixed issues fixed issue when detaching from attached process that can result in a crash fixed thread ordering issues present with several info cuda commands added support for opening of gpu core dumps when no valid warps are present on the device added missing dwarf operators used by optix fixed issue with parsing cuda fortran pointer types fixed issue where cuda cluster coordinates were being displayed when no cuda cluster was present  release features enabled printing of extended error messages when a cuda debugger api error is encountered enabled support for debugging with confidential compute mode with devtools mode see confidential computing deployment guide httpsdocsnvidiacomconfidentialcomputingdeploymentguidepdf for more details on how to enable the mode fixed issues fixed  appearing in backtrace in optix applications host shadow breakpoints are now handled correctly with cuda lazy loading enabled fixed name mangling issue when debugging llvm generated cubins cuda cluster coordinates are now displayed correctly fixed issue with attaching to an application using cuda lazy loading when debugging remotely with cudagdbserver  release cuda driver api added for controlling core,https://docs.nvidia.com/cuda/nsightee-plugins-install-guide/index.html,nsight eclipse plugins installation guide
nsight eclipse plugins installation guide 1 introduction v125 pdf archive nsight eclipse plugins installation guide the user guide for installing nsight eclipse plugins 1 introduction this guide provides the procedures to install the nsight eclipse edition plugins in users own eclipse environment nsight eclipse plugins offers fullfeatured ide that provides an allinone integrated environment to edit build debug and profile cudac applications  install plugins using eclipse ide you can install nsight eclipse plugins in your own eclipse environment or download and install eclipse ide for cc developers  launch eclipse and go to help install new software menu click on the add button enter name ex nsightee in the name field choose the the zip filecomnvidiacudarepozip that contains the plugins using archive button or enter the full path of zip file nsight ee plugns zip file can be found in usrlocalcuda118nsighteeplugins directory click ok button select cuda main features option and go to next page accept the license agreement and click on finish button to install the plugins click ok on the security warning dialog to ignore the warning message about unsigned content this warning message is displayed for all the plugins that are not signed by eclipseorg restart eclipse when prompted nsight eclipse plugins installation is now completego to help installation details menu to verify the cuda developer tools and cuda remote launch plugins are installed  uninstall plugins using eclipse ide launch eclipse and go to help installation details menu select cuda developer tools and cuda remote launch options from the dialog click on the uninstall button click finish button when asked to review and confirm restart eclipse when prompted nsight eclipse plugins will be uninstalled after restarting eclipse go to help installation details menu to verify  install using script to install or uninstall the nsight eclipse plugins using the script run the installation script provided in the bin directory of the toolkit by default it is located in usrlocalcuda118bin the usage of the script is as follows usage nsighteepluginsmanagesh action eclipsedir action install or uninstall eclipsedir eclipse installation directory to install the nsight eclipse plugins run the following command usrlocalcuda118binnsighteepluginsmanagesh install eclipsedir to uninstall the nsight eclipse plugins run the following command usrlocalcuda118binnsighteepluginsmanagesh uninstall eclipsedir 2 notices  notice this document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality condition or quality of a product nvidia corporation nvidia makes no representations or warranties expressed or implied as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein nvidia shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use this document is not a commitment to develop release or deliver any material defined below code or functionality nvidia reserves the right to make corrections modifications enhancements improvements and any other changes to this document at any time without notice customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete nvidia products are sold subject to the nvidia standard terms and conditions of sale supplied at the time of order acknowledgement unless otherwise agreed in an individual sales agreement signed by authorized representatives of nvidia and customer terms of sale nvidia hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the nvidia product referenced in this document no contractual obligations are formed either directly or indirectly by this document nvidia products are not designed authorized or warranted to be suitable for use in medical military aircraft space or life support equipment nor in applications where failure or malfunction of the nvidia product can reasonably be expected to result in personal injury death or property or environmental damage nvidia accepts no liability for inclusion andor use of nvidia products in such equipment or applications and therefore such inclusion andor use is at customers own risk nvidia makes no representation or warranty that products based on this document will be suitable for any specified use testing of all parameters of each product is not necessarily performed by nvidia it is customers sole responsibility to evaluate and determine the applicability of any information contained in this document ensure the product is suitable and fit for the application planned by customer and perform the necessary testing for the application in order to avoid a default of the application or the product weaknesses in customers product designs may affect the quality and reliability of the nvidia product and may result in additional or different conditions andor requirements beyond those contained in this document nvidia accepts no liability related to any default damage costs or problem which may be based on or attributable to i the use of the nvidia product in any manner that is contrary to this document or ii customer product designs no license either expressed or implied is granted under any nvidia patent right copyright or other nvidia intellectual property right under this document information published by nvidia regarding thirdparty products or services does not constitute a license from nvidia to use such products or services or a warranty or endorsement thereof use of such information may require a license from a third party under the patents or other intellectual property rights of the third party or a license from nvidia under the patents or other intellectual property rights of nvidia reproduction of information in this document is permissible only if approved in advance by nvidia in writing reproduced without alteration and in full compliance with all applicable export laws and regulations and accompanied by all associated conditions limitations and notices this document and all nvidia design specifications reference boards files drawings diagnostics lists and other documents together and separately materials are being provided as is nvidia makes no warranties expressed implied statutory or otherwise with respect to the materials,https://docs.nvidia.com/cuda/nsight-eclipse-plugins-guide/index.html,nsight eclipse plugins edition
nsight eclipse plugins guide 1 introduction v125 pdf archive nsight eclipse plugins edition getting started guide the user guide for using nsight eclipse plugins edition 1 introduction this guide introduces nsight eclipse plugins edition and provides instructions necessary to start using this tool nsight eclipse is based on eclipse cdt project for a detailed description of eclipse cdt features consult the integrated help cc development user guide available from inside nsight through helphelp contents menu  about nsight eclipse plugins edition nvidia nsight eclipse edition is a unified cpu plus gpu integrated development environment ide for developing cuda applications on linux and mac os x for the x86 power and arm platforms it is designed to help developers on all stages of the software development process nsight eclipse plugins can be installed on vanilla eclipse using the standard helpinstall new software menu the principal features are as follows edit build debug and profile cudac applications cuda aware source code editor syntax highlighting code completion and inline help graphical user interface for debugging heterogeneous applications profiler integration launch visual profiler as an external application with the cuda application built in this ide to easily identify performance bottlenecks for more information about eclipse platform visit httpeclipseorg 2 using nsight eclipse edition  installing nsight eclipse edition nsight eclipse plugins archive is part of the cuda toolkit nsight eclipse plugins archive can be installed using the help install new software menu on eclipse 1 installing cuda toolkit to install cuda toolkit visit the nvidia cuda toolkit download page httpsdevelopernvidiacomcudadownloads select appropriate operating system nsight eclipse edition is available in mac os x and linux toolkit packages download and install the cuda driver download and install the cuda toolkit follow instructions to configure cuda driver and toolkit on your system 2 configure cuda toolkit path when eclipse is first launched with nsight eclipse plugins in the new workspace nvidia usage data collection dialog will be displayed as below click yes to enable usage collection this can be disabled later from the cuda preference page usage data collection page to get started cuda toolkit path must be configured in eclipse with nsight plugins open the preferences page window preferences go to cuda toolkit section select the cuda toolkit path to be used by nsight cuda tookits that are installed in the default location will automatically appear cuda toolkit path can be also specified in the project properties page in order to use different toolkit for a project enable usage data collection if you wish to send usage data to nvidia click on the button to set cudagdb and visual profiler as the default launchers for qnx when qnx is selected as target os a dialog will be displayed to set the qnxhost and qnxtarget environment variables if they were not already set qnxhost environment variable identifies the directory that holds the hostrelated components qnxtarget environment variable identifies the directory that holds the targetrelated components  nsight eclipse main window on the first run eclipse will ask to pick a workspace location the workspace is a folder where nsight will store its settings local files history and caches an empty folder should be selected to avoid overwriting existing files the main nsight window will open after the workspace location is selected the main window is divided into the following areas editor displays source files that are opened for editing project explorer displays project files outline displays structure of the source file in the current editor problems displays errors and warnings detected by static code analysis in ide or by a compiler during the build console displays make output during the build or output from the running application  creating a new project from the main menu open the new project wizard file new cuda cc project specify the project name and project files location specify the project type like executable project specify the cuda toolchain from the list of toolchains specify the project configurations on the next wizard page complete the wizard the project will be shown in the project explorer view and source editor will be opened build the project by clicking on the hammer button on the main toolbar nsight main window after creating a new project  importing cuda samples the cuda samples are an optional component of the cuda toolkit installation nsight provides a mechanism to import these samples and work with them easily note samples that use the cuda driver api suffixed with drv are not supported by nsight from the main menu open the new project wizard file new cuda cc project specify the project name and project files location select import cuda sample under executable in the project type tree select cuda toolchain from the toolchains option location on the next wizard page select project sample you want to import also select the target cpu architecture press next specify the project parameters on the next wizard page complete the wizard the project will be shown in the project explorer view and source editor will be opened build the project by clicking on the hammer button on the main toolbar 1 cuhook sample cuhook sample builds both the library and the executable cuhook sample should be imported as the makefile project using the following steps from the main menu open the new project wizard file new cuda cc project select project type makefile project and choose empty project specify the project name and project files location complete the wizard the project will be shown in the project explorer view right click on the project import general file system on the next wizard page select the location of cuhook samplesamples7cudalibrariescuhook select all the source files and makefile and finish the wizard build the project by clicking on the hammer button on the main toolbar to run the sample from the main menu run run configurations select the executable go to environment tab new enter nameldpreload valuelibcuhookso1 run will execute the sample  configure build settings to define build settings in the,https://docs.nvidia.com/cuda/profiler-users-guide/index.html,profiler
profiler 1 preparing an application for profiling v125 pdf archive profiler users guide the user manual for nvidia profiling tools for optimizing performance of cuda applications profiling overview this document describes nvidia profiling tools that enable you to understand and optimize the performance of your cuda openacc or openmp applications the visual profiler is a graphical profiling tool that displays a timeline of your applications cpu and gpu activity and that includes an automated analysis engine to identify optimization opportunities the nvprof profiling tool enables you to collect and view profiling data from the commandline note that visual profiler and nvprof will be deprecated in a future cuda release the nvidia volta platform is the last architecture on which these tools are fully supported it is recommended to use nextgeneration tools nvidia nsight systems for gpu and cpu sampling and tracing and nvidia nsight compute for gpu kernel profiling refer the migrating to nsight tools from visual profiler and nvprof section for more details terminology an event is a countable activity action or occurrence on a device it corresponds to a single hardware counter value which is collected during kernel execution to see a list of all available events on a particular nvidia gpu type nvprof queryevents  a metric is a characteristic of an application that is calculated from one or more event values to see a list of all available metrics on a particular nvidia gpu type nvprof querymetrics  you can also refer to the metrics reference  1 preparing an application for profiling the cuda profiling tools do not require any application changes to enable profiling however by making some simple modifications and additions you can greatly increase the usability and effectiveness profiling this section describes these modifications and how they can improve your profiling results  focused profiling by default the profiling tools collect profile data over the entire run of your application but as explained below you typically only want to profile the regions of your application containing some or all of the performancecritical code limiting profiling to performancecritical regions reduces the amount of profile data that both you and the tools must process and focuses attention on the code where optimization will result in the greatest performance gains there are several common situations where profiling a region of the application is helpful the application is a test harness that contains a cuda implementation of all or part of your algorithm the test harness initializes the data invokes the cuda functions to perform the algorithm and then checks the results for correctness using a test harness is a common and productive way to quickly iterate and test algorithm changes when profiling you want to collect profile data for the cuda functions implementing the algorithm but not for the test harness code that initializes the data or checks the results the application operates in phases where a different set of algorithms is active in each phase when the performance of each phase of the application can be optimized independently of the others you want to profile each phase separately to focus your optimization efforts the application contains algorithms that operate over a large number of iterations but the performance of the algorithm does not vary significantly across those iterations in this case you can collect profile data from a subset of the iterations to limit profiling to a region of your application cuda provides functions to start and stop profile data collection cudaprofilerstart is used to start profiling and cudaprofilerstop is used to stop profiling using the cuda driver api you get the same functionality with cuprofilerstart and cuprofilerstop  to use these functions you must include cudaprofilerapih or cudaprofilerh for the driver api when using the start and stop functions you also need to instruct the profiling tool to disable profiling at the start of the application for nvprof you do this with the profilefromstart off flag for the visual profiler you use the start execution with profiling enabled checkbox in the settings view   marking regions of cpu activity the visual profiler can collect a trace of the cuda function calls made by your application the visual profiler shows these calls in the timeline view  allowing you to see where each cpu thread in the application is invoking cuda functions to understand what the applications cpu threads are doing outside of cuda function calls you can use the nvidia tools extension api nvtx when you add nvtx markers and ranges to your application the timeline view shows when your cpu threads are executing within those regions nvprof also supports nvtx markers and ranges markers and ranges are shown in the api trace output in the timeline in summary mode each range is shown with cuda activities associated with that range  naming cpu and cuda resources the visual profiler timeline view shows default naming for cpu thread and gpu devices context and streams using custom names for these resources can improve understanding of the application behavior especially for cuda applications that have many host threads devices contexts or streams you can use the nvidia tools extension api to assign custom names for your cpu and gpu resources your custom names will then be displayed in the timeline view  nvprof also supports nvtx naming names of cuda devices contexts and streams are displayed in summary and trace mode thread names are displayed in summary mode  flush profile data to reduce profiling overhead the profiling tools collect and record profile information into internal buffers these buffers are then flushed asynchronously to disk with low priority to avoid perturbing application behavior to avoid losing profile information that has not yet been flushed the application being profiled should make sure before exiting that all gpu work is done using cuda synchronization calls and then call cudaprofilerstop or cuprofilerstop  doing so forces buffered profile information on corresponding contexts to be flushed if your cuda application includes graphics that operate using a display or,https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html,cuda binary utilities
cudabinaryutilities 1 overview v125 pdf archive cuda binary utilities the application notes for cuobjdump nvdisasm cufilt and nvprune 1 overview this document introduces cuobjdump  nvdisasm  cufilt and nvprune  four cuda binary tools for linux x86 arm and p9 windows mac os and android  what is a cuda binary a cuda binary also referred to as cubin file is an elfformatted file which consists of cuda executable code sections as well as other sections containing symbols relocators debug info etc by default the cuda compiler driver nvcc embeds cubin files into the host executable file but they can also be generated separately by using the cubin option of nvcc  cubin files are loaded at run time by the cuda driver api note for more details on cubin files or the cuda compilation trajectory refer to nvidia cuda compiler driver nvcc   differences between cuobjdump and nvdisasm cuda provides two binary utilities for examining and disassembling cubin files and host executables cuobjdump and nvdisasm  basically cuobjdump accepts both cubin files and host binaries while nvdisasm only accepts cubin files but nvdisasm provides richer output options heres a quick comparison of the two tools table 1 comparison of cuobjdump and nvdisasm cuobjdump nvdisasm disassemble cubin yes yes extract ptx and extract and disassemble cubin from the following input files host binaries executables object files static libraries external fatbinary files yes no control flow analysis and output no yes advanced display options no yes  command option types and notation this section of the document provides common details about the command line options for the following tools cuobjdump nvdisasm nvprune each commandline option has a long name and a short name which are interchangeable with each other these two variants are distinguished by the number of hyphens that must precede the option name ie long names must be preceded by two hyphens and short names must be preceded by a single hyphen for example i is the short name of includepath  long options are intended for use in build scripts where size of the option is less important than descriptive value and short options are intended for interactive use the tools mentioned above recognize three types of command options boolean options single value options and list options boolean options do not have an argument they are either specified on a command line or not single value options must be specified at most once and list options may be repeated examples of each of these option types are respectively boolean option nvdisams printraw file single value nvdisasm binary sm70 file list options cuobjdump function foobarfoobar file single value options and list options must have arguments which must follow the name of the option by either one or more spaces or an equals character when a onecharacter short name such as i  l  and l is used the value of the option may also immediately follow the option itself without being seperated by spaces or an equal character the individual values of list options may be separated by commas in a single instance of the option or the option may be repeated or any combination of these two cases hence for the two sample options mentioned above that may take values the following notations are legal o file ofile idir1dir2 idir3 i dir4dir5 for options taking a single value if specified multiple times the rightmost value in the command line will be considered for that option in the below example testbin binary will be disassembled assuming sm75 as the architecture nvdisasmexe b sm70 b sm75 testbin nvdisasm warning incompatible redefinition for option binary the last value of this option was used for options taking a list of values if specified multiple times the values get appended to the list if there are duplicate values specified they are ignored in the below example functions foo and bar are considered as valid values for option function and the duplicate value foo is ignored cuobjdump function foo function bar function foo sass testcubin 2 cuobjdump cuobjdump extracts information from cuda binary files both standalone and those embedded in host binaries and presents them in human readable format the output of cuobjdump includes cuda assembly code for each kernel cuda elf section headers string tables relocators and other cuda specific sections it also extracts embedded ptx text from host binaries for a list of cuda assembly instruction set of each gpu architecture see instruction set reference   usage cuobjdump accepts a single input file each time its run the basic usage is as following cuobjdump options file to disassemble a standalone cubin or cubins embedded in a host executable and show cuda assembly of the kernels use the following command cuobjdump sass input file to dump cuda elf sections in human readable format from a cubin file use the following command cuobjdump elf cubin file to extract ptx text from a host binary use the following command cuobjdump ptx host binary heres a sample output of cuobjdump cuobjdump aout sass ptx fatbin elf code arch sm70 code version 17 producer cuda host linux compilesize 64bit identifier addcu code for sm70 function z3addpiss headerflags efcudasm70 efcudaptxsmefcudasm70 0000 imadmovu32 r1 rz rz c0x00x28 0x00000a00ff017624 0x000fd000078e00ff 0010 pt shflidx pt rz rz rz rz 0x000000fffffff389 0x000fe200000e00ff 0020 imadmovu32 r2 rz rz c0x00x160 0x00005800ff027624 0x000fe200078e00ff 0030 mov r3 c0x00x164 0x0000590000037a02 0x000fe20000000f00 0040 imadmovu32 r4 rz rz c0x00x168 0x00005a00ff047624 0x000fe200078e00ff 0050 mov r5 c0x00x16c 0x00005b0000057a02 0x000fcc0000000f00 0060 ldgesys r2 r2 0x0000000002027381 0x000ea800001ee900 0070 ldgesys r5 r4 0x0000000004057381 0x000ea200001ee900 0080 imadmovu32 r6 rz rz c0x00x170 0x00005c00ff067624 0x000fe200078e00ff 0090 mov r7 c0x00x174 0x00005d0000077a02 0x000fe40000000f00 00a0 iadd3 r9 r2 r5 rz 0x0000000502097210 0x004fd00007ffe0ff 00b0 stgesys r6 r9 0x0000000906007386 0x000fe2000010e900 00c0 exit 0x000000000000794d 0x000fea0003800000 00d0 bra 0xd0 0xfffffff000007947 0x000fc0000383ffff 00e0 nop 0x0000000000007918 0x000fc00000000000 00f0 nop 0x0000000000007918 0x000fc00000000000  fatbin ptx code arch sm70 code version 70 producer cuda host linux compilesize 64bit compressed identifier addcu version  target sm70 addresssize 64 visible entry z3addpiss param u64 z3addpissparam0 param,https://docs.nvidia.com/cuda/floating-point/index.html,floating point and ieee 754
floating point and ieee 754 1 introduction v125 pdf archive floating point and ieee 754 compliance for nvidia gpus white paper covering the most common issues related to nvidia gpus a number of issues related to floating point accuracy and compliance are a frequent source of confusion on both cpus and gpus the purpose of this white paper is to discuss the most common issues related to nvidia gpus and to supplement the documentation in the cuda c programming guide 1 introduction since the widespread adoption in 1985 of the ieee standard for binary floatingpoint arithmetic ieee 7541985 1 virtually all mainstream computing systems have implemented the standard including nvidia with the cuda architecture ieee 754 standardizes how arithmetic results should be approximated in floating point whenever working with inexact results programming decisions can affect accuracy it is important to consider many aspects of floating point behavior in order to achieve the highest performance with the precision required for any specific application this is especially true in a heterogeneous computing environment where operations will be performed on different types of hardware understanding some of the intricacies of floating point and the specifics of how nvidia hardware handles floating point is obviously important to cuda programmers striving to implement correct numerical algorithms in addition users of libraries such as cublas and cufft will also find it informative to learn how nvidia handles floating point under the hood we review some of the basic properties of floating point calculations in chapter 2  we also discuss the fused multiplyadd operator which was added to the ieee 754 standard in 2008 2 and is built into the hardware of nvidia gpus in chapter 3 we work through an example of computing the dot product of two short vectors to illustrate how different choices of implementation affect the accuracy of the final result in chapter 4 we describe nvidia hardware versions and nvcc compiler options that affect floating point calculations in chapter 5 we consider some issues regarding the comparison of cpu and gpu results finally in chapter 6 we conclude with concrete recommendations to programmers that deal with numeric issues relating to floating point on the gpu 2 floating point  formats floating point encodings and functionality are defined in the ieee 754 standard 2 last revised in 2008 goldberg 5 gives a good introduction to floating point and many of the issues that arise the standard mandates binary floating point data be encoded on three fields a one bit sign field followed by exponent bits encoding the exponent offset by a numeric bias specific to each format and bits encoding the significand or fraction in order to ensure consistent computations across platforms and to exchange floating point data ieee 754 defines basic and interchange formats the 32 and 64 bit basic binary floating point formats correspond to the c data types float and double  their corresponding representations have the following bit lengths for numerical data representing finite values the sign is either negative or positive the exponent field encodes the exponent in base 2 and the fraction field encodes the significand without the most significant nonzero bit for example the value 192 equals 1 1 x 2 7 x  and can be represented as having a negative sign an exponent of 7 and a fractional part 5 the exponents are biased by 127 and 1023 respectively to allow exponents to extend from negative to positive hence the exponent 7 is represented by bit strings with values 134 for float and 1030 for double the integral part of 1 is implicit in the fraction also encodings to represent infinity and notanumber nan data are reserved the ieee 754 standard 2 describes floating point encodings in full given that the fraction field uses a limited number of bits not all real numbers can be represented exactly for example the mathematical value of the fraction 23 represented in binary is  which has an infinite number of bits after the binary point the value 23 must be rounded first in order to be represented as a floating point number with limited precision the rules for rounding and the rounding modes are specified in ieee 754 the most frequently used is the roundtonearestoreven mode abbreviated as roundtonearest the value 23 rounded in this mode is represented in binary as the sign is positive and the stored exponent value represents an exponent of 1  operations and accuracy the ieee 754 standard requires support for a handful of operations these include the arithmetic operations add subtract multiply divide square root fusedmultiplyadd remainder conversion operations scaling sign operations and comparisons the results of these operations are guaranteed to be the same for all implementations of the standard for a given format and rounding mode the rules and properties of mathematical arithmetic do not hold directly for floating point arithmetic because of floating points limited precision for example the table below shows single precision values a  b  and c  and the mathematical exact value of their sum computed using different associativity beginmatrix a 21 times  b 20 times  c 23 times  a b c 23 times  a b c 23 times  endmatrix mathematically a b c does equal a b c  let rn x denote one rounding step on x  performing these same computations in single precision floating point arithmetic in roundtonearest mode according to ieee 754 we obtain beginmatrix a b 21 times  textrna b 21 times  b c 23 times  textrnb c 23 times  a b c 23 times  textrnleft textrna b c right 23 times  textrnleft a textrnb c right 23 times  endmatrix for reference the exact mathematical results are computed as well in the table above not only are the results computed according to ieee 754 different from the exact mathematical results but also the results corresponding to the sum rnrna b c and the sum,https://docs.nvidia.com/cuda/incomplete-lu-cholesky/index.html,incompletelu and cholesky preconditioned iterative methods
incompletelu and cholesky preconditioned iterative methods 1 introduction v125 pdf archive incompletelu and cholesky preconditioned iterative methods using cusparse and cublas white paper describing how to use the cusparse and cublas libraries to achieve a 2x speedup over cpu in the incompletelu and cholesky preconditioned iterative methods 1 introduction the solution of large sparse linear systems is an important problem in computational mechanics atmospheric modeling geophysics biology circuit simulation and many other applications in the field of computational science and engineering in general these linear systems can be solved using direct or preconditioned iterative methods although the direct methods are often more reliable they usually have large memory requirements and do not scale well on massively parallel computer platforms the iterative methods are more amenable to parallelism and therefore can be used to solve larger problems currently the most popular iterative schemes belong to the krylov subspace family of methods they include biconjugate gradient stabilized bicgstab and conjugate gradient cg iterative methods for nonsymmetric and symmetric positive definite spd linear systems respectively 2  11  we describe these methods in more detail in the next section in practice we often use a variety of preconditioning techniques to improve the convergence of the iterative methods in this white paper we focus on the incompletelu and cholesky preconditioning 11  which is one of the most popular of these preconditioning techniques it computes an incomplete factorization of the coefficient matrix and requires a solution of lower and upper triangular linear systems in every iteration of the iterative method in order to implement the preconditioned bicgstab and cg we use the sparse matrixvector multiplication 3  15 and the sparse triangular solve 8  16 implemented in the cusparse library we point out that the underlying implementation of these algorithms takes advantage of the cuda parallel programming paradigm 5  9  13  which allows us to explore the computational resources of the graphical processing unit gpu in our numerical experiments the incomplete factorization is performed on the cpu host and the resulting lower and upper triangular factors are then transferred to the gpu device memory before starting the iterative method however the computation of the incomplete factorization could also be accelerated on the gpu we point out that the parallelism available in these iterative methods depends highly on the sparsity pattern of the coefficient matrix at hand in our numerical experiments the incompletelu and cholesky preconditioned iterative methods achieve on average more than 2x speedup using the cusparse and cublas libraries on the gpu over the mkl 17 implementation on the cpu for example the speedup for the preconditioned iterative methods with the incompletelu and cholesky factorization with 0 fillin ilu0 is shown in figure 1 for matrices resulting from a variety of applications it will be described in more detail in the last section speedup of the incompletelu cholesky with 0 fillin prec iterative methods in the next sections we briefly describe the methods of interest and comment on the role played in them by the parallel sparse matrixvector multiplication and triangular solve algorithms 2 preconditioned iterative methods let us consider the linear system amathbfx mathbff where a in mathbbrn times n is a nonsingular coefficient matrix and mathbfxmathbff in mathbbrn are the solution and righthandside vectors in general the iterative methods start with an initial guess and perform a series of steps that find more accurate approximations to the solution there are two types of iterative methods i the stationary iterative methods such as the splittingbased jacobi and gaussseidel gs and ii the nonstationary iterative methods such as the krylov subspace family of methods which includes cg and bicgstab  as we mentioned earlier we focus on the latter in this white paper the convergence of the iterative methods depends highly on the spectrum of the coefficient matrix and can be significantly improved using preconditioning the preconditioning modifies the spectrum of the coefficient matrix of the linear system in order to reduce the number of iterative steps required for convergence it often involves finding a preconditioning matrix m  such that m 1 is a good approximation of a 1 and the systems with m are relatively easy to solve for the spd matrix a we can let m be its incompletecholesky factorization so that a approx m widetildertwidetilder  where widetilder is an upper triangular matrix let us assume that m is nonsingular then widetilder tawidetilder 1 is spd and instead of solving the linear system 1  we can solve the preconditioned linear system left widetilder tawidetilder 1 rightleft widetildermathbfx right widetilder tmathbff the pseudocode for the preconditioned cg iterative method is shown in algorithm 1   algorithm 1 conjugate gradient cg 1 textletting initial guess be mathbfx0text compute mathbfrleftarrowmathbff amathbfx0 2 textbffor ileftarrow 12text until convergence textbfdo 3 quadquadtextsolve mmathbfzleftarrowmathbfr vartriangleright textsparse lower and upper triangular solves 4 quadquadrhoileftarrowmathbfrtmathbfz 5 quadquadtextbfif i1textbf then 6 quadquadquadquadmathbfpleftarrowmathbfz 7 quadquadtextbfelse 8 quadquadquadquadbetaleftarrowfracrhoirhoi 1 9 quadquadquadquadmathbfpleftarrowmathbfz betamathbfp 10 quadquadtextbfend if 11 quadquadtextcompute mathbfqleftarrow amathbfp vartriangleright textsparse matrixvector multiplication 12 quadquadalphaleftarrowfracrhoimathbfptmathbfq 13 quadquadmathbfxleftarrowmathbfx alphamathbfp 14 quadquadmathbfrleftarrowmathbfr alphamathbfq 15 textbfend for notice that in every iteration of the incompletecholesky preconditioned cg iterative method we need to perform one sparse matrixvector multiplication and two triangular solves the corresponding cg code using the cusparse and cublas libraries in c programming language is shown below cg code assumptions 1 the cusparse and cublas libraries have been initialized 2 the appropriate memory has been allocated and set to zero 3 the matrix a vala csrrowptra csrcolinda and the incomplete cholesky upper triangular factor r valr csrrowptrr csrcolindr have been computed and are present in the device gpu memory create the info and analyse the lower and upper triangular factors cusparsecreatesolveanalysisinfo inforrt cusparsecreatesolveanalysisinfo inforr cusparsedcsrsvanalysis handle  cusparseoperationtranspose  n  descrr  valr  csrrowptrr  csrcolindr  inforrt cusparsedcsrsvanalysis handle  cusparseoperationnontranspose  n  descrr  valr  csrrowptrr  csrcolindr  inforr 1 compute initial residual r f a x0 using,https://docs.nvidia.com/cuda/cuda-for-tegra-appnote/index.html,cuda for tegra
cuda for tegra 1 cuda for tegra v125 pdf archive 1 cuda for tegra this application note provides an overview of nvidia tegra memory architecture and considerations for porting code from a discrete gpu dgpu attached to an x86 system to the tegra integrated gpu igpu it also discusses egl interoperability 2 overview this document provides an overview of nvidia tegra memory architecture and considerations for porting code from a discrete gpu dgpu attached to an x86 system to the tegra integrated gpu igpu it also discusses egl interoperability this guide is for developers who are already familiar with programming in cuda and cc and who want to develop applications for the tegra soc performance guidelines best practices terminology and general information provided in the cuda c programming guide and the cuda c best practices guide are applicable to all cudacapable gpu architectures including tegra devices the cuda c programming guide and the cuda c best practices guide are available at the following web sites cuda c programming guide httpsdocsnvidiacomcudacudacprogrammingguideindexhtml cuda c best practices guide httpsdocsnvidiacomcudacudacbestpracticesguideindexhtml 3 memory management in tegra devices both the cpu host and the igpu share soc dram memory a dgpu with separate dram memory can be connected to the tegra device over pcie or nvlink it is currently supported only on the nvidia drive platform an overview of a dgpuconnected tegra memory system is shown in figure 1  dgpuconnected tegra memory system in tegra device memory host memory and unified memory are allocated on the same physical soc dram on a dgpu device memory is allocated on the dgpu dram the caching behavior in a tegra system is different from that of an x86 system with a dgpu the caching and accessing behavior of different memory types in a tegra system is shown in table 1  table 1 characteristics of different memory types in a tegra system memory type cpu igpu tegraconnected dgpu device memory not directly accessible cached cached pageable host memory cached not directly accessible not directly accessible pinned host memory uncached where compute capability is less than  cached where compute capability is greater than or equal to  uncached uncached unified memory cached cached not supported on tegra because device memory host memory and unified memory are allocated on the same physical soc dram duplicate memory allocations and data transfers can be avoided  io coherency io coherency also known as oneway coherency is a feature with which an io device such as a gpu can read the latest updates in cpu caches it removes the need to perform cpu cache management operations when the same physical memory is shared between cpu and gpu the gpu cache management operations still need to be performed because the coherency is one way please note that the cuda driver internally performs the gpu cache management operations when managed memory or interop memory is used io coherency is supported on tegra devices starting with xavier soc applications should realize benefits from this hw feature without needing to make changes to the applications code see point 2 below the following functionalities depend on io coherency support cudahostregister cumemhostregister is supported only on platforms which are io coherent the host register support can be queried using the device attribute cudadevattrhostregistersupported cudeviceattributehostregistersupported cpu cache for pinned memory allocated using cudamallochost cumemhostalloc cumemallochost is enabled only on platforms which are io coherent  estimating total allocatable device memory on an integrated gpu device the cudamemgetinfo api returns the snapshot of free and total amount of memory available for allocation for the gpu the free memory could change if any other client allocate memory the discrete gpu has the dedicated dram called vidmem which is separate from cpu memory the snapshot of free memory in discrete gpu is returned by the cudamemgetinfo api the integrated gpu on tegra soc shares the dram with cpu and other the tegra engines the cpu can control the contents of dram and free dram memory by moving the contents of dmar to swap area or vice versa the cudamemgetinfo api currently does not account for swap memory area the cudamemgetinfo api may return a smaller size than the actually allocatable memory since the cpu may be able to free up some dram region by moving pages to the swap area in order to estimate the amount of allocatable device memory cuda application developers should consider following on linux and android platforms device allocatable memory on linux and android depends mainly on the total and free sizes of swap space and main memory the following points can help users to estimate the total amount of device allocatable memory in various situations host allocated memory total used physical memory device allocated memory if host allocated memory free swap space then device allocatable memory total physical memory already allocated device memory if host allocated memory free swap space then device allocatable memory total physical memory host allocated memory free swap space here device allocated memory is memory already allocated on the device it can be obtained from the nvmapmemused field in procmeminfo or from the total field of syskerneldebugnvmapiovmmclients  total used physical memory can be obtained using the free m command the used field in row mem represents this information total physical memory is obtained from the memtotal field in procmeminfo  free swap space can be find by using the free m command the free field in the swap row represents this information if the free command is not available the same information can be obtained from procmeminfo as total used physical memory memtotal memfree free swap space swapfree on qnx platforms qnx does not use swap space hence cudamemgetinfofree will be a fair estimate of allocatable device memory as there is no swap space to move memory pages to swap area 4 porting considerations cuda applications originally developed for dgpus attached to x86 systems may require modifications to perform efficiently on tegra systems this section describes the considerations for porting such applications,https://docs.nvidia.com/cuda/libnvvm-api/index.html,libnvvm api
libnvvm api v40 1 libnvvm api v125 archive libnvvm api libnvvm api v40 reference manual 1 libnvvm api  introduction libnvvm api provides an interface for generating ptx code from both binary and text nvvm ir inputs compatible input can be generated by tools and libraries that produce llvm  ir and bitcode support for reading the text nvvm ir representation is deprecated and may be removed in a later release  thread safety libnvvm api provides a threadsafe interface to libnvvm clients can take advantage of improved compilation speeds by spawning multiple compilation threads concurrently  module this chapter presents the api of the libnvvm library here is a list of all modules error handling general information query compilation 2 error handling enumerations nvvmresult nvvm api call result code functions const char nvvmgeterrorstring nvvmresult result get the message string for the given nvvmresult code  enumerations enum nvvmresult nvvm api call result code values enumerator nvvmsuccess enumerator nvvmerroroutofmemory enumerator nvvmerrorprogramcreationfailure enumerator nvvmerrorirversionmismatch enumerator nvvmerrorinvalidinput enumerator nvvmerrorinvalidprogram enumerator nvvmerrorinvalidir enumerator nvvmerrorinvalidoption enumerator nvvmerrornomoduleinprogram enumerator nvvmerrorcompilation  functions const char nvvmgeterrorstring nvvmresult result get the message string for the given nvvmresult code parameters result in nvvm api result code returns message string for the given nvvmresult code 3 general information query functions nvvmresult nvvmirversion int majorir int minorir int majordbg int minordbg get the nvvm ir version nvvmresult nvvmversion int major int minor get the nvvm version  functions nvvmresult nvvmirversion int majorir  int minorir  int majordbg  int minordbg get the nvvm ir version parameters majorir out nvvm ir major version number minorir out nvvm ir minor version number majordbg out nvvm ir debug metadata major version number minordbg out nvvm ir debug metadata minor version number returns nvvmsuccess nvvmresult nvvmversion int major  int minor get the nvvm version parameters major out nvvm major version number minor out nvvm minor version number returns nvvmsuccess 4 compilation functions nvvmresult nvvmaddmoduletoprogram nvvmprogram prog const char buffer sizet size const char name add a module level nvvm ir to a program nvvmresult nvvmcompileprogram nvvmprogram prog int numoptions const char options compile the nvvm program nvvmresult nvvmcreateprogram nvvmprogram prog create a program and set the value of its handle to prog  nvvmresult nvvmdestroyprogram nvvmprogram prog destroy a program nvvmresult nvvmgetcompiledresult nvvmprogram prog char buffer get the compiled result nvvmresult nvvmgetcompiledresultsize nvvmprogram prog sizet buffersizeret get the size of the compiled result nvvmresult nvvmgetprogramlog nvvmprogram prog char buffer get the compilerverifier message nvvmresult nvvmgetprogramlogsize nvvmprogram prog sizet buffersizeret get the size of compilerverifier message nvvmresult nvvmlazyaddmoduletoprogram nvvmprogram prog const char buffer sizet size const char name add a module level nvvm ir to a program nvvmresult nvvmverifyprogram nvvmprogram prog int numoptions const char options verify the nvvm program typedefs nvvmprogram nvvm program  functions nvvmresult nvvmaddmoduletoprogram nvvmprogram prog  const char buffer  sizet size  const char name add a module level nvvm ir to a program the buffer should contain an nvvm ir module the module should have nvvm ir either in the llvm 1 bitcode representation or in the llvm 1 text representation support for reading the text representation of nvvm ir is deprecated and may be removed in a later version parameters prog in nvvm program buffer in nvvm ir module in the bitcode or text representation size in size of the nvvm ir module name in name of the nvvm ir module if null unnamed is used as the name returns nvvmsuccess nvvmerroroutofmemory nvvmerrorinvalidinput nvvmerrorinvalidprogram nvvmresult nvvmcompileprogram nvvmprogram prog  int numoptions  const char options compile the nvvm program the nvvm ir modules in the program will be linked at the ir level the linked ir program is compiled to ptx the target datalayout in the linked ir program is used to determine the address size 32bit vs 64bit the valid compiler options are g enable generation of full debugging information full debug support is only valid with opt0 debug support requires the input module to utilize nvvm ir debug metadata line number line info only generation is also enabled via nvvm ir debug metadata there is no specific libnvvm api flag for that case opt 0 disable optimizations 3 default enable optimizations arch compute50 compute52 default compute53 compute60 compute61 compute62 compute70 compute72 compute75 compute80 compute87 compute89 compute90 ftz 0 default preserve denormal values when performing singleprecision floatingpoint operations 1 flush denormal values to zero when performing singleprecision floatingpoint operations precsqrt 0 use a faster approximation for singleprecision floatingpoint square root 1 default use ieee roundtonearest mode for singleprecision floatingpoint square root precdiv 0 use a faster approximation for singleprecision floatingpoint division and reciprocals 1 default use ieee roundtonearest mode for singleprecision floatingpoint division and reciprocals fma 0 disable fma contraction 1 default enable fma contraction jumptabledensity0101 specify the case density percentage in switch statements and use it as a minimal threshold to determine whether jump tablebrxidx instruction will be used to implement a switch statement default value is 101 the percentage ranges from 0 to 101 inclusively genlto generate lto ir instead of ptx parameters prog in nvvm program numoptions in number of compiler options passed options in compiler options in the form of c string array returns nvvmsuccess nvvmerroroutofmemory nvvmerrorirversionmismatch nvvmerrorinvalidprogram nvvmerrorinvalidoption nvvmerrornomoduleinprogram nvvmerrorcompilation nvvmresult nvvmcreateprogram nvvmprogram prog create a program and set the value of its handle to prog  see also nvvmdestroyprogram parameters prog in nvvm program returns nvvmsuccess nvvmerroroutofmemory nvvmerrorinvalidprogram nvvmresult nvvmdestroyprogram nvvmprogram prog destroy a program see also nvvmcreateprogram parameters prog in nvvm program returns nvvmsuccess nvvmerrorinvalidprogram nvvmresult nvvmgetcompiledresult nvvmprogram prog  char buffer get the compiled result the result is stored in the memory pointed to by buffer  parameters prog in nvvm program buffer out compiled result returns nvvmsuccess nvvmerrorinvalidprogram nvvmresult nvvmgetcompiledresultsize nvvmprogram prog  sizet buffersizeret get the size of the compiled result parameters prog in nvvm program buffersizeret out size of the compiled result including the trailing null returns nvvmsuccess nvvmerrorinvalidprogram nvvmresult nvvmgetprogramlog nvvmprogram prog  char buffer get the compilerverifier message the null terminated,https://docs.nvidia.com/cuda/libdevice-users-guide/index.html,libdevice users guide
users guide to libdevice,https://docs.nvidia.com/cuda/nvvm-ir-spec/index.html,nvvm ir
nvvm ir specification 1 introduction v125 pdf archive nvvm ir specification reference guide to the nvvm compiler intermediate representation based on the llvm ir 1 introduction nvvm ir is a compiler ir intermediate representation based on the llvm ir the nvvm ir is designed to represent gpu compute kernels for example cuda kernels highlevel language frontends like the cuda c compiler frontend can generate nvvm ir the nvvm compiler which is based on llvm generates ptx code from nvvm ir nvvm ir and nvvm compilers are mostly agnostic about the source language being used the ptx codegen part of a nvvm compiler needs to know the source language because of the difference in dci drivercompiler interface nvvm ir is a binary format and is based on a subset of llvm ir bitcode format this document uses only humanreadable form to describe nvvm ir technically speaking nvvm ir is llvm ir with a set of rules restrictions and conventions plus a set of supported intrinsic functions a program specified in nvvm ir is always a legal llvm program a legal llvm program may not be a legal nvvm program there are three levels of support for nvvm ir supported the feature is fully supported most ir features should fall into this category accepted and ignored the nvvm compiler will accept this ir feature but will ignore the required semantics this applies to some ir features that do not have meaningful semantics on gpus and that can be ignored calling convention markings are an example illegal not supported the specified semantics is not supported such as a fence instruction future versions of nvvm may either support or accept and ignore irs that are illegal in the current version this document describes version  of the nvvm ir and version  of the nvvm debug metadata see source level debugging support  the  version of nvvm ir is incompatible with the previous version  linking of nvvm ir version  with  will result in compiler error the current nvvm ir is based on llvm 1 for the complete semantics of the ir readers of this document should check the official llvm language reference manual httpsreleasesllvmorg701docslangrefhtml  2 identifiers the name of a named global identifier must have the form azazazaz09 note that it cannot contain the  character llvmnvvm and nvvm are reserved words 3 high level structure  linkage types supported private internal availableexternally linkonce weak common linkonceodr weakodr external not supported appending externweak see nvvm abi for ptx for details on how linkage types are translated to ptx  calling conventions all llvm calling convention markings are accepted and ignored functions and calls are generated according to the ptx calling convention 1 rules and restrictions when an argument with width less than 32bit is passed the zeroextsignext parameter attribute should be set zeroext will be assumed if not set when a value with width less than 32bit is returned the zeroextsignext parameter attribute should be set zeroext will be assumed if not set arguments of aggregate or vector types that are passed by value can be passed by pointer with the byval attribute set referred to as the bypointerbyval case below the align attribute must be set if the type requires a nonnatural alignment natural alignment is the alignment inferred for the aggregate type according to the data layout section if a function has an argument of aggregate or vector type that is passed by value directly and the type has a nonnatural alignment requirement the alignment must be annotated by the global property annotation align  alignment where alignment is a 32bit integer whose upper 16 bits represent the argument position starting from 1 and the lower 16 bits represent the alignment if the return type of a function is an aggregate or a vector that has a nonnatural alignment then the alignment requirement must be annotated by the global property annotation align  alignment where the upper 16 bits is 0 and the lower 16 bits represent the alignment it is not required to annotate a function with align  alignment otherwise if annotated the alignment must match the natural alignment or the align attribute in the bypointerbyval case for an indirect call instruction of a function that has a nonnatural alignment for its return value or one of its arguments that is not expressed in alignment in the bypointerbyval case the call instruction must have an attached metadata of kind callalign  the metadata contains a sequence of i32 fields each of which represents a nonnatural alignment requirement the upper 16 bits of an i32 field represent the argument position 0 for return value 1 for the first argument and so on and the lower 16 bits represent the alignment the i32 fields must be sorted in the increasing order for example call call struct  s fp1 struct  s byval align 8 arg1p  struct  s arg2   callalign  10  10  i32 8  i32 520 it is not required to have an i32 metadata field for the other arguments or the return value otherwise if presented the alignment must match the natural alignment or the align attribute in the bypointerbyval case  it is not required to have a callalign metadata attached to a direct call instruction if attached the alignment must match the natural alignment or the alignment in the bypointerbyval case the absence of the metadata in an indirect call instruction means using natural alignment or the align attribute in the bypointerbyval case  visibility styles all stylesdefault hidden and protectedare accepted and ignored  dll storage classes not supported  thread local storage models not supported  runtime preemption specifiers not supported  structure types fully supported  nonintegral pointer type not supported  comdats not supported  sourcefilename accepted and ignored  global variables a global variable that is not an intrinsic global variable may be optionally declared to reside in one of the,https://docs.nvidia.com/cuda/nvvm-ir-spec/index.html,nvvm ir
