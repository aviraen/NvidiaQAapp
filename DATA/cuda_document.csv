                                                                                                                                                                                       section,title,url,content 
CUDA Toolkit,Release Notes,https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html,"Release Notes 1. CUDA 12.5 Update 1 Release Notes v12.5 PDF Archive NVIDIA CUDA Toolkit Release Notes The Release Notes for the CUDA Toolkit. 1. CUDA 12.5 Update 1 Release Notes The release notes for the NVIDIA CUDA Toolkit can be found online at httpsdocs.nvidia.comcudacudatoolkitreleasenotesindex.html . Note The release notes have been reorganized into two major sections the general CUDA release notes, and the CUDA libraries release notes including historical information for 12.x releases. 1.1. CUDA Toolkit Major Component Versions CUDA Components Starting with CUDA 11, the various components in the toolkit are versioned independently. For CUDA 12.5 Update 1, the table below indicates the versions Table 1 CUDA 12.5 Update 1 Component Versions Component Name Version Information Supported Architectures Supported Platforms CUDA C Core Compute Libraries Thrust 2.4.0 x8664, arm64sbsa, aarch64jetson Linux, Windows CUB 2.4.0 libcu 2.4.0 Cooperative Groups 12.5.82 CUDA Compatibility 12.5.36505571 aarch64jetson Linux CUDA Runtime cudart 12.5.82 x8664, arm64sbsa, aarch64jetson Linux, Windows, WSL cuobjdump 12.5.39 x8664, arm64sbsa, aarch64jetson Linux, Windows CUPTI 12.5.82 x8664, arm64sbsa, aarch64jetson Linux, Windows, WSL CUDA cuxxfilt demangler 12.5.82 x8664, arm64sbsa, aarch64jetson Linux, Windows CUDA Demo Suite 12.5.82 x8664 Linux, Windows CUDA GDB 12.5.82 x8664, arm64sbsa, aarch64jetson Linux, WSL CUDA Nsight Eclipse Plugin 12.5.82 x8664 Linux CUDA NVCC 12.5.82 x8664, arm64sbsa, aarch64jetson Linux, Windows, WSL CUDA nvdisasm 12.5.39 x8664, arm64sbsa, aarch64jetson Linux, Windows CUDA NVML Headers 12.5.82 x8664, arm64sbsa, aarch64jetson Linux, Windows, WSL CUDA nvprof 12.5.82 x8664 Linux, Windows CUDA nvprune 12.5.82 x8664, arm64sbsa, aarch64jetson Linux, Windows, WSL CUDA NVRTC 12.5.82 x8664, arm64sbsa, aarch64jetson Linux, Windows, WSL NVTX 12.5.82 x8664, arm64sbsa, aarch64jetson Linux, Windows, WSL CUDA NVVP 12.5.82 x8664, Linux, Windows CUDA OpenCL 12.5.39 x8664 Linux, Windows CUDA Profiler API 12.5.39 x8664, arm64sbsa, aarch64jetson Linux, Windows, WSL CUDA Compute Sanitizer API 12.5.81 x8664, arm64sbsa, aarch64jetson Linux, Windows, WSL CUDA cuBLAS 12.5.3.2 x8664, arm64sbsa, aarch64jetson Linux, Windows, WSL cuDLA 12.5.82 aarch64jetson Linux CUDA cuFFT 11.2.3.61 x8664, arm64sbsa, aarch64jetson Linux, Windows, WSL CUDA cuFile 1.10.1.7 x8664, arm64sbsa, aarch64jetson Linux CUDA cuRAND 10.3.6.82 x8664, arm64sbsa, aarch64jetson Linux, Windows, WSL CUDA cuSOLVER 11.6.3.83 x8664, arm64sbsa, aarch64jetson Linux, Windows, WSL CUDA cuSPARSE 12.5.1.3 x8664, arm64sbsa, aarch64jetson Linux, Windows, WSL CUDA NPP 12.3.0.159 x8664, arm64sbsa, aarch64jetson Linux, Windows, WSL CUDA nvFatbin 12.5.82 x8664, arm64sbsa, aarch64jetson Linux, Windows, WSL CUDA nvJitLink 12.5.82 x8664, arm64sbsa, aarch64jetson Linux, Windows, WSL CUDA nvJPEG 12.3.2.81 x8664, arm64sbsa, aarch64jetson Linux, Windows, WSL Nsight Compute 2024.2.1.2 x8664, arm64sbsa, aarch64jetson Linux, Windows, WSL Windows 11 Nsight Systems 2024.2.3.38 x8664, arm64sbsa, Linux, Windows, WSL Nsight Visual Studio Edition VSE 2024.2.1.24155 x8664 Windows Windows nvidiafs 1 2.20.6 x8664, arm64sbsa, aarch64jetson Linux Visual Studio Integration 12.5.82 x8664 Windows Windows NVIDIA Linux Driver 555.42.06 x8664, arm64sbsa Linux NVIDIA Windows Driver 555.85 x8664 Windows Windows, WSL CUDA Driver Running a CUDA application requires the system with at least one CUDA capable GPU and a driver that is compatible with the CUDA Toolkit. See Table 3 . For more information various GPU products that are CUDA capable, visit httpsdeveloper.nvidia.comcudagpus . Each release of the CUDA Toolkit requires a minimum version of the CUDA driver. The CUDA driver is backward compatible, meaning that applications compiled against a particular version of the CUDA will continue to work on subsequent later driver releases. More information on compatibility can be found at httpsdocs.nvidia.comcudacudacbestpracticesguideindex.htmlcudacompatibilityandupgrades . Note Starting with CUDA 11.0, the toolkit components are individually versioned, and the toolkit itself is versioned as shown in the table below. The minimum required driver version for CUDA minor version compatibility is shown below. CUDA minor version compatibility is described in detail in httpsdocs.nvidia.comdeploycudacompatibilityindex.html Table 2 CUDA Toolkit and Minimum Required Driver Version for CUDA Minor Version Compatibility CUDA Toolkit Minimum Required Driver Version for CUDA Minor Version Compatibility Linux x8664 Driver Version Windows x8664 Driver Version CUDA 12.x 525.60.13 528.33 CUDA 11.8.x CUDA 11.7.x CUDA 11.6.x CUDA 11.5.x CUDA 11.4.x CUDA 11.3.x CUDA 11.2.x CUDA 11.1.x 450.80.02 452.39 CUDA 11.0 11.0.3 450.36.06 451.22 Using a Minimum Required Version that is different from Toolkit Driver Version could be allowed in compatibility mode please read the CUDA Compatibility Guide for details. CUDA 11.0 was released with an earlier driver version, but by upgrading to Tesla Recommended Drivers 450.80.02 Linux 452.39 Windows, minor version compatibility is possible across the CUDA 11.x family of toolkits. The version of the development NVIDIA GPU Driver packaged in each CUDA Toolkit release is shown below. Table 3 CUDA Toolkit and Corresponding Driver Versions CUDA Toolkit Toolkit Driver Version Linux x8664 Driver Version Windows x8664 Driver Version CUDA 12.5 Update 1 555.42.06 555.85 CUDA 12.5 GA 555.42.02 555.85 CUDA 12.4 Update 1 550.54.15 551.78 CUDA 12.4 GA 550.54.14 551.61 CUDA 12.3 Update 1 545.23.08 546.12 CUDA 12.3 GA 545.23.06 545.84 CUDA 12.2 Update 2 535.104.05 537.13 CUDA 12.2 Update 1 535.86.09 536.67 CUDA 12.2 GA 535.54.03 536.25 CUDA 12.1 Update 1 530.30.02 531.14 CUDA 12.1 GA 530.30.02 531.14 CUDA 12.0 Update 1 525.85.12 528.33 CUDA 12.0 GA 525.60.13 527.41 CUDA 11.8 GA 520.61.05 520.06 CUDA 11.7 Update 1 515.48.07 516.31 CUDA 11.7 GA 515.43.04 516.01 CUDA 11.6 Update 2 510.47.03 511.65 CUDA 11.6 Update 1 510.47.03 511.65 CUDA 11.6 GA 510.39.01 511.23 CUDA 11.5 Update 2 495.29.05 496.13 CUDA 11.5 Update 1 495.29.05 496.13 CUDA 11.5 GA 495.29.05 496.04 CUDA 11.4 Update 4 470.82.01 472.50 CUDA 11.4 Update 3 470.82.01 472.50 CUDA 11.4 Update 2 470.57.02 471.41 CUDA 11.4 Update 1 470.57.02 471.41 CUDA 11.4.0 GA 470.42.01 471.11 CUDA 11.3.1 Update 1 465.19.01 465.89 CUDA 11.3.0 GA 465.19.01 465.89 CUDA 11.2.2 Update 2 460.32.03 461.33 CUDA 11.2.1 Update 1 460.32.03 461.09 CUDA 11.2.0 GA 460.27.03 460.82 CUDA 11.1.1 Update 1 455.32 456.81 CUDA 11.1 GA 455.23 456.38 CUDA 11.0.3 Update 1 450.51.06 451.82 CUDA 11.0.2 GA 450.51.05 451.48 CUDA 11.0.1 RC 450.36.06 451.22 CUDA 10.2.89 440.33 441.22 CUDA 10.1 10.1.105 general release, and updates 418.39 418.96 CUDA 10.0.130 410.48 411.31 CUDA 9.2 9.2.148 Update 1 396.37 398.26 CUDA 9.2 9.2.88 396.26 397.44 CUDA 9.1 9.1.85 390.46 391.29 CUDA 9.0 9.0.76 384.81 385.54 CUDA 8.0 8.0.61 GA2 375.26 376.51 CUDA 8.0 8.0.44 367.48 369.30 CUDA 7.5 7.5.16 352.31 353.66 CUDA"
Miscellaneous,CUDA Features Archive,https://docs.nvidia.com/cuda/cuda-features-archive/index.html,"CUDA Features Archive 1. CUDA 11.6 Features v12.5 PDF Archive NVIDIA CUDA Features Archive The list of CUDA features by release. 1. CUDA 11.6 Features 1.1. Compiler 1.1.1. VS2022 Support CUDA 11.6 officially supports the latest VS2022 as host compiler. A separate Nsight Visual Studio installer 2022.1.1 must be downloaded from here . A future CUDA release will have the Nsight Visual Studio installer with VS2022 support integrated into it. 1.1.2. New instructions in public PTX New instructions for bit mask creationBMSK, and sign extensionSZEXT, are added to the public PTX ISA. You can find documentation for these instructions in the PTX ISA guide BMSK and SZEXT . 1.1.3. Unused Kernel Optimization In CUDA 11.5, unused kernel pruning was introduced with the potential benefits of reducing binary size and improving performance through more efficient optimizations. This was an optin feature but in 11.6, this feature is enabled by default. As mentioned in the 11.5 blog, there is an optout flag that can be used in case it becomes necessary for debug purposes or for other special situations. nvcc rdctrue user.cu testlib.a o user Xnvlink ignorehostinfo 1.1.4. New archnative option In addition to the archall and archallmajor options added in CUDA 11.5, NVCC introduced arch native in CUDA 11.5 update 1. This archnative option is a convenient way for users to let NVCC determine the right target architecture to compile the CUDA device code to based on the GPU installed on the system. This can be particularly helpful for testing when applications are run on the same system they are compiled in. 1.1.5. Generate PTX from nvlink Using the following command line, device linker, nvlink will produce PTX as an output in addition to CUBIN nvcc dlto dlink ptx Device linking by nvlink is the final stage in the CUDA compilation process. Applications that have multiple source translation units have to be compiled in separate compilation mode. LTO introduced in CUDA 11.4 allowed nvlink to perform optimizations at device link time instead of at compile time so that separately compiled applications with several translation units can be optimized to the same level as whole program compilations with a single translation unit. However, without the option to output PTX, applications that cared about forward compatibility of device code could not benefit from Link Time Optimization or had to constrain the device code to a single source file. With the option for nvlink that performs LTO to generate the output in PTX, customer applications that require forward compatibility across GPU architectures can span across multiple files and can also take advantage of Link Time Optimization. 1.1.6. Bullseye support NVCC compiled source code now works with the code coverage tool Bullseye. The code coverage is only for the CPU or the host functions. Code coverage for device function is not supported through bullseye. 1.1.7. INT128 developer tool support In 11.5, CUDA C support for 128 bit was added. In 11.6, developer tools support the datatype as well. With the latest version of libcu, int 128 data datype is supported by math functions. 2. Notices 2.1. Notice This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation NVIDIA makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material defined below, code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer Terms of Sale. NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion andor use of NVIDIA products in such equipment or applications and therefore such inclusion andor use is at customers own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customers sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customers product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions andor requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to i the use of the NVIDIA product in any manner that is contrary to this document or ii customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding thirdparty products or"
Miscellaneous,EULA,https://docs.nvidia.com/cuda/eula/index.html,"EULA 1. License Agreement for NVIDIA Software Development Kits v12.5 PDF Archive End User License Agreement NVIDIA Software License Agreement and CUDA Supplement to Software License Agreement. Last updated October 8, 2021 The CUDA Toolkit End User License Agreement applies to the NVIDIA CUDA Toolkit, the NVIDIA CUDA Samples, the NVIDIA Display Driver, NVIDIA Nsight tools Visual Studio Edition, and the associated documentation on CUDA APIs, programming model and development tools. If you do not agree with the terms and conditions of the license agreement, then do not download or use the software. Last updated October 8, 2021. Preface The Software License Agreement in Chapter 1 and the Supplement in Chapter 2 contain license terms and conditions that govern the use of NVIDIA CUDA toolkit. By accepting this agreement, you agree to comply with all the terms and conditions applicable to the products included herein. NVIDIA Driver Description This package contains the operating system driver and fundamental system software components for NVIDIA GPUs. NVIDIA CUDA Toolkit Description The NVIDIA CUDA Toolkit provides commandline and graphical tools for building, debugging and optimizing the performance of applications accelerated by NVIDIA GPUs, runtime and math libraries, and documentation including programming guides, user manuals, and API references. Default Install Location of CUDA Toolkit Windows platform ProgramFilesNVIDIA GPU Computing ToolkitCUDAv. Linux platform usrlocalcuda. Mac platform DeveloperNVIDIACUDA. NVIDIA CUDA Samples Description CUDA Samples are now located in httpsgithub.comnvidiacudasamples , which includes instructions for obtaining, building, and running the samples. They are no longer included in the CUDA toolkit. NVIDIA Nsight Visual Studio Edition Windows only Description NVIDIA Nsight Development Platform, Visual Studio Edition is a development environment integrated into Microsoft Visual Studio that provides tools for debugging, profiling, analyzing and optimizing your GPU computing and graphics applications. Default Install Location of Nsight Visual Studio Edition Windows platform ProgramFilesx86NVIDIA CorporationNsight Visual Studio Edition . 1. License Agreement for NVIDIA Software Development Kits Important NoticeRead before downloading, installing, copying or using the licensed software This license agreement, including exhibits attached Agreement is a legal agreement between you and NVIDIA Corporation NVIDIA and governs your use of a NVIDIA software development kit SDK. Each SDK has its own set of software and materials, but here is a description of the types of items that may be included in a SDK source code, header files, APIs, data sets and assets examples include images, textures, models, scenes, videos, native API inputoutput files, binary software, sample code, libraries, utility programs, programming code and documentation. This Agreement can be accepted only by an adult of legal age of majority in the country in which the SDK is used. If you are entering into this Agreement on behalf of a company or other legal entity, you represent that you have the legal authority to bind the entity to this Agreement, in which case you will mean the entity you represent. If you dont have the required age or authority to accept this Agreement, or if you dont accept all the terms and conditions of this Agreement, do not download, install or use the SDK. You agree to use the SDK only for purposes that are permitted by a this Agreement, and b any applicable law, regulation or generally accepted practices or guidelines in the relevant jurisdictions. 1.1. License 1.1.1. License Grant Subject to the terms of this Agreement, NVIDIA hereby grants you a nonexclusive, nontransferable license, without the right to sublicense except as expressly provided in this Agreement to Install and use the SDK, Modify and create derivative works of sample source code delivered in the SDK, and Distribute those portions of the SDK that are identified in this Agreement as distributable, as incorporated in object code format into a software application that meets the distribution requirements indicated in this Agreement. 1.1.2. Distribution Requirements These are the distribution requirements for you to exercise the distribution grant Your application must have material additional functionality, beyond the included portions of the SDK. The distributable portions of the SDK shall only be accessed by your application. The following notice shall be included in modifications and derivative works of sample source code distributed This software contains source code provided by NVIDIA Corporation. Unless a developer tool is identified in this Agreement as distributable, it is delivered for your internal use only. The terms under which you distribute your application must be consistent with the terms of this Agreement, including without limitation terms relating to the license grant and license restrictions and protection of NVIDIAs intellectual property rights. Additionally, you agree that you will protect the privacy, security and legal rights of your application users. You agree to notify NVIDIA in writing of any known or suspected distribution or use of the SDK not in compliance with the requirements of this Agreement, and to enforce the terms of your agreements with respect to distributed SDK. 1.1.3. Authorized Users You may allow employees and contractors of your entity or of your subsidiaryies to access and use the SDK from your secure network to perform work on your behalf. If you are an academic institution you may allow users enrolled or employed by the academic institution to access and use the SDK from your secure network. You are responsible for the compliance with the terms of this Agreement by your authorized users. If you become aware that your authorized users didnt follow the terms of this Agreement, you agree to take reasonable steps to resolve the noncompliance and prevent new occurrences. 1.1.4. PreRelease SDK The SDK versions identified as alpha, beta, preview or otherwise as prerelease, may not be fully functional, may contain errors or design flaws, and may have reduced or different security, privacy, accessibility, availability, and reliability standards relative to commercial versions of NVIDIA software and materials. Use of a prerelease SDK may result in unexpected results, loss of data, project delays or other unpredictable damage or loss. You may use a prerelease SDK at your own risk, understanding that prerelease SDKs are not intended for use in production"
Miscellaneous,Quick Start Guide,https://docs.nvidia.com/cuda/cuda-quick-start-guide/index.html,"Quick Start Guide 1. Introduction v12.5 PDF Archive CUDA Quick Start Guide Minimal firststeps instructions to get CUDA running on a standard system. 1. Introduction This guide covers the basic instructions needed to install CUDA and verify that a CUDA application can run on each supported platform. These instructions are intended to be used on a clean installation of a supported platform. For questions which are not answered in this document, please refer to the Windows Installation Guide and Linux Installation Guide . The CUDA installation packages can be found on the CUDA Downloads Page . 2. Windows When installing CUDA on Windows, you can choose between the Network Installer and the Local Installer. The Network Installer allows you to download only the files you need. The Local Installer is a standalone installer with a large initial download. For more details, refer to the Windows Installation Guide . 2.1. Network Installer Perform the following steps to install CUDA and verify the installation. Launch the downloaded installer package. Read and accept the EULA. Select next to download and install all components. Once the download completes, the installation will begin automatically. Once the installation completes, click next to acknowledge the Nsight Visual Studio Edition installation summary. Click close to close the installer. Navigate to the Samples nbody directory in httpsgithub.comNVIDIAcudasamplestreemasterSamples5DomainSpecificnbody . Open the nbody Visual Studio solution file for the version of Visual Studio you have installed, for example, nbodyvs2019.sln . Open the Build menu within Visual Studio and click Build Solution . Navigate to the CUDA Samples build directory and run the nbody sample. Note Run samples by navigating to the executables location, otherwise it will fail to locate dependent resources. 2.2. Local Installer Perform the following steps to install CUDA and verify the installation. Launch the downloaded installer package. Read and accept the EULA. Select next to install all components. Once the installation completes, click next to acknowledge the Nsight Visual Studio Edition installation summary. Click close to close the installer. Navigate to the Samples nbody directory in httpsgithub.comNVIDIAcudasamplestreemasterSamples5DomainSpecificnbody . Open the nbody Visual Studio solution file for the version of Visual Studio you have installed. Open the Build menu within Visual Studio and click Build Solution . Navigate to the CUDA Samples build directory and run the nbody sample. Note Run samples by navigating to the executables location, otherwise it will fail to locate dependent resources. 2.3. Pip Wheels Windows NVIDIA provides Python Wheels for installing CUDA through pip, primarily for using CUDA with Python. These packages are intended for runtime use and do not currently include developer tools these can be installed separately. Please note that with this installation method, CUDA installation environment is managed via pip and additional care must be taken to set up your host environment to use CUDA outside the pip environment. Prerequisites To install Wheels, you must first install the nvidiapyindex package, which is required in order to set up your pip installation to fetch additional Python modules from the NVIDIA NGC PyPI repo. If your pip and setuptools Python modules are not uptodate, then use the following command to upgrade these Python modules. If these Python modules are outofdate then the commands which follow later in this section may fail. py m pip install upgrade setuptools pip wheel You should now be able to install the nvidiapyindex module. py m pip install nvidiapyindex If your project is using a requirements.txt file, then you can add the following line to your requirements.txt file as an alternative to installing the nvidiapyindex package extraindexurl httpspypi.ngc.nvidia.com Procedure Install the CUDA runtime package py m pip install nvidiacudaruntimecu12 Optionally, install additional packages as listed below using the following command py m pip install nvidialibrary Metapackages The following metapackages will install the latest version of the named component on Windows for the indicated CUDA version. cu12 should be read as cuda12. nvidiacudaruntimecu12 nvidiacudacupticu12 nvidiacudanvcccu12 nvidianvmldevcu12 nvidiacudanvrtccu12 nvidianvtxcu12 nvidiacudasanitizerapicu12 nvidiacublascu12 nvidiacufftcu12 nvidiacurandcu12 nvidiacusolvercu12 nvidiacusparsecu12 nvidianppcu12 nvidianvjpegcu12 These metapackages install the following packages nvidianvmldevcu125 nvidiacudanvcccu125 nvidiacudaruntimecu125 nvidiacudacupticu125 nvidiacublascu125 nvidiacudasanitizerapicu125 nvidianvtxcu125 nvidiacudanvrtccu125 nvidianppcu125 nvidiacusparsecu125 nvidiacusolvercu125 nvidiacurandcu125 nvidiacufftcu125 nvidianvjpegcu125 2.4. Conda The Conda packages are available at httpsanaconda.orgnvidia . Installation To perform a basic install of all CUDA Toolkit components using Conda, run the following command conda install cuda c nvidia Uninstallation To uninstall the CUDA Toolkit using Conda, run the following command conda remove cuda 3. Linux CUDA on Linux can be installed using an RPM, Debian, Runfile, or Conda package, depending on the platform being installed on. 3.1. Linux x8664 For development on the x8664 architecture. In some cases, x8664 systems may act as host platforms targeting other architectures. See the Linux Installation Guide for more details. 3.1.1. Redhat CentOS When installing CUDA on Redhat or CentOS, you can choose between the Runfile Installer and the RPM Installer. The Runfile Installer is only available as a Local Installer. The RPM Installer is available as both a Local Installer and a Network Installer. The Network Installer allows you to download only the files you need. The Local Installer is a standalone installer with a large initial download. In the case of the RPM installers, the instructions for the Local and Network variants are the same. For more details, refer to the Linux Installation Guide . 3.1.1.1. RPM Installer Perform the following steps to install CUDA and verify the installation. Install EPEL to satisfy the DKMS dependency by following the instructions at EPELs website . Enable optional repos On RHEL 8 Linux only, execute the following steps to enable optional repositories. On x8664 workstation subscriptionmanager repos enablerhel8forx8664appstreamrpms subscriptionmanager repos enablerhel8forx8664baseosrpms subscriptionmanager repos enablecodereadybuilderforrhel8x8664rpms Install the repository metadata, clean the yum cache, and install CUDA sudo rpm install cudarepodistroversion.architecture.rpm sudo rpm erase gpgpubkey7fa2af80 sudo yum clean expirecache sudo yum install cuda Reboot the system to load the NVIDIA drivers sudo reboot Set up the development environment by modifying the PATH and LDLIBRARYPATH variables export PATHusrlocalcuda12.5binPATHPATH export LDLIBRARYPATHusrlocalcuda12.5lib64 LDLIBRARYPATHLDLIBRARYPATH Install a writable copy of the samples from httpsgithub.comnvidiacudasamples , then build and run the"
Installation Guide,Installation Guide Windows,https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html,"Installation Guide Windows 1. Introduction v12.5 PDF Archive CUDA Installation Guide for Microsoft Windows The installation instructions for the CUDA Toolkit on Microsoft Windows systems. 1. Introduction CUDA is a parallel computing platform and programming model invented by NVIDIA. It enables dramatic increases in computing performance by harnessing the power of the graphics processing unit GPU. CUDA was developed with several design goals in mind Provide a small set of extensions to standard programming languages, like C, that enable a straightforward implementation of parallel algorithms. With CUDA CC, programmers can focus on the task of parallelization of the algorithms rather than spending time on their implementation. Support heterogeneous computation where applications use both the CPU and GPU. Serial portions of applications are run on the CPU, and parallel portions are offloaded to the GPU. As such, CUDA can be incrementally applied to existing applications. The CPU and GPU are treated as separate devices that have their own memory spaces. This configuration also allows simultaneous computation on the CPU and GPU without contention for memory resources. CUDAcapable GPUs have hundreds of cores that can collectively run thousands of computing threads. These cores have shared resources including a register file and a shared memory. The onchip shared memory allows parallel tasks running on these cores to share data without sending it over the system memory bus. This guide will show you how to install and check the correct operation of the CUDA development tools. 1.1. System Requirements To use CUDA on your system, you will need the following installed A CUDAcapable GPU A supported version of Linux with a gcc compiler and toolchain NVIDIA CUDA Toolkit available at httpsdeveloper.nvidia.comcudadownloads Supported Microsoft Windows operating systems Microsoft Windows 11 21H2 Microsoft Windows 11 22H2SV2 Microsoft Windows 11 23H2 Microsoft Windows 10 21H2 Microsoft Windows 10 22H2 Microsoft Windows Server 2022 Table 1 Windows Compiler Support in CUDA 12.5 Compiler IDE Native x8664 Crosscompilation 32bit on 64bit C Dialect MSVC Version 193x Visual Studio 2022 17.x YES Not supported C14 default, C17, C20 MSVC Version 192x Visual Studio 2019 16.x YES C14 default, C17 MSVC Version 191x Visual Studio 2017 15.x RTW and all updates YES C14 default, C17 Support for Visual Studio 2015 is deprecated in release 11.1 support for Visual Studio 2017 is deprecated in release 12.5. 32bit compilation native and crosscompilation is removed from CUDA 12.0 and later Toolkit. Use the CUDA Toolkit from earlier releases for 32bit compilation. CUDA Driver will continue to support running 32bit application binaries on GeForce GPUs until Ada. Ada will be the last architecture with driver support for 32bit applications. Hopper does not support 32bit applications. Support for running x86 32bit applications on x8664 Windows is limited to use with CUDA Driver CUDA Runtime cudart CUDA Math Library math.h 1.2. About This Document This document is intended for readers familiar with Microsoft Windows operating systems and the Microsoft Visual Studio environment. You do not need previous experience with CUDA or experience with parallel computation. 2. Installing CUDA Development Tools Basic instructions can be found in the Quick Start Guide . Read on for more detailed instructions. The setup of CUDA development tools on a system running the appropriate version of Windows consists of a few simple steps Verify the system has a CUDAcapable GPU. Download the NVIDIA CUDA Toolkit. Install the NVIDIA CUDA Toolkit. Test that the installed software runs correctly and communicates with the hardware. 2.1. Verify You Have a CUDACapable GPU You can verify that you have a CUDAcapable GPU through the Display Adapters section in the Windows Device Manager . Here you will find the vendor name and model of your graphics cards. If you have an NVIDIA card that is listed in httpsdeveloper.nvidia.comcudagpus , that GPU is CUDAcapable. The Release Notes for the CUDA Toolkit also contain a list of supported products. The Windows Device Manager can be opened via the following steps Open a run window from the Start Menu Run control name Microsoft.DeviceManager 2.2. Download the NVIDIA CUDA Toolkit The NVIDIA CUDA Toolkit is available at httpsdeveloper.nvidia.comcudadownloads . Choose the platform you are using and one of the following installer formats Network Installer A minimal installer which later downloads packages required for installation. Only the packages selected during the selection phase of the installer are downloaded. This installer is useful for users who want to minimize download time. Full Installer An installer which contains all the components of the CUDA Toolkit and does not require any further download. This installer is useful for systems which lack network access and for enterprise deployment. The CUDA Toolkit installs the CUDA driver and tools needed to create, build and run a CUDA application as well as libraries, header files, and other resources. Download Verification The download can be verified by comparing the MD5 checksum posted at httpsdeveloper.download.nvidia.comcomputecuda12.5.1docssidebarmd5sum.txt with that of the downloaded file. If either of the checksums differ, the downloaded file is corrupt and needs to be downloaded again. 2.3. Install the CUDA Software Before installing the toolkit, you should read the Release Notes, as they provide details on installation and software functionality. Note The driver and toolkit must be installed for CUDA to function. If you have not installed a standalone driver, install the driver from the NVIDIA CUDA Toolkit. Note The installation may fail if Windows Update starts after the installation has begun. Wait until Windows Update is complete and then try the installation again. Graphical Installation Install the CUDA Software by executing the CUDA installer and following the onscreen prompts. Silent Installation The installer can be executed in silent mode by executing the package with the s flag. Additional parameters can be passed which will install specific subpackages instead of all packages. See the table below for a list of all the subpackage names. Table 2 Possible Subpackage Names Subpackage Name Subpackage Description Toolkit Subpackages defaults to CProgram FilesNVIDIA GPU Computing ToolkitCUDAv12.5 cudaprofilerapi12.5 CUDA Profiler API. cudart12.5 CUDA Runtime libraries. cuobjdump12.5 Extracts information from cubin files. cupti12.5 The CUDA"
Installation Guide,Installation Guide Linux,https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html,"Installation Guide for Linux 1. Introduction v12.5 PDF Archive NVIDIA CUDA Installation Guide for Linux The installation instructions for the CUDA Toolkit on Linux. 1. Introduction CUDA is a parallel computing platform and programming model invented by NVIDIA . It enables dramatic increases in computing performance by harnessing the power of the graphics processing unit GPU. CUDA was developed with several design goals in mind Provide a small set of extensions to standard programming languages, like C, that enable a straightforward implementation of parallel algorithms. With CUDA CC, programmers can focus on the task of parallelization of the algorithms rather than spending time on their implementation. Support heterogeneous computation where applications use both the CPU and GPU. Serial portions of applications are run on the CPU, and parallel portions are offloaded to the GPU. As such, CUDA can be incrementally applied to existing applications. The CPU and GPU are treated as separate devices that have their own memory spaces. This configuration also allows simultaneous computation on the CPU and GPU without contention for memory resources. CUDAcapable GPUs have hundreds of cores that can collectively run thousands of computing threads. These cores have shared resources including a register file and a shared memory. The onchip shared memory allows parallel tasks running on these cores to share data without sending it over the system memory bus. This guide will show you how to install and check the correct operation of the CUDA development tools. 1.1. System Requirements To use NVIDIA CUDA on your system, you will need the following installed CUDAcapable GPU A supported version of Linux with a gcc compiler and toolchain CUDA Toolkit available at httpsdeveloper.nvidia.comcudadownloads The CUDA development environment relies on tight integration with the host development environment, including the host compiler and C runtime libraries, and is therefore only supported on distribution versions that have been qualified for this CUDA Toolkit release. The following table lists the supported Linux distributions. Please review the footnotes associated with the table. Table 1 Native Linux Distribution Support in CUDA 12.5 Update 1 Distribution Kernel 1 Default GCC GLIBC x8664 RHEL 9.y y 4 5.14.0427 11.4.1 2.34 RHEL 8.y y 10 4.18.0553 8.5.0 2.28 OpenSUSE Leap 15.y y 5 5.14.21150500 7.5.0 2.31 Rocky Linux 8.y y10 4.18.0553 8.5.0 2.28 Rocky Linux 9.y y4 5.14.0427 11.4.1 2.34 SUSE SLES 15.y y 5 5.14.21150500 7.5.0 2.31 Ubuntu 24.04 LTS 6.8.031 13.2.0 2.39 Ubuntu 22.04.z z 4 LTS 6.5.027 12.3.0 2.35 Ubuntu 20.04.z z 6 LTS 5.15.067 9.4.0 2.31 Debian 12.x x5 6.1.761 12.2.0 2.36 Debian 11.y y9 5.10.2092 10.2.1 2.31 Debian 10.z z13 4.19.021 8.3.0 2.28 Fedora 39 6.5.6300 13.2.1 2.38 KylinOS V10 SP2 4.19.9025.14.v2101.ky10 7.3.0 2.28 Amazon Linux 2023 6.1.8299.168 11.4.1 2.34 Arm64 sbsa RHEL 9.y y 4 5.14.0427 11.4.1 2.34 RHEL 8.y y 10 4.18.0553 8.5.0 2.28 SUSE SLES 15.y y 5 5.14.21150500 7.5.0 2.32 Ubuntu 24.04 LTS 6.8.031 13.2.0 2.39 Ubuntu 22.04 LTS z 5 LTS 5.15.0102 11.4.0 2.35 Ubuntu 20.04.z z 5 LTS 5.4.0174 9.4.0 2.31 Arm64 sbsa Jetson dGPU 20.04.06 LTS Rel35 JP 5.x 5.10.192tegra 9.4.0 2.31 22.04.4 LTS Rel36 JP6.x 5.15.136tegra 11.4.0 2.35 Aarch64 Jetson iGPU L4T Ubuntu 22.04 Rel36 JP6.x 6.1.80tegra 11.4.0 2.3.5 The following notes apply to the kernel versions supported by CUDA For specific kernel versions supported on Red Hat Enterprise Linux RHEL, visit httpsaccess.redhat.comarticles3078 . A list of kernel versions including the release dates for SUSE Linux Enterprise Server SLES is available at httpswww.suse.comsupportkbdoc?id000019587 . L4T provides a Linux kernel and a sample root filesystem derived from Ubuntu 20.04. For more details, visit httpsdeveloper.nvidia.comembeddedjetsonlinux . 1.2. OS Support Policy CUDA support for Ubuntu 20.04.x, Ubuntu 22.04.x, RHEL 8.x, RHEL 9.x, Rocky Linux 8.x, Rocky Linux 9.x, SUSE SLES 15.x and OpenSUSE Leap 15.x will be until the standard EOSS as defined for each OS. Please refer to the support lifecycle for these OSes to know their support timelines. CUDA supports the latest Fedora release version. For Fedora release timelines, visit httpsdocs.fedoraproject.orgenUSreleases . CUDA supports a single KylinOS release version. For details, visit httpswww.kylinos.cn . Refer to the support lifecycle for these supported OSes to know their support timelines and plan to move to newer releases accordingly. 1.3. Host Compiler Support Policy In order to compile the CPU Host code in the CUDA source, the CUDA compiler NVCC requires a compatible host compiler to be installed on the system. The version of the host compiler supported on Linux platforms is tabulated as below. NVCC performs a version check on the host compilers major version and so newer minor versions of the compilers listed below will be supported, but major versions falling outside the range will not be supported. Table 2 Supported Compilers Distribution GCC Clang NVHPC XLC ArmCC ICC x8664 6.x 13.2 7.x 17.0 24.3 No No 2021.7 Arm64 sbsa 6.x 13.2 7.x 17.0 24.3 No 23.04.1 No For GCC and Clang, the preceding table indicates the minimum version and the latest version supported. If you are on a Linux distribution that may use an older version of GCC toolchain as default than what is listed above, it is recommended to upgrade to a newer toolchain CUDA 11.0 or later toolkit. Newer GCC toolchains are available with the Red Hat Developer Toolset for example. For platforms that ship a compiler version older than GCC 6 by default, linking to static or dynamic libraries that are shipped with the CUDA Toolkit is not supported. We only support libstdc GCCs implementation for all the supported host compilers for the platforms listed above. 1.3.1. Supported C Dialects NVCC and NVRTC CUDA Runtime Compiler support the following C dialect C11, C14, C17, C20 on supported host compilers. The default C dialect of NVCC is determined by the default dialect of the host compiler used for compilation. Refer to host compiler documentation and the CUDA Programming Guide for more details on language support. C20 is supported with the following flavors of host compiler in both host and device code. GCC Clang NVHPC Arm CC 10.x 11.x 22.x 22.x 1.4. About"
Programming Guide,Programming Guide,https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html,"CUDA C Programming Guide 1. Introduction v12.5 PDF Archive CUDA C Programming Guide The programming guide to the CUDA model and interface. Changes from Version 12.4 Added section Asynchronous Data Copies using Tensor Memory Access TMA . Added Unified Memory Programming guide supporting Grace Hopper with Address Translation Service ATS and Heterogeneous Memory Management HMM on x86. 1. Introduction 1.1. The Benefits of Using GPUs The Graphics Processing Unit GPU 1 provides much higher instruction throughput and memory bandwidth than the CPU within a similar price and power envelope. Many applications leverage these higher capabilities to run faster on the GPU than on the CPU see GPU Applications . Other computing devices, like FPGAs, are also very energy efficient, but offer much less programming flexibility than GPUs. This difference in capabilities between the GPU and the CPU exists because they are designed with different goals in mind. While the CPU is designed to excel at executing a sequence of operations, called a thread , as fast as possible and can execute a few tens of these threads in parallel, the GPU is designed to excel at executing thousands of them in parallel amortizing the slower singlethread performance to achieve greater throughput. The GPU is specialized for highly parallel computations and therefore designed such that more transistors are devoted to data processing rather than data caching and flow control. The schematic Figure 1 shows an example distribution of chip resources for a CPU versus a GPU. Figure 1 The GPU Devotes More Transistors to Data Processing Devoting more transistors to data processing, for example, floatingpoint computations, is beneficial for highly parallel computations the GPU can hide memory access latencies with computation, instead of relying on large data caches and complex flow control to avoid long memory access latencies, both of which are expensive in terms of transistors. In general, an application has a mix of parallel parts and sequential parts, so systems are designed with a mix of GPUs and CPUs in order to maximize overall performance. Applications with a high degree of parallelism can exploit this massively parallel nature of the GPU to achieve higher performance than on the CPU. 1.2. CUDA A GeneralPurpose Parallel Computing Platform and Programming Model In November 2006, NVIDIA introduced CUDA , a general purpose parallel computing platform and programming model that leverages the parallel compute engine in NVIDIA GPUs to solve many complex computational problems in a more efficient way than on a CPU. CUDA comes with a software environment that allows developers to use C as a highlevel programming language. As illustrated by Figure 2 , other languages, application programming interfaces, or directivesbased approaches are supported, such as FORTRAN, DirectCompute, OpenACC. Figure 2 GPU Computing Applications. CUDA is designed to support various languages and application programming interfaces. 1.3. A Scalable Programming Model The advent of multicore CPUs and manycore GPUs means that mainstream processor chips are now parallel systems. The challenge is to develop application software that transparently scales its parallelism to leverage the increasing number of processor cores, much as 3D graphics applications transparently scale their parallelism to manycore GPUs with widely varying numbers of cores. The CUDA parallel programming model is designed to overcome this challenge while maintaining a low learning curve for programmers familiar with standard programming languages such as C. At its core are three key abstractions a hierarchy of thread groups, shared memories, and barrier synchronization that are simply exposed to the programmer as a minimal set of language extensions. These abstractions provide finegrained data parallelism and thread parallelism, nested within coarsegrained data parallelism and task parallelism. They guide the programmer to partition the problem into coarse subproblems that can be solved independently in parallel by blocks of threads, and each subproblem into finer pieces that can be solved cooperatively in parallel by all threads within the block. This decomposition preserves language expressivity by allowing threads to cooperate when solving each subproblem, and at the same time enables automatic scalability. Indeed, each block of threads can be scheduled on any of the available multiprocessors within a GPU, in any order, concurrently or sequentially, so that a compiled CUDA program can execute on any number of multiprocessors as illustrated by Figure 3 , and only the runtime system needs to know the physical multiprocessor count. This scalable programming model allows the GPU architecture to span a wide market range by simply scaling the number of multiprocessors and memory partitions from the highperformance enthusiast GeForce GPUs and professional Quadro and Tesla computing products to a variety of inexpensive, mainstream GeForce GPUs see CUDAEnabled GPUs for a list of all CUDAenabled GPUs. Figure 3 Automatic Scalability Note A GPU is built around an array of Streaming Multiprocessors SMs see Hardware Implementation for more details. A multithreaded program is partitioned into blocks of threads that execute independently from each other, so that a GPU with more multiprocessors will automatically execute the program in less time than a GPU with fewer multiprocessors. 1.4. Document Structure This document is organized into the following sections Introduction is a general introduction to CUDA. Programming Model outlines the CUDA programming model. Programming Interface describes the programming interface. Hardware Implementation describes the hardware implementation. Performance Guidelines gives some guidance on how to achieve maximum performance. CUDAEnabled GPUs lists all CUDAenabled devices. C Language Extensions is a detailed description of all extensions to the C language. Cooperative Groups describes synchronization primitives for various groups of CUDA threads. CUDA Dynamic Parallelism describes how to launch and synchronize one kernel from another. Virtual Memory Management describes how to manage the unified virtual address space. Stream Ordered Memory Allocator describes how applications can order memory allocation and deallocation. Graph Memory Nodes describes how graphs can create and own memory allocations. Mathematical Functions lists the mathematical functions supported in CUDA. C Language Support lists the C features supported in device code. Texture Fetching gives more details on texture fetching. Compute Capabilities gives the technical specifications of various devices, as well as more architectural"
Miscellaneous,Best Practices Guide,https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html,"CUDA C Best Practices Guide 1. Preface v12.5 PDF Archive CUDA C Best Practices Guide The programming guide to using the CUDA Toolkit to obtain the best performance from NVIDIA GPUs. 1. Preface 1.1. What Is This Document? This Best Practices Guide is a manual to help developers obtain the best performance from NVIDIA CUDA GPUs. It presents established parallelization and optimization techniques and explains coding metaphors and idioms that can greatly simplify programming for CUDAcapable GPU architectures. While the contents can be used as a reference manual, you should be aware that some topics are revisited in different contexts as various programming and configuration topics are explored. As a result, it is recommended that firsttime readers proceed through the guide sequentially. This approach will greatly improve your understanding of effective programming practices and enable you to better use the guide for reference later. 1.2. Who Should Read This Guide? The discussions in this guide all use the C programming language, so you should be comfortable reading C code. This guide refers to and relies on several other documents that you should have at your disposal for reference, all of which are available at no cost from the CUDA website httpsdocs.nvidia.comcuda . The following documents are especially important resources CUDA Installation Guide CUDA C Programming Guide CUDA Toolkit Reference Manual In particular, the optimization section of this guide assumes that you have already successfully downloaded and installed the CUDA Toolkit if not, please refer to the relevant CUDA Installation Guide for your platform and that you have a basic familiarity with the CUDA C programming language and environment if not, please refer to the CUDA C Programming Guide. 1.3. Assess, Parallelize, Optimize, Deploy This guide introduces the Assess, Parallelize, Optimize, DeployAPOD design cycle for applications with the goal of helping application developers to rapidly identify the portions of their code that would most readily benefit from GPU acceleration, rapidly realize that benefit, and begin leveraging the resulting speedups in production as early as possible. APOD is a cyclical process initial speedups can be achieved, tested, and deployed with only minimal initial investment of time, at which point the cycle can begin again by identifying further optimization opportunities, seeing additional speedups, and then deploying the even faster versions of the application into production. 1.3.1. Assess For an existing project, the first step is to assess the application to locate the parts of the code that are responsible for the bulk of the execution time. Armed with this knowledge, the developer can evaluate these bottlenecks for parallelization and start to investigate GPU acceleration. By understanding the endusers requirements and constraints and by applying Amdahls and Gustafsons laws, the developer can determine the upper bound of performance improvement from acceleration of the identified portions of the application. 1.3.2. Parallelize Having identified the hotspots and having done the basic exercises to set goals and expectations, the developer needs to parallelize the code. Depending on the original code, this can be as simple as calling into an existing GPUoptimized library such as cuBLAS , cuFFT , or Thrust , or it could be as simple as adding a few preprocessor directives as hints to a parallelizing compiler. On the other hand, some applications designs will require some amount of refactoring to expose their inherent parallelism. As even CPU architectures will require exposing parallelism in order to improve or simply maintain the performance of sequential applications, the CUDA family of parallel programming languages CUDA C, CUDA Fortran, etc. aims to make the expression of this parallelism as simple as possible, while simultaneously enabling operation on CUDAcapable GPUs designed for maximum parallel throughput. 1.3.3. Optimize After each round of application parallelization is complete, the developer can move to optimizing the implementation to improve performance. Since there are many possible optimizations that can be considered, having a good understanding of the needs of the application can help to make the process as smooth as possible. However, as with APOD as a whole, program optimization is an iterative process identify an opportunity for optimization, apply and test the optimization, verify the speedup achieved, and repeat, meaning that it is not necessary for a programmer to spend large amounts of time memorizing the bulk of all possible optimization strategies prior to seeing good speedups. Instead, strategies can be applied incrementally as they are learned. Optimizations can be applied at various levels, from overlapping data transfers with computation all the way down to finetuning floatingpoint operation sequences. The available profiling tools are invaluable for guiding this process, as they can help suggest a nextbest course of action for the developers optimization efforts and provide references into the relevant portions of the optimization section of this guide. 1.3.4. Deploy Having completed the GPU acceleration of one or more components of the application it is possible to compare the outcome with the original expectation. Recall that the initial assess step allowed the developer to determine an upper bound for the potential speedup attainable by accelerating given hotspots. Before tackling other hotspots to improve the total speedup, the developer should consider taking the partially parallelized implementation and carry it through to production. This is important for a number of reasons for example, it allows the user to profit from their investment as early as possible the speedup may be partial but is still valuable, and it minimizes risk for the developer and the user by providing an evolutionary rather than revolutionary set of changes to the application. 1.4. Recommendations and Best Practices Throughout this guide, specific recommendations are made regarding the design and implementation of CUDA C code. These recommendations are categorized by priority, which is a blend of the effect of the recommendation and its scope. Actions that present substantial improvements for most CUDA applications have the highest priority, while small optimizations that affect only very specific situations are given a lower priority. Before implementing lower priority recommendations, it is good practice to make sure all higher priority recommendations that are relevant have"
Miscellaneous,Maxwell Compatibility Guide,https://docs.nvidia.com/cuda/maxwell-compatibility-guide/index.html,"Maxwell Compatibility Guide 1. Maxwell Compatibility v12.5 PDF Archive Maxwell Compatibility Guide for CUDA Applications The guide to building CUDA applications for GPUs based on the NVIDIA Maxwell Architecture. 1. Maxwell Compatibility 1.1. About this Document This application note, Maxwell Compatibility Guide for CUDA Applications, is intended to help developers ensure that their NVIDIA CUDA applications will run on GPUs based on the NVIDIA Maxwell Architecture. This document provides guidance to developers who are already familiar with programming in CUDA C and want to make sure that their software applications are compatible with Maxwell. 1.2. Application Compatibility on Maxwell The NVIDIA CUDA C compiler, nvcc , can be used to generate both architecturespecific cubin files and forwardcompatible PTX versions of each kernel. Each cubin file targets a specific computecapability version and is forwardcompatible only with GPU architectures of the same major version number . For example, cubin files that target compute capability 3.0 are supported on all computecapability 3.x Kepler devices but are not supported on computecapability 5.x Maxwell devices. For this reason, to ensure forward compatibility with GPU architectures introduced after the application has been released, it is recommended that all applications include PTX versions of their kernels. Note CUDA Runtime applications containing both cubin and PTX code for a given architecture will automatically use the cubin by default, keeping the PTX path strictly for forwardcompatibility purposes. Applications that already include PTX versions of their kernels should work asis on Maxwellbased GPUs. Applications that only support specific GPU architectures via cubin files, however, will need to be updated to provide Maxwellcompatible PTX or cubins. 1.3. Verifying Maxwell Compatibility for Existing Applications The first step is to check that Maxwellcompatible device code at least PTX is compiled in to the application. The following sections show how to accomplish this for applications built with different CUDA Toolkit versions. 1.3.1. Applications Using CUDA Toolkit 5.5 or Earlier CUDA applications built using CUDA Toolkit versions 2.1 through 5.5 are compatible with Maxwell as long as they are built to include PTX versions of their kernels. To test that PTX JIT is working for your application, you can do the following Download and install the latest driver from httpswww.nvidia.comdrivers . Set the environment variable CUDAFORCEPTXJIT1 . Launch your application. When starting a CUDA application for the first time with the above environment flag, the CUDA driver will JITcompile the PTX for each CUDA kernel that is used into native cubin code. If you set the environment variable above and then launch your program and it works properly, then you have successfully verified Maxwell compatibility. Note Be sure to unset the CUDAFORCEPTXJIT environment variable when you are done testing. 1.3.2. Applications Using CUDA Toolkit 6.0 or Later CUDA applications built using CUDA Toolkit 6.0 or Later 1 are compatible with Maxwell as long as they are built to include kernels in either Maxwellnative cubin format see Building Applications with Maxwell Support or PTX format see Applications Using CUDA Toolkit 5.5 or Earlier or both. 1.4. Building Applications with Maxwell Support When a CUDA application launches a kernel, the CUDA Runtime determines the compute capability of each GPU in the system and uses this information to automatically find the best matching cubin or PTX version of the kernel that is available. If a cubin file supporting the architecture of the target GPU is available, it is used otherwise, the CUDA Runtime will load the PTX and JITcompile that PTX to the GPUs native cubin format before launching it. If neither is available, then the kernel launch will fail. The method used to build your application with either native cubin or at least PTX support for Maxwell depend on the version of the CUDA Toolkit used. The main advantages of providing native cubins are as follows It saves the end user the time it takes to JITcompile kernels that are available only as PTX. All kernels compiled into the application must have native binaries at load time or else they will be built justintime from PTX, including kernels from all libraries linked to the application, even if those kernels are never launched by the application. Especially when using large libraries, this JIT compilation can take a significant amount of time. The CUDA driver will cache the cubins generated as a result of the PTX JIT, so this is mostly a onetime cost for a given user, but it is time best avoided whenever possible. PTX JITcompiled kernels often cannot take advantage of architectural features of newer GPUs, meaning that nativecompiled code may be faster or of greater accuracy. 1.4.1. Applications Using CUDA Toolkit 5.5 or Earlier The compilers included in CUDA Toolkit 5.5 or earlier generate cubin files native to earlier NVIDIA architectures such as Fermi and Kepler, but they cannot generate cubin files native to the Maxwell architecture. To allow support for Maxwell and future architectures when using version 5.5 or earlier of the CUDA Toolkit, the compiler must generate a PTX version of each kernel. Below are compiler settings that could be used to build mykernel.cu to run on Fermi or Kepler devices natively and on Maxwell devices via PTX JIT. Note computeXX refers to a PTX version and smXX refers to a cubin version. The arch clause of the gencode commandline option to nvcc specifies the frontend compilation target and must always be a PTX version. The code clause specifies the backend compilation target and can either be cubin or PTX or both. Only the backend target versions specified by the code clause will be retained in the resulting binary at least one must be PTX to provide Maxwell compatibility. Windows nvcc.exe ccbin Cvs2010VCbin Xcompiler EHsc W3 nologo O2 Zi MT gencodearchcompute20,codesm20 gencodearchcompute30,codesm30 gencodearchcompute35,codesm35 gencodearchcompute35,codecompute35 compile o Releasemykernel.cu.obj mykernel.cu MacLinux usrlocalcudabinnvcc gencodearchcompute20,codesm20 gencodearchcompute30,codesm30 gencodearchcompute35,codesm35 gencodearchcompute35,codecompute35 O2 o mykernel.o c mykernel.cu Alternatively, you may be familiar with the simplified nvcc commandline option archsmXX , which is a shorthand equivalent to the following more explicit gencode commandline options used above. archsmXX expands to the following gencodearchcomputeXX,codesmXX"
Miscellaneous,Pascal Compatibility Guide,https://docs.nvidia.com/cuda/pascal-compatibility-guide/index.html,"Pascal Compatibility Guide 1. Pascal Compatibility v12.5 PDF Archive Pascal Compatibility Guide for CUDA Applications The guide to building CUDA applications for GPUs based on the NVIDIA Pascal Architecture. 1. Pascal Compatibility 1.1. About this Document This application note, Pascal Compatibility Guide for CUDA Applications, is intended to help developers ensure that their NVIDIA CUDA applications will run on GPUs based on the NVIDIA Pascal Architecture. This document provides guidance to developers who are already familiar with programming in CUDA C and want to make sure that their software applications are compatible with Pascal. 1.2. Application Compatibility on Pascal The NVIDIA CUDA C compiler, nvcc , can be used to generate both architecturespecific cubin files and forwardcompatible PTX versions of each kernel. Each cubin file targets a specific computecapability version and is forwardcompatible only with GPU architectures of the same major version number . For example, cubin files that target compute capability 3.0 are supported on all computecapability 3.x Kepler devices but are not supported on computecapability 5.x Maxwell or 6.x Pascal devices. For this reason, to ensure forward compatibility with GPU architectures introduced after the application has been released, it is recommended that all applications include PTX versions of their kernels. Note CUDA Runtime applications containing both cubin and PTX code for a given architecture will automatically use the cubin by default, keeping the PTX path strictly for forwardcompatibility purposes. Applications that already include PTX versions of their kernels should work asis on Pascalbased GPUs. Applications that only support specific GPU architectures via cubin files, however, will need to be updated to provide Pascalcompatible PTX or cubins. 1.3. Verifying Pascal Compatibility for Existing Applications The first step is to check that Pascalcompatible device code at least PTX is compiled in to the application. The following sections show how to accomplish this for applications built with different CUDA Toolkit versions. 1.3.1. Applications Using CUDA Toolkit 7.5 or Earlier CUDA applications built using CUDA Toolkit versions 2.1 through 7.5 are compatible with Pascal as long as they are built to include PTX versions of their kernels. To test that PTX JIT is working for your application, you can do the following Download and install the latest driver from httpswww.nvidia.comdrivers . Set the environment variable CUDAFORCEPTXJIT1 . Launch your application. When starting a CUDA application for the first time with the above environment flag, the CUDA driver will JITcompile the PTX for each CUDA kernel that is used into native cubin code. If you set the environment variable above and then launch your program and it works properly, then you have successfully verified Pascal compatibility. Note Be sure to unset the CUDAFORCEPTXJIT environment variable when you are done testing. 1.3.2. Applications Using CUDA Toolkit 8.0 CUDA applications built using CUDA Toolkit 8.0 are compatible with Pascal as long as they are built to include kernels in either Pascalnative cubin format see Building Applications with Pascal Support or PTX format see Applications Using CUDA Toolkit 7.5 or Earlier or both. 1.4. Building Applications with Pascal Support When a CUDA application launches a kernel, the CUDA Runtime determines the compute capability of each GPU in the system and uses this information to automatically find the best matching cubin or PTX version of the kernel that is available. If a cubin file supporting the architecture of the target GPU is available, it is used otherwise, the CUDA Runtime will load the PTX and JITcompile that PTX to the GPUs native cubin format before launching it. If neither is available, then the kernel launch will fail. The method used to build your application with either native cubin or at least PTX support for Pascal depend on the version of the CUDA Toolkit used. The main advantages of providing native cubins are as follows It saves the end user the time it takes to JITcompile kernels that are available only as PTX. All kernels compiled into the application must have native binaries at load time or else they will be built justintime from PTX, including kernels from all libraries linked to the application, even if those kernels are never launched by the application. Especially when using large libraries, this JIT compilation can take a significant amount of time. The CUDA driver will cache the cubins generated as a result of the PTX JIT, so this is mostly a onetime cost for a given user, but it is time best avoided whenever possible. PTX JITcompiled kernels often cannot take advantage of architectural features of newer GPUs, meaning that nativecompiled code may be faster or of greater accuracy. 1.4.1. Applications Using CUDA Toolkit 7.5 or Earlier The compilers included in CUDA Toolkit 7.5 or earlier generate cubin files native to earlier NVIDIA architectures such as Kepler and Maxwell, but they cannot generate cubin files native to the Pascal architecture. To allow support for Pascal and future architectures when using version 7.5 or earlier of the CUDA Toolkit, the compiler must generate a PTX version of each kernel. Below are compiler settings that could be used to build mykernel.cu to run on Kepler or Maxwell devices natively and on Pascal devices via PTX JIT. Note computeXX refers to a PTX version and smXX refers to a cubin version. The arch clause of the gencode commandline option to nvcc specifies the frontend compilation target and must always be a PTX version. The code clause specifies the backend compilation target and can either be cubin or PTX or both. Only the backend target versions specified by the code clause will be retained in the resulting binary at least one must be PTX to provide Pascal compatibility. Windows nvcc.exe ccbin Cvs2010VCbin Xcompiler EHsc W3 nologo O2 Zi MT gencodearchcompute30,codesm30 gencodearchcompute35,codesm35 gencodearchcompute50,codesm50 gencodearchcompute52,codesm52 gencodearchcompute52,codecompute52 compile o Releasemykernel.cu.obj mykernel.cu MacLinux usrlocalcudabinnvcc gencodearchcompute30,codesm30 gencodearchcompute35,codesm35 gencodearchcompute50,codesm50 gencodearchcompute52,codesm52 gencodearchcompute52,codecompute52 O2 o mykernel.o c mykernel.cu Alternatively, you may be familiar with the simplified nvcc commandline option archsmXX , which is a shorthand equivalent to the following more explicit gencode commandline options used above. archsmXX expands to the following gencodearchcomputeXX,codesmXX"
Miscellaneous,Volta Compatibility Guide,https://docs.nvidia.com/cuda/volta-compatibility-guide/index.html,"Volta Compatibility Guide 1. Volta Compatibility v12.5 PDF Archive Volta Compatibility Guide for CUDA Applications The guide to building CUDA applications for GPUs based on the NVIDIA Volta Architecture. 1. Volta Compatibility 1.1. About this Document This application note, Volta Compatibility Guide for CUDA Applications, is intended to help developers ensure that their NVIDIA CUDA applications will run on GPUs based on the NVIDIA Volta Architecture. This document provides guidance to developers who are already familiar with programming in CUDA C and want to make sure that their software applications are compatible with Volta. 1.2. Application Compatibility on Volta The NVIDIA CUDA C compiler, nvcc , can be used to generate both architecturespecific cubin files and forwardcompatible PTX versions of each kernel. Each cubin file targets a specific computecapability version and is forwardcompatible only with GPU architectures of the same major version number . For example, cubin files that target compute capability 3.0 are supported on all computecapability 3.x Kepler devices but are not supported on computecapability 5.x Maxwell or 6.x Pascal devices. For this reason, to ensure forward compatibility with GPU architectures introduced after the application has been released, it is recommended that all applications include PTX versions of their kernels. Note CUDA Runtime applications containing both cubin and PTX code for a given architecture will automatically use the cubin by default, keeping the PTX path strictly for forwardcompatibility purposes. Applications that already include PTX versions of their kernels should work asis on Voltabased GPUs. Applications that only support specific GPU architectures via cubin files, however, will need to be updated to provide Voltacompatible PTX or cubins. 1.3. Verifying Volta Compatibility for Existing Applications The first step is to check that Voltacompatible device code at least PTX is compiled into the application. The following sections show how to accomplish this for applications built with different CUDA Toolkit versions. 1.3.1. Applications Using CUDA Toolkit 8.0 or Earlier CUDA applications built using CUDA Toolkit versions 2.1 through 8.0 are compatible with Volta as long as they are built to include PTX versions of their kernels. To test that PTX JIT is working for your application, you can do the following Download and install the latest driver from httpwww.nvidia.comdrivers . Set the environment variable CUDAFORCEPTXJIT1 . Launch your application. When starting a CUDA application for the first time with the above environment flag, the CUDA driver will JITcompile the PTX for each CUDA kernel that is used into native cubin code. If you set the environment variable above and then launch your program and it works properly, then you have successfully verified Volta compatibility. Note Be sure to unset the CUDAFORCEPTXJIT environment variable when you are done testing. 1.3.2. Applications Using CUDA Toolkit 9.0 CUDA applications built using CUDA Toolkit 9.0 are compatible with Volta as long as they are built to include kernels in either Voltanative cubin format see Building Applications with Volta Support or PTX format see Applications Using CUDA Toolkit 8.0 or Earlier or both. 1.4. Building Applications with Volta Support When a CUDA application launches a kernel, the CUDA Runtime determines the compute capability of each GPU in the system and uses this information to automatically find the best matching cubin or PTX version of the kernel that is available. If a cubin file supporting the architecture of the target GPU is available, it is used otherwise, the CUDA Runtime will load the PTX and JITcompile that PTX to the GPUs native cubin format before launching it. If neither is available, then the kernel launch will fail. The method used to build your application with either native cubin or at least PTX support for Volta depend on the version of the CUDA Toolkit used. The main advantages of providing native cubins are as follows It saves the end user the time it takes to JITcompile kernels that are available only as PTX. All kernels compiled into the application must have native binaries at load time or else they will be built justintime from PTX, including kernels from all libraries linked to the application, even if those kernels are never launched by the application. Especially when using large libraries, this JIT compilation can take a significant amount of time. The CUDA driver will cache the cubins generated as a result of the PTX JIT, so this is mostly a onetime cost for a given user, but it is time best avoided whenever possible. PTX JITcompiled kernels often cannot take advantage of architectural features of newer GPUs, meaning that nativecompiled code may be faster or of greater accuracy. 1.4.1. Applications Using CUDA Toolkit 8.0 or Earlier The compilers included in CUDA Toolkit 8.0 or earlier generate cubin files native to earlier NVIDIA architectures such as Maxwell and Pascal, but they cannot generate cubin files native to the Volta architecture. To allow support for Volta and future architectures when using version 8.0 or earlier of the CUDA Toolkit, the compiler must generate a PTX version of each kernel. Below are compiler settings that could be used to build mykernel.cu to run on Maxwell or Pascal devices natively and on Volta devices via PTX JIT. Note computeXX refers to a PTX version and smXX refers to a cubin version. The arch clause of the gencode commandline option to nvcc specifies the frontend compilation target and must always be a PTX version. The code clause specifies the backend compilation target and can either be cubin or PTX or both. Only the backend target versions specified by the code clause will be retained in the resulting binary at least one must be PTX to provide Volta compatibility. Windows nvcc.exe ccbin Cvs2010VCbin Xcompiler EHsc W3 nologo O2 Zi MT gencodearchcompute50,codesm50 gencodearchcompute52,codesm52 gencodearchcompute60,codesm60 gencodearchcompute61,codesm61 gencodearchcompute61,codecompute61 compile o Releasemykernel.cu.obj mykernel.cu MacLinux usrlocalcudabinnvcc gencodearchcompute50,codesm50 gencodearchcompute52,codesm52 gencodearchcompute60,codesm60 gencodearchcompute61,codesm61 gencodearchcompute61,codecompute61 O2 o mykernel.o c mykernel.cu Alternatively, you may be familiar with the simplified nvcc commandline option archsmXX , which is a shorthand equivalent to the following more explicit gencode commandline options used above. archsmXX expands to the following gencodearchcomputeXX,codesmXX gencodearchcomputeXX,codecomputeXX"
Miscellaneous,Turing Compatibility Guide,https://docs.nvidia.com/cuda/turing-compatibility-guide/index.html,"Turing Compatibility Guide 1. Turing Compatibility v12.5 PDF Archive Turing Compatibility Guide for CUDA Applications The guide to building CUDA applications for NVIDIA Turing GPUs. 1. Turing Compatibility 1.1. About this Document This application note, Turing Compatibility Guide for CUDA Applications, is intended to help developers ensure that their NVIDIA CUDA applications will run on GPUs based on the NVIDIA Turing Architecture. This document provides guidance to developers who are already familiar with programming in CUDA C and want to make sure that their software applications are compatible with Turing. 1.2. Application Compatibility on Turing The NVIDIA CUDA C compiler, nvcc , can be used to generate both architecturespecific cubin files and forwardcompatible PTX versions of each kernel. Each cubin file targets a specific computecapability version and is forwardcompatible only with GPU architectures of the same major version number . For example, cubin files that target compute capability 3.0 are supported on all computecapability 3.x Kepler devices but are not supported on computecapability 5.x Maxwell or 6.x Pascal devices. For this reason, to ensure forward compatibility with GPU architectures introduced after the application has been released, it is recommended that all applications include PTX versions of their kernels. Note CUDA Runtime applications containing both cubin and PTX code for a given architecture will automatically use the cubin by default, keeping the PTX path strictly for forwardcompatibility purposes. Applications that already include PTX versions of their kernels should work asis on Turingbased GPUs. Applications that only support specific GPU architectures via cubin files, however, will need to be updated to provide Turingcompatible PTX or cubins. 1.3. Compatibility between Volta and Turing The Turing architecture is based on Voltas Instruction Set Architecture ISA 7.0, extending it with new instructions. As a consequence, any binary that runs on Volta will be able to run on Turing forward compatibility, but a Turing binary will not be able to run on Volta. Please note that Volta kernels using more than 64KB of shared memory via the explicit optin, see CUDA C Programming Guide will not be able to launch on Turing, as they would exceed Turings shared memory capacity. Most applications compiled for Volta should run efficiently on Turing, except if the application uses heavily the Tensor Cores, or if recompiling would allow use of new Turingspecific instructions. Voltas Tensor Core instructions can only reach half of the peak performance on Turing. Recompiling explicitly for Turing is thus recommended. 1.4. Verifying Turing Compatibility for Existing Applications The first step is to check that Turingcompatible device code at least PTX is compiled into the application. The following sections show how to accomplish this for applications built with different CUDA Toolkit versions. 1.4.1. Applications Using CUDA Toolkit 8.0 or Earlier CUDA applications built using CUDA Toolkit versions 2.1 through 8.0 are compatible with Turing as long as they are built to include PTX versions of their kernels. To test that PTX JIT is working for your application, you can do the following Download and install the latest driver from httpswww.nvidia.comdrivers . Set the environment variable CUDAFORCEPTXJIT1 . Launch your application. When starting a CUDA application for the first time with the above environment flag, the CUDA driver will JITcompile the PTX for each CUDA kernel that is used into native cubin code. If you set the environment variable above and then launch your program and it works properly, then you have successfully verified Turing compatibility. Note Be sure to unset the CUDAFORCEPTXJIT environment variable when you are done testing. 1.4.2. Applications Using CUDA Toolkit 9.x CUDA applications built using CUDA Toolkit 9.x are compatible with Turing as long as they are built to include kernels in either Voltanative cubin format see Compatibility between Volta and Turing or PTX format see Applications Using CUDA Toolkit 8.0 or Earlier or both. 1.4.3. Applications Using CUDA Toolkit 10.0 CUDA applications built using CUDA Toolkit 10.0 are compatible with Turing as long as they are built to include kernels in Voltanative or Turingnative cubin format see Compatibility between Volta and Turing , or PTX format see Applications Using CUDA Toolkit 8.0 or Earlier , or both. 1.5. Building Applications with Turing Support When a CUDA application launches a kernel, the CUDA Runtime determines the compute capability of each GPU in the system and uses this information to automatically find the best matching cubin or PTX version of the kernel that is available. If a cubin file supporting the architecture of the target GPU is available, it is used otherwise, the CUDA Runtime will load the PTX and JITcompile that PTX to the GPUs native cubin format before launching it. If neither is available, then the kernel launch will fail. The method used to build your application with either native cubin or at least PTX support for Turing depend on the version of the CUDA Toolkit used. The main advantages of providing native cubins are as follows It saves the end user the time it takes to JITcompile kernels that are available only as PTX. All kernels compiled into the application must have native binaries at load time or else they will be built justintime from PTX, including kernels from all libraries linked to the application, even if those kernels are never launched by the application. Especially when using large libraries, this JIT compilation can take a significant amount of time. The CUDA driver will cache the cubins generated as a result of the PTX JIT, so this is mostly a onetime cost for a given user, but it is time best avoided whenever possible. PTX JITcompiled kernels often cannot take advantage of architectural features of newer GPUs, meaning that nativecompiled code may be faster or of greater accuracy. 1.5.1. Applications Using CUDA Toolkit 8.0 or Earlier The compilers included in CUDA Toolkit 8.0 or earlier generate cubin files native to earlier NVIDIA architectures such as Maxwell and Pascal, but they cannot generate cubin files native to Volta or Turing architecture. To allow support for Volta, Turing and future architectures when using"
Miscellaneous,NVIDIA Ampere GPU Architecture Compatibility Guide,https://docs.nvidia.com/cuda/ampere-compatibility-guide/index.html,"Ampere Compatibility Guide 1. NVIDIA Ampere GPU Architecture Compatibility v12.5 PDF Archive NVIDIA Ampere GPU Architecture Compatibility Guide for CUDA Applications The guide to building CUDA applications for GPUs based on the NVIDIA Ampere GPU Architecture. 1. NVIDIA Ampere GPU Architecture Compatibility 1.1. About this Document This application note, NVIDIA Ampere GPU Architecture Compatibility Guide for CUDA Applications, is intended to help developers ensure that their NVIDIA CUDA applications will run on the NVIDIA Ampere Architecture based GPUs. This document provides guidance to developers who are familiar with programming in CUDA C and want to make sure that their software applications are compatible with the NVIDIA Ampere GPU architecture. 1.2. Application Compatibility on the NVIDIA Ampere GPU Architecture A CUDA application binary with one or more GPU kernels can contain the compiled GPU code in two forms, binary cubin objects and forwardcompatible PTX assembly for each kernel. Both cubin and PTX are generated for a certain target compute capability. A cubin generated for a certain compute capability is supported to run on any GPU with the same major revision and same or higher minor revision of compute capability. For example, a cubin generated for compute capability 7.0 is supported to run on a GPU with compute capability 7.5, however a cubin generated for compute capability 7.5 is not supported to run on a GPU with compute capability 7.0, and a cubin generated with compute capability 7.x is not supported to run on a GPU with compute capability 8.x. Kernel can also be compiled to a PTX form. At the application load time, PTX is compiled to cubin and the cubin is used for kernel execution. Unlike cubin, PTX is forwardcompatible. Meaning PTX is supported to run on any GPU with compute capability higher than the compute capability assumed for generation of that PTX. For example, PTX code generated for compute capability 7.x is supported to run on compute capability 7.x or any higher revision major or minor, including compute capability 8.x. Therefore although it is optional, it is recommended that all applications should include PTX of the kernels to ensure forwardcompatibility. To read more about cubin and PTX compatibilities see Compilation with NVCC from the Programming Guide. When a CUDA application launches a kernel on a GPU, the CUDA Runtime determines the compute capability of the GPU in the system and uses this information to find the best matching cubin or PTX version of the kernel. If a cubin compatible with that GPU is present in the binary, the cubin is used asis for execution. Otherwise, the CUDA Runtime first generates compatible cubin by JITcompiling 1 the PTX and then the cubin is used for the execution. If neither compatible cubin nor PTX is available, kernel launch results in a failure. Application binaries that include PTX version of kernels, should work asis on the NVIDIA Ampere architecture based GPUs. In such cases, rebuilding the application is not required. However application binaries which do not include PTX only include cubins, need to be rebuilt to run on the NVIDIA Ampere architecture based GPUs. To know more about building compatible applications read Building Applications with the NVIDIA Ampere GPU Architecture Support . 1.3. Verifying Ampere Compatibility for Existing Applications The first step towards making a CUDA application compatible with the NVIDIA Ampere GPU architecture is to check if the application binary already contains compatible GPU code at least the PTX. The following sections explain how to accomplish this for an already built CUDA application. 1.3.1. Applications Built Using CUDA Toolkit 10.2 or Earlier CUDA applications built using CUDA Toolkit versions 2.1 through 10.2 are compatible with NVIDIA Ampere architecture based GPUs as long as they are built to include PTX versions of their kernels. This can be tested by forcing the PTX to JITcompile at application load time with following the steps Download and install the latest driver from httpswww.nvidia.comdrivers . Set the environment variable CUDAFORCEPTXJIT1 . Launch the application. With CUDAFORCEPTXJIT1 , GPU binary code embedded in an application binary is ignored. Instead PTX code for each kernel is JITcompiled to produce GPU binary code. An application fails to execute if it does not include PTX. This means the application is not compatible with the NVIDIA Ampere GPU architecture and needs to be rebuilt for compatibility. On the other hand, if the application works properly with this environment variable set, then the application is compatible with the NVIDIA Ampere GPU architecture. Note Be sure to unset the CUDAFORCEPTXJIT environment variable after testing is done. 1.3.2. Applications Built Using CUDA Toolkit 11.0 CUDA applications built using CUDA Toolkit 11.0 are compatible with the NVIDIA Ampere GPU architecture as long as they are built to include kernels in native cubin compute capability 8.0 or PTX form or both. 1.4. Building Applications with the NVIDIA Ampere GPU Architecture Support Depending on the version of the CUDA Toolkit used for building the application, it can be built to include PTX andor native cubin for the NVIDIA Ampere GPU architecture. Although it is enough to just include PTX, including native cubin also has the following advantages It saves the end user the time it takes to JITcompile kernels that are available only as PTX. All kernels which do not have native cubins are JITcompiled from PTX, including kernels from all the libraries linked to the application, even if those kernels are never launched by the application. Especially when using large libraries, this JIT compilation can take a significant amount of time. The CUDA driver caches the cubins generated as a result of the PTX JIT, so this is mostly a onetime cost for a user, but it is time best avoided whenever possible. PTX JITcompiled kernels often cannot take advantage of architectural features of newer GPUs, meaning that nativecompiled cubins may be faster or of greater accuracy. 1.4.1. Building Applications Using CUDA Toolkit 10.x or Earlier The nvcc compiler included with versions 10.x 10.0, 10.1 and 10.2 of the CUDA Toolkit can generate cubins native to"
Miscellaneous,Hopper Compatibility Guide,https://docs.nvidia.com/cuda/hopper-compatibility-guide/index.html,"Hopper Compatibility Guide 1. Hopper Architecture Compatibility v12.5 PDF Archive Hopper Compatibility Guide for CUDA Applications The guide to building CUDA applications for Hopper GPUs 1. Hopper Architecture Compatibility 1.1. About this Document This application note, Hopper Architecture Compatibility Guide for CUDA Applications, is intended to help developers ensure that their NVIDIA CUDA applications will run on the NVIDIA Hopper architecture based GPUs. This document provides guidance to developers who are familiar with programming in CUDA C and want to make sure that their software applications are compatible with Hopper architecture. 1.2. Application Compatibility on Hopper Architecture A CUDA application binary with one or more GPU kernels can contain the compiled GPU code in two forms, binary cubin objects and forwardcompatible PTX assembly for each kernel. Both cubin and PTX are generated for a certain target compute capability. A cubin generated for a certain compute capability is supported to run on any GPU with the same major revision and same or higher minor revision of compute capability. For example, a cubin generated for compute capability 8.0 is supported to run on a GPU with compute capability 8.6, however a cubin generated for compute capability 8.6 is not supported to run on a GPU with compute capability 8.0, and a cubin generated with compute capability 8.x is not supported to run on a GPU with compute capability 9.0. Kernel can also be compiled to a PTX form. At the application load time, PTX is compiled to cubin and the cubin is used for kernel execution. Unlike cubin, PTX is forwardcompatible. Meaning PTX is supported to run on any GPU with compute capability higher than the compute capability assumed for generation of that PTX. For example, PTX code generated for compute capability 8.x is supported to run on compute capability 8.x or any higher revision major or minor, including compute capability 9.0. Therefore although it is optional, it is recommended that all applications should include PTX of the kernels to ensure forwardcompatibility. To read more about cubin and PTX compatibilities see Compilation with NVCC from the CUDA C Programming Guide . When a CUDA application launches a kernel on a GPU, the CUDA Runtime determines the compute capability of the GPU in the system and uses this information to find the best matching cubin or PTX version of the kernel. If a cubin compatible with that GPU is present in the binary, the cubin is used asis for execution. Otherwise, the CUDA Runtime first generates compatible cubin by JITcompiling 1 the PTX and then the cubin is used for the execution. If neither compatible cubin nor PTX is available, kernel launch results in a failure. Application binaries that include PTX version of kernels, should work asis on the Hopper GPUs. In such cases, rebuilding the application is not required. However application binaries which do not include PTX only include cubins, need to be rebuilt to run on the Hopper GPUs. To know more about building compatible applications read Building Applications with Hopper Architecture Support Application binaries that include PTX version of kernels with architecture conditional features using sm90a or compute90a in order to take full advantage of Hopper GPU architecture, are not forward or backward compatible. 1.3. Verifying Hopper Compatibility for Existing Applications The first step towards making a CUDA application compatible with Hopper architecture is to check if the application binary already contains compatible GPU code at least the PTX. The following sections explain how to accomplish this for an already built CUDA application. 1.3.1. Applications Built Using CUDA Toolkit 11.7 or Earlier CUDA applications built using CUDA Toolkit versions 2.1 through 11.7 are compatible with Hopper GPUs as long as they are built to include PTX versions of their kernels. This can be tested by forcing the PTX to JITcompile at application load time with following the steps Download and install the latest driver from httpswww.nvidia.comdrivers . Set the environment variable CUDAFORCEPTXJIT1 . Launch the application. With CUDAFORCEPTXJIT1 , GPU binary code embedded in an application binary is ignored. Instead PTX code for each kernel is JITcompiled to produce GPU binary code. An application fails to execute if it does not include PTX. This means the application is not Hopper architecture compatible and needs to be rebuilt for compatibility. On the other hand, if the application works properly with this environment variable set, then the application is Hopper compatible. Note Be sure to unset the CUDAFORCEPTXJIT environment variable after testing is done. 1.3.2. Applications Built Using CUDA Toolkit 11.8 CUDA applications built using CUDA Toolkit 11.8 are compatible with Hopper architecture as long as they are built to include kernels in native cubin compute capability 9.0 or PTX form or both. 1.4. Building Applications with Hopper Architecture Support Depending on the version of the CUDA Toolkit used for building the application, it can be built to include PTX andor native cubin for the Hopper architecture. Although it is enough to just include PTX, including native cubin also has the following advantages It saves the end user the time it takes to JITcompile kernels that are available only as PTX. All kernels which do not have native cubins are JITcompiled from PTX, including kernels from all the libraries linked to the application, even if those kernels are never launched by the application 2 . Especially when using large libraries, this JIT compilation can take a significant amount of time. The CUDA driver caches the cubins generated as a result of the PTX JIT, so this is mostly a onetime cost for a user, but it is time best avoided whenever possible. PTX JITcompiled kernels often cannot take advantage of architectural features of newer GPUs, meaning that nativecompiled cubins may be faster or of greater accuracy. PTX code compiled to target architecture conditional features using sm90a or compute90a only runs on devices with compute capability 9.0 and is not backward or forward compatible. 1.4.1. Building Applications Using CUDA Toolkit 11.7 or Earlier The nvcc compiler included with version 11.7 or earlier 11.011.7"
Miscellaneous,Ada Compatibility Guide,https://docs.nvidia.com/cuda/ada-compatibility-guide/index.html,"Ada Compatibility Guide 1. NVIDIA Ada GPU Architecture Compatibility v12.5 PDF Archive NVIDIA Ada GPU Architecture Compatibility Guide for CUDA Applications The guide to building CUDA applications for NVIDIA Ada GPUs. 1. NVIDIA Ada GPU Architecture Compatibility 1.1. About this Document This application note, NVIDIA Ada GPU Architecture Compatibility Guide for CUDA Applications , is intended to help developers ensure that their NVIDIA CUDA applications will run on the NVIDIA Ada Architecture based GPUs. This document provides guidance to developers who are familiar with programming in CUDA C and want to make sure that their software applications are compatible with the NVIDIA Ada GPU architecture. 1.2. Application Compatibility on the NVIDIA Ada GPU Architecture A CUDA application binary with one or more GPU kernels can contain the compiled GPU code in two forms, binary cubin objects and forwardcompatible PTX assembly for each kernel. Both cubin and PTX are generated for a certain target compute capability. A cubin generated for a certain compute capability is supported to run on any GPU with the same major revision and same or higher minor revision of compute capability. For example, a cubin generated for compute capability 8.6 is supported to run on a GPU with compute capability 8.9 however, a cubin generated for compute capability 8.9 is not supported to run on a GPU with compute capability 8.6, and a cubin generated with compute capability 8.x is not supported to run on a GPU with compute capability 9.0. Kernels can also be compiled to a PTX form. At the application load time, PTX is compiled to cubin and the cubin is used for kernel execution. Unlike cubin, PTX is forwardcompatible. Meaning PTX is supported to run on any GPU with compute capability higher than the compute capability assumed for generation of that PTX. For example, PTX code generated for compute capability 8.x is supported to run on compute capability 8.x or any higher revision major or minor, including compute capability 9.x. Therefore, although it is optional, it is recommended that all applications should include PTX of the kernels to ensure forwardcompatibility. To read more about cubin and PTX compatibilities see Compilation with NVCC from the CUDA C Programming Guide . When a CUDA application launches a kernel on a GPU, the CUDA Runtime determines the compute capability of the GPU in the system and uses this information to find the best matching cubin or PTX version of the kernel. If a cubin compatible with that GPU is present in the binary, the cubin is used asis for execution. Otherwise, the CUDA Runtime first generates compatible cubin by JITcompiling 1 the PTX and then the cubin is used for the execution. If neither compatible cubin nor PTX is available, kernel launch results in a failure. Application binaries that include PTX version of kernels should work asis on the NVIDIA Ada architecture based GPUs. In such cases, rebuilding the application is not required. However, application binaries that do not include PTX only include cubins need to be rebuilt to run on the NVIDIA Ada architecture based GPUs. To know more about building compatible applications, read Building Applications with the NVIDIA Ada GPU Architecture Support . 1.3. Compatibility between Ampere and Ada The NVIDIA Ada architecture is based on Amperes Instruction Set Architecture ISA 8.0, extending it with new instructions. As a consequence, any binary that runs on Ampere will be able to run on Ada forward compatibility, but an Ada binary will not be able to run on Ampere. 1.4. Verifying Ada Compatibility for Existing Applications The first step towards making a CUDA application compatible with the NVIDIA Ada GPU architecture is to check if the application binary already contains compatible GPU code at least the PTX. The following sections explain how to accomplish this for an already built CUDA application. 1.4.1. Applications Built Using CUDA Toolkit 10.2 or Earlier CUDA applications built using CUDA Toolkit versions 2.1 through 10.2 are compatible with NVIDIA Ada architecture based GPUs as long as they are built to include PTX versions of their kernels. This can be tested by forcing the PTX to JITcompile at application load time with following the steps Download and install the latest driver from httpswww.nvidia.comdrivers . Set the environment variable CUDAFORCEPTXJIT1 . Launch the application. With CUDAFORCEPTXJIT1 , GPU binary code embedded in an application binary is ignored. Instead PTX code for each kernel is JITcompiled to produce GPU binary code. An application fails to execute if it does not include PTX. This means the application is not compatible with the NVIDIA Ada GPU architecture and needs to be rebuilt for compatibility. On the other hand, if the application works properly with this environment variable set, then the application is compatible with the NVIDIA Ada GPU architecture. Note Be sure to unset the CUDAFORCEPTXJIT environment variable after testing is done. 1.4.2. Applications Built Using CUDA Toolkit 11.0 through 11.7 CUDA applications built using CUDA Toolkit 11.0 through 11.7 are compatible with the NVIDIA Ada GPU architecture as long as they are built to include kernels in Amperenative cubin see Compatibility between Ampere and Ada or PTX format see Applications Built Using CUDA Toolkit 10.2 or Earlier , or both. 1.4.3. Applications Built Using CUDA Toolkit 11.8 CUDA applications built using CUDA Toolkit 11.8 are compatible with the NVIDIA Ada GPU architecture as long as they are built to include kernels in Amperenative or Adanative cubin see Compatibility between Ampere and Ada , or PTX format see Applications Built Using CUDA Toolkit 10.2 or Earlier , or both. 1.5. Building Applications with the NVIDIA Ada GPU Architecture Support Depending on the version of the CUDA Toolkit used for building the application, it can be built to include PTX andor native cubin for the NVIDIA Ada GPU architecture. Although it is sufficient to just include PTX, including native cubin also has the following advantages It saves the end user the time it takes to JITcompile kernels that are available only as PTX. All kernels that do"
Miscellaneous,Maxwell Tuning Guide,https://docs.nvidia.com/cuda/maxwell-tuning-guide/index.html,"Maxwell Tuning Guide 1. Maxwell Tuning Guide v12.5 PDF Archive Tuning CUDA Applications for Maxwell The programming guide to tuning CUDA Applications for GPUs based on the NVIDIA Maxwell Architecture. 1. Maxwell Tuning Guide 1.1. NVIDIA Maxwell Compute Architecture Maxwell is NVIDIAs nextgeneration architecture for CUDA compute applications. Maxwell retains and extends the same CUDA programming model as in previous NVIDIA architectures such as Fermi and Kepler, and applications that follow the best practices for those architectures should typically see speedups on the Maxwell architecture without any code changes. This guide summarizes the ways that an application can be finetuned to gain additional speedups by leveraging Maxwell architectural features. 1 Maxwell introduces an allnew design for the Streaming Multiprocessor SM that dramatically improves energy efficiency. Although the Kepler SMX design was extremely efficient for its generation, through its development, NVIDIAs GPU architects saw an opportunity for another big leap forward in architectural efficiency the Maxwell SM is the realization of that vision. Improvements to control logic partitioning, workload balancing, clockgating granularity, compilerbased scheduling, number of instructions issued per clock cycle, and many other enhancements allow the Maxwell SM also called SMM to far exceed Kepler SMX efficiency. The first Maxwellbased GPU is codenamed GM107 and is designed for use in powerlimited environments like notebooks and small form factor SFF PCs. GM107 is described in a whitepaper entitled NVIDIA GeForce GTX 750 Ti Featuring FirstGeneration Maxwell GPU Technology, Designed for Extreme Performance per Watt . 2 The first GPU using the secondgeneration Maxwell architecture is codenamed GM204 . Secondgeneration Maxwell GPUs retain the power efficiency of the earlier generation while delivering significantly higher performance. GM204 is described in a whitepaper entitled NVIDIA GeForce GTX 980 Featuring Maxwell, The Most Advanced GPU Ever Made . Compute programming features of GM204 are similar to those of GM107, except where explicitly noted in this guide. For details on the programming features discussed in this guide, please refer to the CUDA C Programming Guide . 1.2. CUDA Best Practices The performance guidelines and best practices described in the CUDA C Programming Guide and the CUDA C Best Practices Guide apply to all CUDAcapable GPU architectures. Programmers must primarily focus on following those recommendations to achieve the best performance. The highpriority recommendations from those guides are as follows Find ways to parallelize sequential code, Minimize data transfers between the host and the device, Adjust kernel launch configuration to maximize device utilization, Ensure global memory accesses are coalesced, Minimize redundant accesses to global memory whenever possible, Avoid long sequences of diverged execution by threads within the same warp. 1.3. Application Compatibility Before addressing specific performance tuning issues covered in this guide, refer to the Maxwell Compatibility Guide for CUDA Applications to ensure that your application is compiled in a way that is compatible with Maxwell. 1.4. Maxwell Tuning 1.4.1. SMM The Maxwell Streaming Multiprocessor, SMM, is similar in many respects to the Kepler architectures SMX. The key enhancements of SMM over SMX are geared toward improving efficiency without requiring significant increases in available parallelism per SM from the application. 1.4.1.1. Occupancy The maximum number of concurrent warps per SMM remains the same as in SMX i.e., 64, and factors influencing warp occupancy remain similar or improved over SMX The register file size 64k 32bit registers is the same as that of SMX. The maximum registers per thread, 255, matches that of Kepler GK110. As with Kepler, experimentation should be used to determine the optimum balance of register spilling vs. occupancy, however. The maximum number of thread blocks per SM has been increased from 16 to 32. This should result in an automatic occupancy improvement for kernels with small thread blocks of 64 or fewer threads shared memory and register file resource requirements permitting. Such kernels would have tended to underutilize SMX, but less so SMM. Shared memory capacity is increased see Shared Memory Capacity . As such, developers can expect similar or improved occupancy on SMM without changes to their application. At the same time, warp occupancy requirements i.e., available parallelism for maximum device utilization are similar to or less than those of SMX see Instruction Latencies . 1.4.1.2. Instruction Scheduling The number of CUDA Cores per SM has been reduced to a power of two, however with Maxwells improved execution efficiency, performance per SM is usually within 10 of Kepler performance, and the improved area efficiency of SMM means CUDA Cores per GPU will be substantially higher vs. comparable Fermi or Kepler chips. SMM retains the same number of instruction issue slots per clock and reduces arithmetic latencies compared to the Kepler design. As with SMX, each SMM has four warp schedulers. Unlike SMX, however, all SMM core functional units are assigned to a particular scheduler, with no shared units. Along with the selection of a poweroftwo number of CUDA Cores per SM, which simplifies scheduling and reduces stall cycles, this partitioning of SM computational resources in SMM is a major component of the streamlined efficiency of SMM. The poweroftwo number of CUDA Cores per partition simplifies scheduling, as each of SMMs warp schedulers issue to a dedicated set of CUDA Cores equal to the warp width. Each warp scheduler still has the flexibility to dualissue such as issuing a math operation to a CUDA Core in the same cycle as a memory operation to a loadstore unit, but singleissue is now sufficient to fully utilize all CUDA Cores. 1.4.1.3. Instruction Latencies Another major improvement of SMM is that dependent math latencies have been significantly reduced a consequence of this is a further reduction of stall cycles, as the available warplevel parallelism i.e., occupancy on SMM should be equal to or greater than that of SMX see Occupancy , while at the same time each math operation takes less time to complete, improving utilization and throughput. 1.4.1.4. Instruction Throughput The most significant changes to peak instruction throughputs in SMM are as follows The change in number of CUDA Cores per SM brings with it a corresponding"
Miscellaneous,Pascal Tuning Guide,https://docs.nvidia.com/cuda/pascal-tuning-guide/index.html,"Pascal Tuning Guide 1. Pascal Tuning Guide v12.5 PDF Archive Tuning CUDA Applications for Pascal The programming guide to tuning CUDA Applications for GPUs based on the NVIDIA Pascal Architecture. 1. Pascal Tuning Guide 1.1. NVIDIA Pascal Compute Architecture Pascal retains and extends the same CUDA programming model provided by previous NVIDIA architectures such as Maxwell, and applications that follow the best practices for those architectures should typically see speedups on the Pascal architecture without any code changes. This guide summarizes the ways that an application can be finetuned to gain additional speedups by leveraging Pascal architectural features. 1 Pascal architecture comprises two major variants GP100 and GP104. 2 A detailed overview of the major improvements in GP100 and GP104 over earlier NVIDIA architectures are described in a pair of white papers entitled NVIDIA Tesla P100 The Most Advanced Datacenter Accelerator Ever Built for GP100 and NVIDIA GeForce GTX 1080 Gaming Perfected for GP104. For further details on the programming features discussed in this guide, please refer to the CUDA C Programming Guide . Some of the Pascal features described in this guide are specific to either GP100 or GP104, as noted if not specified, features apply to both Pascal variants. 1.2. CUDA Best Practices The performance guidelines and best practices described in the CUDA C Programming Guide and the CUDA C Best Practices Guide apply to all CUDAcapable GPU architectures. Programmers must primarily focus on following those recommendations to achieve the best performance. The highpriority recommendations from those guides are as follows Find ways to parallelize sequential code, Minimize data transfers between the host and the device, Adjust kernel launch configuration to maximize device utilization, Ensure global memory accesses are coalesced, Minimize redundant accesses to global memory whenever possible, Avoid long sequences of diverged execution by threads within the same warp. 1.3. Application Compatibility Before addressing specific performance tuning issues covered in this guide, refer to the Pascal Compatibility Guide for CUDA Applications to ensure that your application is compiled in a way that is compatible with Pascal. 1.4. Pascal Tuning 1.4.1. Streaming Multiprocessor The Pascal Streaming Multiprocessor SM is in many respects similar to that of Maxwell. Pascal further improves the already excellent power efficiency provided by the Maxwell architecture through both an improved 16nm FinFET manufacturing process and various architectural modifications. 1.4.1.1. Instruction Scheduling Like Maxwell, Pascal employs a poweroftwo number of CUDA Cores per partition. This simplifies scheduling, since each of the SMs warp schedulers issue to a dedicated set of CUDA Cores equal to the warp width 32. Each warp scheduler still has the flexibility to dualissue such as issuing a math operation to a CUDA Core in the same cycle as a memory operation to a loadstore unit, but singleissue is now sufficient to fully utilize all CUDA Cores. GP100 and GP104 designs incorporate different numbers of CUDA Cores per SM. Like Maxwell, each GP104 SM provides four warp schedulers managing a total of 128 singleprecision FP32 and four doubleprecision FP64 cores. A GP104 processor provides up to 20 SMs, and the similar GP102 design provides up to 30 SMs. By contrast GP100 provides smaller but more numerous SMs. Each GP100 provides up to 60 SMs. 3 Each SM contains two warp schedulers managing a total of 64 FP32 and 32 FP64 cores. The resulting 21 ratio of FP32 to FP64 cores aligns well with GP100s new datapath configuration, allowing Pascal to process FP64 workloads more efficiently than Kepler GK210, the previous NVIDIA architecture to emphasize FP64 performance. 1.4.1.2. Occupancy The maximum number of concurrent warps per SM remains the same as in Maxwell i.e., 64, and other factors influencing warp occupancy remain similar as well The register file size 64k 32bit registers is the same as that of Maxwell. The maximum registers per thread, 255, matches that of Maxwell. As with previous architectures, experimentation should be used to determine the optimum balance of register spilling vs. occupancy, however. The maximum number of thread blocks per SM is 32, the same as Maxwell. Shared memory capacity per SM is 64KB for GP100 and 96KB for GP104. For comparison, Maxwell provided 96KB and up to 112KB of shared memory, respectively. But each GP100 SM contains fewer CUDA Cores, so the shared memory available per core actually increases on GP100. The maximum shared memory per block remains limited at 48KB as with prior architectures see Shared Memory Capacity . As such, developers can expect similar occupancy as on Maxwell without changes to their application. As a result of scheduling improvements relative to Kepler, warp occupancy requirements i.e., available parallelism needed for maximum device utilization are generally reduced. 1.4.2. New Arithmetic Primitives 1.4.2.1. FP16 Arithmetic Support Pascal provides improved FP16 support for applications, like deep learning, that are tolerant of low floatingpoint precision. The half type is used to represent FP16 values on the device. As with Maxwell, FP16 storage can be used to reduce the required memory footprint and bandwidth compared to FP32 or FP64 storage. Pascal also adds support for native FP16 instructions. Peak FP16 throughput is attained by using a paired operation to perform two FP16 instructions per core simultaneously. To be eligible for the paired operation the operands must be stored in a half2 vector type. GP100 and GP104 provide different FP16 throughputs. GP100, designed with training deep neural networks in mind, provides FP16 throughput up to 2x that of FP32 arithmetic. On GP104, FP16 throughput is lower, 164th that of FP32. However, compensating for reduced FP16 throughput, GP104 provides additional highthroughput INT8 support not available in GP100. 1.4.2.2. INT8 Dot Product GP104 provides specialized instructions for twoway and fourway integer dot products. These are well suited for accelerating Deep Learning inference workloads. The dp4a intrinsic computes a dot product of four 8bit integers with accumulation into a 32bit integer. Similarly, dp2a performs a twoelement dot product between two 16bit integers in one vector, and two 8bit integers in another with accumulation into a 32bit integer. Both instructions offer a throughput equal to"
Miscellaneous,Volta Tuning Guide,https://docs.nvidia.com/cuda/volta-tuning-guide/index.html,"Volta Tuning Guide 1. Volta Tuning Guide v12.5 PDF Archive Tuning CUDA Applications for Volta The programming guide to tuning CUDA Applications for GPUs based on the NVIDIA Volta Architecture. 1. Volta Tuning Guide 1.1. NVIDIA Volta Compute Architecture Volta is NVIDIAs latest architecture for CUDA compute applications. Volta retains and extends the same CUDA programming model provided by previous NVIDIA architectures such as Maxwell and Pascal, and applications that follow the best practices for those architectures should typically see speedups on the Volta architecture without any code changes. This guide summarizes the ways that an application can be finetuned to gain additional speedups by leveraging Volta architectural features. 1 Volta architecture comprises a single variant GV100. A detailed overview of the major improvements in GV100 over earlier NVIDIA architectures is provided in a white paper entitled NVIDIA Tesla V100 GPU Architecture The Worlds Most Advanced Datacenter GPU . For further details on the programming features discussed in this guide, please refer to the CUDA C Programming Guide . 1.2. CUDA Best Practices The performance guidelines and best practices described in the CUDA C Programming Guide and the CUDA C Best Practices Guide apply to all CUDAcapable GPU architectures. Programmers must primarily focus on following those recommendations to achieve the best performance. The highpriority recommendations from those guides are as follows Find ways to parallelize sequential code, Minimize data transfers between the host and the device, Adjust kernel launch configuration to maximize device utilization, Ensure global memory accesses are coalesced, Minimize redundant accesses to global memory whenever possible, Avoid long sequences of diverged execution by threads within the same warp. 1.3. Application Compatibility Before addressing specific performance tuning issues covered in this guide, refer to the Volta Compatibility Guide for CUDA Applications to ensure that your application is compiled in a way that is compatible with Volta. 1.4. Volta Tuning 1.4.1. Streaming Multiprocessor The Volta Streaming Multiprocessor SM provides the following improvements over Pascal. 1.4.1.1. Instruction Scheduling Each Volta SM includes 4 warpscheduler units. Each scheduler handles a static set of warps and issues to a dedicated set of arithmetic instruction units. Instructions are performed over two cycles, and the schedulers can issue independent instructions every cycle. Dependent instruction issue latency for core FMA math operations are reduced to four clock cycles, compared to six cycles on Pascal. As a result, execution latencies of core math operations can be hidden by as few as 4 warps per SM, assuming 4way instructionlevel parallelism ILP per warp. Many more warps are, of course, recommended to cover the much greater latency of memory transactions and controlflow operations. Similar to GP100, the GV100 SM provides 64 FP32 cores and 32 FP64 cores. The GV100 SM additionally includes 64 INT32 cores and 8 mixedprecision Tensor Cores. GV100 provides up to 84 SMs. 1.4.1.2. Independent Thread Scheduling The Volta architecture introduces Independent Thread Scheduling among threads in a warp. This feature enables intrawarp synchronization patterns previously unavailable and simplifies code changes when porting CPU code. However, Independent Thread Scheduling can also lead to a rather different set of threads participating in the executed code than intended if the developer made assumptions about warpsynchronicity 2 of previous hardware architectures. When porting existing codes to Volta, the following three code patterns need careful attention. For more details see the CUDA C Programming Guide . To avoid data corruption, applications using warp intrinsics shfl , any , all , and ballot should transition to the new, safe, synchronizing counterparts, with the sync suffix. The new warp intrinsics take in a mask of threads that explicitly define which lanes threads of a warp must participate in the warp intrinsic. Applications that assume reads and writes are implicitly visible to other threads in the same warp need to insert the new syncwarp warpwide barrier synchronization instruction between steps where data is exchanged between threads via global or shared memory. Assumptions that code is executed in lockstep or that readswrites from separate threads are visible across a warp without synchronization are invalid. Applications using syncthreads or the PTX bar.sync and their derivatives in such a way that a barrier will not be reached by some nonexited thread in the thread block must be modified to ensure that all nonexited threads reach the barrier. The racecheck and synccheck tools provided by computesanitizer can help with locating violations. 1.4.1.3. Occupancy The maximum number of concurrent warps per SM remains the same as in Pascal i.e., 64, and other factors influencing warp occupancy remain similar as well The register file size is 64k 32bit registers per SM. The maximum registers per thread is 255. The maximum number of thread blocks per SM is 32. Shared memory capacity per SM is 96KB, similar to GP104, and a 50 increase compared to GP100. Overall, developers can expect similar occupancy as on Pascal without changes to their application. 1.4.1.4. Integer Arithmetic Unlike Pascal GPUs, the GV100 SM includes dedicated FP32 and INT32 cores. This enables simultaneous execution of FP32 and INT32 operations. Applications can now interleave pointer arithmetic with floatingpoint computations. For example, each iteration of a pipelined loop could update addresses and load data for the next iteration while simultaneously processing the current iteration at full FP32 throughput. 1.4.2. Tensor Core Operations Each Tensor Core performs the following operation D AxB C, where A, B, C, and D are 4x4 matrices. The matrix multiply inputs A and B are FP16 matrices, while the accumulation matrices C and D may be FP16 or FP32 matrices. When accumulating in FP32, the FP16 multiply results in a full precision product that is then accumulated using FP32 addition with the other intermediate products for a 4x4x4 matrix multiply. In practice, Tensor Cores are used to perform much larger 2D or higher dimensional matrix operations, built up from these smaller elements. The Volta tensor cores are exposed as WarpLevel Matrix Operations in the CUDA 9 C API. The API exposes specialized matrix load, matrix multiply and accumulate, and matrix store operations to"
Miscellaneous,Turing Tuning Guide,https://docs.nvidia.com/cuda/turing-tuning-guide/index.html,"Turing Tuning Guide 1. Turing Tuning Guide v12.5 PDF Archive Tuning CUDA Applications for Turing The programming guide to tuning CUDA Applications for GPUs based on the NVIDIA Turing Architecture. 1. Turing Tuning Guide 1.1. NVIDIA Turing Compute Architecture Turing is NVIDIAs latest architecture for CUDA compute applications. Turing retains and extends the same CUDA programming model provided by previous NVIDIA architectures such as Pascal and Volta, and applications that follow the best practices for those architectures should typically see speedups on the Turing architecture without any code changes. This guide summarizes the ways that an application can be finetuned to gain additional speedups by leveraging Turing architectural features. 1 For further details on the programming features discussed in this guide, please refer to the CUDA C Programming Guide . 1.2. CUDA Best Practices The performance guidelines and best practices described in the CUDA C Programming Guide and the CUDA C Best Practices Guide apply to all CUDAcapable GPU architectures. Programmers must primarily focus on following those recommendations to achieve the best performance. The highpriority recommendations from those guides are as follows Find ways to parallelize sequential code, Minimize data transfers between the host and the device, Adjust kernel launch configuration to maximize device utilization, Ensure global memory accesses are coalesced, Minimize redundant accesses to global memory whenever possible, Avoid long sequences of diverged execution by threads within the same warp. 1.3. Application Compatibility Before addressing specific performance tuning issues covered in this guide, refer to the Turing Compatibility Guide for CUDA Applications to ensure that your application is compiled in a way that is compatible with Turing. 1.4. Turing Tuning 1.4.1. Streaming Multiprocessor The Turing Streaming Multiprocessor SM is based on the same major architecture 7.x as Volta, and provides similar improvements over Pascal. 1.4.1.1. Instruction Scheduling Each Turing SM includes 4 warpscheduler units. Each scheduler handles a static set of warps and issues to a dedicated set of arithmetic instruction units. Instructions are performed over two cycles, and the schedulers can issue independent instructions every cycle. Dependent instruction issue latency for core FMA math operations is four clock cycles, like Volta, compared to six cycles on Pascal. As a result, execution latencies of core math operations can be hidden by as few as 4 warps per SM, assuming 4way instructionlevel parallelism ILP per warp, or by 16 warps per SM without any instuctionlevel parallelism. Like Volta, the Turing SM provides 64 FP32 cores, 64 INT32 cores and 8 improved mixedprecision Tensor Cores. Turing has a lower double precision throughput than Volta with only 2 FP64 cores. 1.4.1.2. Independent Thread Scheduling The Turing architecture features the same Independent Thread Scheduling introduced with Volta. This enables intrawarp synchronization patterns previously unavailable and simplifies code changes when porting CPU code. However, Independent Thread Scheduling can also lead to a rather different set of threads participating in the executed code than intended if the developer made assumptions about warpsynchronicity 2 of previous hardware architectures. When porting existing codes to Volta or Turing, the following three code patterns need careful attention. For more details see the CUDA C Programming Guide . To avoid data corruption, applications using warp intrinsics shfl , any , all , and ballot should transition to the new, safe, synchronizing counterparts, with the sync suffix. The new warp intrinsics take in a mask of threads that explicitly define which lanes threads of a warp must participate in the warp intrinsic. Applications that assume reads and writes are implicitly visible to other threads in the same warp need to insert the new syncwarp warpwide barrier synchronization instruction between steps where data is exchanged between threads via global or shared memory. Assumptions that code is executed in lockstep or that readswrites from separate threads are visible across a warp without synchronization are invalid. Applications using syncthreads or the PTX bar.sync and their derivatives in such a way that a barrier will not be reached by some nonexited thread in the thread block must be modified to ensure that all nonexited threads reach the barrier. The racecheck and synccheck tools provided by computesanitizer can help with locating violations. 1.4.1.3. Occupancy The maximum number of concurrent warps per SM is 32 on Turing versus 64 on Volta. Other factors influencing warp occupancy remain otherwise similar The register file size is 64k 32bit registers per SM. The maximum registers per thread is 255. The maximum number of thread blocks per SM is 16. Shared memory capacity per SM is 64KB. Overall, developers can expect similar occupancy as on Pascal or Volta without changes to their application. 1.4.1.4. Integer Arithmetic Similar to Volta, the Turing SM includes dedicated FP32 and INT32 cores. This enables simultaneous execution of FP32 and INT32 operations. Applications can interleave pointer arithmetic with floatingpoint computations. For example, each iteration of a pipelined loop could update addresses and load data for the next iteration while simultaneously processing the current iteration at full FP32 throughput. 1.4.2. Tensor Core Operations Volta introduced Tensor Cores to accelerate matrix multiply operations on mixed precision floating point data. Turing adds acceleration for integer matrix multiply operations. The tensor cores are exposed as WarpLevel Matrix Operations in the CUDA 10 C API. The API provides specialized matrix load, matrix multiply and accumulate, and matrix store operations, where each warp processes a small matrix fragment, allowing to efficiently use Tensor Cores from a CUDAC program. In practice, Tensor Cores are used to perform much larger 2D or higher dimensional matrix operations, built up from these smaller matrix fragments. Each Tensor Core performs the matrix multiplyaccumulate D A x B C. The Tensor Cores support half precision matrix multiplication, where the matrix multiply inputs A and B are FP16 matrices, while the accumulation matrices C and D may be either FP16 or FP32 matrices. When accumulating in FP32, the FP16 multiply results in a full precision product that is then accumulated using FP32 addition. CUDA 10 supports several fragment sizes, 16x16x16, 32x8x16, and 8x32x16 to use the Tensor Cores on"
Miscellaneous,NVIDIA Ampere GPU Architecture Tuning Guide,https://docs.nvidia.com/cuda/ampere-tuning-guide/index.html,"Ampere Tuning Guide 1. NVIDIA Ampere GPU Architecture Tuning Guide v12.5 PDF Archive Tuning CUDA Applications for NVIDIA Ampere GPU Architecture The programming guide for tuning CUDA Applications for GPUs based on the NVIDIA Ampere GPU Architecture. 1. NVIDIA Ampere GPU Architecture Tuning Guide 1.1. NVIDIA Ampere GPU Architecture The NVIDIA Ampere GPU architecture is NVIDIAs latest architecture for CUDA compute applications. The NVIDIA Ampere GPU architecture retains and extends the same CUDA programming model provided by previous NVIDIA GPU architectures such as Turing and Volta, and applications that follow the best practices for those architectures should typically see speedups on the NVIDIA A100 GPU without any code changes. This guide summarizes the ways that an application can be finetuned to gain additional speedups by leveraging the NVIDIA Ampere GPU architectures features. 1 For further details on the programming features discussed in this guide, please refer to the CUDA C Programming Guide . 1.2. CUDA Best Practices The performance guidelines and best practices described in the CUDA C Programming Guide and the CUDA C Best Practices Guide apply to all CUDAcapable GPU architectures. Programmers must primarily focus on following those recommendations to achieve the best performance. The highpriority recommendations from those guides are as follows Find ways to parallelize sequential code. Minimize data transfers between the host and the device. Adjust kernel launch configuration to maximize device utilization. Ensure global memory accesses are coalesced. Minimize redundant accesses to global memory whenever possible. Avoid long sequences of diverged execution by threads within the same warp. 1.3. Application Compatibility Before addressing specific performance tuning issues covered in this guide, refer to the NVIDIA Ampere GPU Architecture Compatibility Guide for CUDA Applications to ensure that your application is compiled in a way that is compatible with the NVIDIA Ampere GPU Architecture. 1.4. NVIDIA Ampere GPU Architecture Tuning 1.4.1. Streaming Multiprocessor The NVIDIA Ampere GPU architectures Streaming Multiprocessor SM provides the following improvements over Volta and Turing. 1.4.1.1. Occupancy The maximum number of concurrent warps per SM remains the same as in Volta i.e., 64 for compute capability 8.0, while for compute capability 8.6 it is 48. Other factors influencing warp occupancy are The register file size is 64K 32bit registers per SM. The maximum number of registers per thread is 255. The maximum number of thread blocks per SM is 32 for devices of compute capability 8.0 i.e., A100 GPUs and 16 for GPUs with compute capability 8.6. For devices of compute capability 8.0 i.e., A100 GPUs shared memory capacity per SM is 164 KB, a 71 increase compared to V100s capacity of 96 KB. For GPUs with compute capability 8.6, shared memory capacity per SM is 100 KB. For devices of compute capability 8.0 i.e., A100 GPUs the maximum shared memory per thread block is 163 KB. For GPUs with compute capability 8.6 maximum shared memory per thread block is 99 KB. Overall, developers can expect similar occupancy as on Volta without changes to their application. 1.4.1.2. Asynchronous Data Copy from Global Memory to Shared Memory The NVIDIA Ampere GPU architecture adds hardware acceleration for copying data from global memory to shared memory. These copy instructions are asynchronous, with respect to computation and allow users to explicitly control overlap of compute with data movement from global memory into the SM. These instructions also avoid using extra registers for memory copies and can also bypass the L1 cache. This new feature is exposed via the pipeline API in CUDA. For more information please refer to the section on Async Copy in the CUDA C Programming Guide . 1.4.1.3. Hardware Acceleration for Split ArriveWait Barrier The NVIDIA Ampere GPU architecture adds hardware acceleration for a split arrivewait barrier in shared memory. These barriers can be used to implement fine grained thread controls, producerconsumer computation pipeline and divergence code patterns in CUDA. These barriers can also be used alongside the asynchronous copy. For more information on the ArriveWait Barriers refer to the ArriveWait Barrier section in the CUDA C Programming Guide . 1.4.1.4. Warp level support for Reduction Operations The NVIDIA Ampere GPU architecture adds native support for warp wide reduction operations for 32bit signed and unsigned integer operands. The warp wide reduction operations support arithmetic add , min , and max operations on 32bit signed and unsigned integers and bitwise and , or and xor operations on 32bit unsigned integers. For more details on the new warp wide reduction operations refer to Warp Reduce Functions in the CUDA C Programming Guide . 1.4.1.5. Improved Tensor Core Operations The NVIDIA Ampere GPU architecture includes new Third Generation Tensor Cores that are more powerful than the Tensor Cores used in Volta and Turing SMs. The new Tensor Cores use a larger base matrix size and add powerful new math modes including Support for FP64 Tensor Core, using new DMMA instructions. Support for Bfloat16 Tensor Core, through HMMA instructions. BFloat16 format is especially effective for DL training scenarios. Bfloat16 provides 8bit exponent i.e., same range as FP32, 7bit mantissa and 1 signbit. Support for TF32 Tensor Core, through HMMA instructions. TF32 is a new 19bit Tensor Core format that can be easily integrated into programs for more accurate DL training than 16bit HMMA formats. TF32 provides 8bit exponent, 10bit mantissa and 1 signbit. Support for bitwise AND along with bitwise XOR which was introduced in Turing, through BMMA instructions. The following table presents the evolution of matrix instruction sizes and supported data types for Tensor Cores across different GPU architecture generations. Instruction GPU Architecture Input Matrix format Output Accumulator format Matrix Instruction Size MxNxK HMMA 16bit precision NVIDIA Volta Architecture FP16 FP16 FP32 8x8x4 NVIDIA Turing Architecture FP16 FP16 FP32 8x8x4 16x8x8 16x8x16 NVIDIA Ampere Architecture FP16 BFloat16 FP16 FP32 BFloat16 only supports FP32 as accumulator 16x8x8 16x8x16 HMMA 19bit precision NVIDIA Volta Architecture NA NA NA NVIDIA Turing Architecture NA NA NA NVIDIA Ampere Architecture TF32 19bits FP32 16x8x4 IMMA Integer MMA NVIDIA Volta Architecture NA NA NA NVIDIA Turing Architecture unsigned charsigned char 8bit precision"
Miscellaneous,Hopper Tuning Guide,https://docs.nvidia.com/cuda/hopper-tuning-guide/index.html,"Hopper Tuning Guide 1. NVIDIA Hopper Tuning Guide v12.5 PDF Archive Tuning CUDA Applications for Hopper GPU Architecture The programming guide for tuning CUDA Applications for GPUs based on the Hopper GPU Architecture. 1. NVIDIA Hopper Tuning Guide 1.1. NVIDIA Hopper GPU Architecture The NVIDIA Hopper GPU architecture is NVIDIAs latest architecture for CUDA compute applications. The NVIDIA Hopper GPU architecture retains and extends the same CUDA programming model provided by previous NVIDIA GPU architectures such as NVIDIA Ampere GPU architecture and NVIDIA Turing, and applications that follow the best practices for those architectures should typically see speedups on the NVIDIA H100 GPU without any code changes. This guide summarizes the ways that an application can be finetuned to gain additional speedups by leveraging the NVIDIA Hopper GPU architectures features. 1 For further details on the programming features discussed in this guide, refer to the CUDA C Programming Guide . 1.2. CUDA Best Practices The performance guidelines and best practices described in the CUDA C Programming Guide and the CUDA C Best Practices Guide apply to all CUDAcapable GPU architectures. Programmers must primarily focus on following those recommendations to achieve the best performance. The highpriority recommendations from those guides are as follows Find ways to parallelize sequential code. Minimize data transfers between the host and the device. Adjust kernel launch configuration to maximize device utilization. Ensure that global memory accesses are coalesced. Minimize redundant accesses to global memory whenever possible. Avoid long sequences of diverged execution by threads within the same warp. 1.3. Application Compatibility Before addressing specific performance tuning issues covered in this guide, refer to the Hopper Compatibility Guide for CUDA Applications to ensure that your application is compiled in a way that is compatible with NVIDIA Hopper. 1.4. NVIDIA Hopper Tuning 1.4.1. Streaming Multiprocessor The NVIDIA Hopper Streaming Multiprocessor SM provides the following improvements over Turing and NVIDIA Ampere GPU architectures. 1.4.1.1. Occupancy The maximum number of concurrent warps per SM remains the same as in NVIDIA Ampere GPU architecture that is, 64, and other factors influencing warp occupancy are The register file size is 64K 32bit registers per SM. The maximum number of registers per thread is 255. The maximum number of thread blocks per SM is 32 for devices of compute capability 9.0 that is, H100 GPUs. For devices of compute capability 9.0 H100 GPUs, shared memory capacity per SM is 228 KB, a 39 increase compared to A100s capacity of 164 KB. For devices of compute capability 9.0 H100 GPUs, the maximum shared memory per thread block is 227 KB. For applications using Thread Block Clusters, it is always recommended to compute the occupancy using cudaOccupancyMaxActiveClusters and launch clusterbased kernels accordingly. Overall, developers can expect similar occupancy as on NVIDIA Ampere GPU architecture GPUs without changes to their application. 1.4.1.2. Tensor Memory Accelerator The Hopper architecture builds on top of the asynchronous copies introduced by NVIDIA Ampere GPU architecture and provides a more sophisticated asynchronous copy engine the Tensor Memory Accelerator TMA. TMA allows applications to transfer 1D and up to 5D tensors between global memory and shared memory, in both directions, as well as between the shared memory regions of different SMs in the same cluster refer to Thread Block Clusters . Additionally, for writes from shared memory to global memory, it allows specifying element wise reduction operations such as addminmax as well as bitwise andor for most common data types. This has several advantages Avoids using registers for moving data between the different memory spaces. Avoids using SM instructions for moving data a single thread can issue large data movement instructions to the TMA unit. The whole block can then continue working on other instructions while the data is in flight and only wait for the data to be consumed when actually necessary. Enables users to write warp specialized codes, where specific warps specialize on data movement between the different memory spaces while other warps only work on local data within the SM. This feature will be exposed through cudamemcpyasync along with the cudabarrier and cudapipeline for synchronizing data movement. 1.4.1.3. Thread Block Clusters NVIDIA Hopper Architecture adds a new optional level of hierarchy, Thread Block Clusters, that allows for further possibilities when parallelizing applications. A thread block can read from, write to, and perform atomics in shared memory of other thread blocks within its cluster. This is known as Distributed Shared Memory. As demonstrated in the CUDA C Programming Guide , there are applications that cannot fit required data within shared memory and must use global memory instead. Distributed shared memory can act as an intermediate step between these two options. Distributed Shared Memory can be used by an SM simultaneously with L2 cache accesses. This can benefit applications that need to communicate data between SMs by utilizing the combined bandwidth of both distributed shared memory and L2. In order to achieve best performance for accesses to Distributed Shared Memory, access patterns to those described in the CUDA C Best Practices Guide for Global Memory should be used. Specifically, accesses to Distributed Shared Memory should be coalesced and aligned to 32byte segments, if possible. Access patterns with nonunit stride should be avoided if possible, which can be achieved by using local shared memory, similar to what is shown in the CUDA C Best Practices Guide for Shared Memory . The maximum portable cluster size supported is 8 however, NVIDIA Hopper H100 GPU allows for a nonportable cluster size of 16 by opting in. Launching a kernel with a nonportable cluster size requires setting the cudaFuncAttributeNonPortableClusterSizeAllowed function attribute. Using larger cluster sizes may reduce the maximum number of active blocks across the GPU refer to Occupancy . 1.4.1.4. Improved FP32 Throughput Devices of compute capability 9.0 have 2x more FP32 operations per cycle per SM than devices of compute capability 8.0. 1.4.1.5. Dynamic Programming Instructions The NVIDIA Hopper architecture adds support for new instructions to accelerate dynamic programming algorithms, such as the SmithWaterman algorithm for sequence alignment in bioinformatics, and algorithms in"
Miscellaneous,Ada Tuning Guide,https://docs.nvidia.com/cuda/ada-tuning-guide/index.html,"Ada Tuning Guide 1. NVIDIA Ada GPU Architecture Tuning Guide v12.5 PDF Archive Tuning CUDA Applications for NVIDIA Ada GPU Architecture The programming guide for tuning CUDA Applications for GPUs based on the NVIDIA Ada GPU Architecture. 1. NVIDIA Ada GPU Architecture Tuning Guide 1.1. NVIDIA Ada GPU Architecture The NVIDIA Ada GPU architecture is NVIDIAs latest architecture for CUDA compute applications. The NVIDIA Ada GPU architecture retains and extends the same CUDA programming model provided by previous NVIDIA GPU architectures such as NVIDIA Ampere and Turing, and applications that follow the best practices for those architectures should typically see speedups on the NVIDIA Ada architecture without any code changes. This guide summarizes the ways that an application can be finetuned to gain additional speedups by leveraging the NVIDIA Ada GPU architectures features. 1 For further details on the programming features discussed in this guide, please refer to the CUDA C Programming Guide . 1.2. CUDA Best Practices The performance guidelines and best practices described in the CUDA C Programming Guide and the CUDA C Best Practices Guide apply to all CUDAcapable GPU architectures. Programmers must primarily focus on following those recommendations to achieve the best performance. The highpriority recommendations from those guides are as follows Find ways to parallelize sequential code. Minimize data transfers between the host and the device. Adjust kernel launch configuration to maximize device utilization. Ensure global memory accesses are coalesced. Minimize redundant accesses to global memory whenever possible. Avoid long sequences of diverged execution by threads within the same warp. 1.3. Application Compatibility Before addressing specific performance tuning issues covered in this guide, refer to the NVIDIA Ada GPU Architecture Compatibility Guide for CUDA Applications to ensure that your application is compiled in a way that is compatible with the NVIDIA Ada GPU Architecture. 1.4. NVIDIA Ada GPU Architecture Tuning 1.4.1. Streaming Multiprocessor The NVIDIA Ada GPU architectures Streaming Multiprocessor SM provides the following improvements over Turing and NVIDIA Ampere GPU architectures. 1.4.1.1. Occupancy The maximum number of concurrent warps per SM is 48, remaining the same compared to compute capability 8.6 GPUs, and other factors influencing warp occupancy are The register file size is 64K 32bit registers per SM. The maximum number of registers per thread is 255. The maximum number of thread blocks per SM is 24. The shared memory capacity per SM is 100 KB. The maximum shared memory per thread block is 99 KB. Overall, developers can expect similar occupancy as on compute capability 8.6 GPUs without changes to their application. 1.4.1.2. Improved Tensor Core Operations The NVIDIA Ada GPU architecture includes new Ada Fourth Generation Tensor Cores featuring the Hopper FP8 Transformer Engine. 1.4.1.3. Improved FP32 throughput Devices of compute capability 8.9 have 2x more FP32 operations per cycle per SM than devices of compute capability 8.0. While a binary compiled for 8.0 will run asis on 8.9, it is recommended to compile explicitly for 8.9 to benefit from the increased FP32 throughput. 1.4.2. Memory System 1.4.2.1. Increased L2 capacity The NVIDIA Ada GPU architecture increases the capacity of the L2 cache to 98304 KB in AD102, 16x larger than GA102. The NVIDIA Ada GPU architecture allows CUDA users to control the persistence of data in the L2 cache. For more information on the persistence of data in the L2 cache, refer to the section on managing the L2 cache in the CUDA C Programming Guide . 1.4.2.2. Unified Shared MemoryL1Texture Cache NVIDIA Ada architecture features a unified L1 cache, texture cache, and shared memory similar to that of the NVIDIA Ampere architecture. The combined L1 cache capacity is 128 KB. In the NVIDIA Ada GPU architecture, the portion of the L1 cache dedicated to shared memory known as the carveout can be selected at runtime as in previous architectures, such as NVIDIA Ampere, using cudaFuncSetAttribute with the attribute cudaFuncAttributePreferredSharedMemoryCarveout . The NVIDIA Ada GPU architecture supports shared memory capacity of 0, 8, 16, 32, 64 or 100 KB per SM. CUDA reserves 1 KB of shared memory per thread block. Hence, GPUs with compute capability 8.9 can address up to 99 KB of shared memory in a single thread block. To maintain architectural compatibility, static shared memory allocations remain limited to 48 KB, and an explicit optin is also required to enable dynamic allocations above this limit. See the CUDA C Programming Guide for details. Like the NVIDIA Ampere and NVIDIA Volta GPU architectures, the NVIDIA Ada GPU architecture combines the functionality of the L1 and texture caches into a unified L1Texture cache that acts as a coalescing buffer for memory accesses, gathering up the data requested by the threads of a warp prior to delivery of that data to the warp. Another benefit of its union with shared memory, similar to previous architectures, is improvement in terms of both latency and bandwidth. 2. Revision History Version 1.0 Initial Public Release Added support for compute capability 8.9 1 Throughout this guide, Volta refers to devices of compute capability 7.0, Turing refers to devices of compute capability 7.5, NVIDIA Ampere GPU Architecture refers to devices of compute capability 8.0 and 8.6, NVIDIA Ada refers to devices of compute capability 8.9. 3. Notices 3.1. Notice This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation NVIDIA makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material defined below, code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant"
Miscellaneous,PTX ISA,https://docs.nvidia.com/cuda/parallel-thread-execution/index.html,"PTX ISA 1. Introduction v8.5 PDF Archive Parallel Thread Execution ISA Version 8.5 The programming guide to using PTX Parallel Thread Execution and ISA Instruction Set Architecture. 1. Introduction This document describes PTX, a lowlevel parallel thread execution virtual machine and instruction set architecture ISA. PTX exposes the GPU as a dataparallel computing device . 1.1. Scalable DataParallel Computing using GPUs Driven by the insatiable market demand for realtime, highdefinition 3D graphics, the programmable GPU has evolved into a highly parallel, multithreaded, manycore processor with tremendous computational horsepower and very high memory bandwidth. The GPU is especially wellsuited to address problems that can be expressed as dataparallel computations the same program is executed on many data elements in parallel with high arithmetic intensity the ratio of arithmetic operations to memory operations. Because the same program is executed for each data element, there is a lower requirement for sophisticated flow control and because it is executed on many data elements and has high arithmetic intensity, the memory access latency can be hidden with calculations instead of big data caches. Dataparallel processing maps data elements to parallel processing threads. Many applications that process large data sets can use a dataparallel programming model to speed up the computations. In 3D rendering large sets of pixels and vertices are mapped to parallel threads. Similarly, image and media processing applications such as postprocessing of rendered images, video encoding and decoding, image scaling, stereo vision, and pattern recognition can map image blocks and pixels to parallel processing threads. In fact, many algorithms outside the field of image rendering and processing are accelerated by dataparallel processing, from general signal processing or physics simulation to computational finance or computational biology. PTX defines a virtual machine and ISA for general purpose parallel thread execution. PTX programs are translated at install time to the target hardware instruction set. The PTXtoGPU translator and driver enable NVIDIA GPUs to be used as programmable parallel computers. 1.2. Goals of PTX PTX provides a stable programming model and instruction set for general purpose parallel programming. It is designed to be efficient on NVIDIA GPUs supporting the computation features defined by the NVIDIA Tesla architecture. High level language compilers for languages such as CUDA and CC generate PTX instructions, which are optimized for and translated to native targetarchitecture instructions. The goals for PTX include the following Provide a stable ISA that spans multiple GPU generations. Achieve performance in compiled applications comparable to native GPU performance. Provide a machineindependent ISA for CC and other compilers to target. Provide a code distribution ISA for application and middleware developers. Provide a common sourcelevel ISA for optimizing code generators and translators, which map PTX to specific target machines. Facilitate handcoding of libraries, performance kernels, and architecture tests. Provide a scalable programming model that spans GPU sizes from a single unit to many parallel units. 1.3. PTX ISA Version 8.5 PTX ISA version 8.5 introduces the following new features Adds support for mma.sporderedmetadata instruction. 1.4. Document Structure The information in this document is organized into the following Chapters Programming Model outlines the programming model. PTX Machine Model gives an overview of the PTX virtual machine model. Syntax describes the basic syntax of the PTX language. State Spaces, Types, and Variables describes state spaces, types, and variable declarations. Instruction Operands describes instruction operands. Abstracting the ABI describes the function and call syntax, calling convention, and PTX support for abstracting the Application Binary Interface ABI . Instruction Set describes the instruction set. Special Registers lists special registers. Directives lists the assembly directives supported in PTX. Release Notes provides release notes for PTX ISA versions 2.x and beyond. References 7542008 IEEE Standard for FloatingPoint Arithmetic. ISBN 9780738157528, 2008. httpieeexplore.ieee.orgservletopac?punumber4610933 The OpenCL Specification, Version 1.1, Document Revision 44, June 1, 2011. httpwww.khronos.orgregistryclspecsopencl1.1.pdf CUDA Programming Guide. httpsdocs.nvidia.comcudacudacprogrammingguideindex.html CUDA Dynamic Parallelism Programming Guide. httpsdocs.nvidia.comcudacudacprogrammingguideindex.htmlcudadynamicparallelism CUDA Atomicity Requirements. httpsnvidia.github.iocccllibcudacxxextendedapimemorymodel.htmlatomicity PTX Writers Guide to Interoperability. httpsdocs.nvidia.comcudaptxwritersguidetointeroperabilityindex.html 2. Programming Model 2.1. A Highly Multithreaded Coprocessor The GPU is a compute device capable of executing a very large number of threads in parallel. It operates as a coprocessor to the main CPU, or host In other words, dataparallel, computeintensive portions of applications running on the host are offloaded onto the device. More precisely, a portion of an application that is executed many times, but independently on different data, can be isolated into a kernel function that is executed on the GPU as many different threads. To that effect, such a function is compiled to the PTX instruction set and the resulting kernel is translated at install time to the target GPU instruction set. 2.2. Thread Hierarchy The batch of threads that executes a kernel is organized as a grid. A grid consists of either cooperative thread arrays or clusters of cooperative thread arrays as described in this section and illustrated in Figure 1 and Figure 2 . Cooperative thread arrays CTAs implement CUDA thread blocks and clusters implement CUDA thread block clusters. 2.2.1. Cooperative Thread Arrays The Parallel Thread Execution PTX programming model is explicitly parallel a PTX program specifies the execution of a given thread of a parallel thread array. A cooperative thread array , or CTA, is an array of threads that execute a kernel concurrently or in parallel. Threads within a CTA can communicate with each other. To coordinate the communication of the threads within the CTA, one can specify synchronization points where threads wait until all threads in the CTA have arrived. Each thread has a unique thread identifier within the CTA. Programs use a data parallel decomposition to partition inputs, work, and results across the threads of the CTA. Each CTA thread uses its thread identifier to determine its assigned role, assign specific input and output positions, compute addresses, and select work to perform. The thread identifier is a threeelement vector tid , with elements tid.x , tid.y , and tid.z that specifies the threads position within a 1D, 2D, or 3D CTA. Each thread identifier component ranges from zero up to the number of"
Miscellaneous,PTX Interoperability,https://docs.nvidia.com/cuda/ptx-writers-guide-to-interoperability/index.html,"PTX Interoperability 1. Introduction v12.5 PDF Archive PTX Writers Guide to Interoperability The guide to writing ABIcompliant PTX. 1. Introduction This document defines the Application Binary Interface ABI for the CUDA architecture when generating PTX. By following the ABI, external developers can generate compliant PTX code that can be linked with other code. PTX is a lowlevel parallelthreadexecution virtual machine and ISA Instruction Set Architecture. PTX can be output from multiple tools or written directly by developers. PTX is meant to be GPUarchitecture independent, so that the same code can be reused for different GPU architectures. For more information on PTX, refer to the latest version of the PTX ISA reference document . There are multiple CUDA architecture families, each with their own ISA e.g. SM 5.x is the Maxwell family, SM 6.x is the Pascal family. This document describes the highlevel ABI for all architectures. Programs conforming to an ABI are expected to be executed on the appropriate architecture GPU, and can assume that instructions from that ISA are available. 2. Data Representation 2.1. Fundamental Types The below table shows the native scalar PTX types that are supported. Any PTX producer must use these sizes and alignments in order for its PTX to be compatible with PTX generated by other producers. PTX also supports native vector types, which are discussed in Aggregates and Unions . The sizes of types are defined by the host. For example, pointer size and long int size are dictated by the hosts ABI. PTX has an .addresssize directive that specifies the address size used throughout the PTX code. The size of pointers is 32 bits on a 32bit host or 64 bits on a 64bit host. However, addresses of the local and shared memory spaces are always 32 bits in size. During separate compilation we store info about the host platform in each object file. The linker will fail to link object files generated for incompatible host platforms. PTX Type Size bytes Align bytes Hardware Representation .b8 1 1 untyped byte .b16 2 2 untyped halfword .b32 4 4 untyped word .b64 8 8 untyped doubleword .s8 1 1 signed integral byte .s16 2 2 signed integral halfword .s32 4 4 signed integral word .s64 8 8 signed integral doubleword .u8 1 1 unsigned integral byte .u16 2 2 unsigned integral halfword .u32 4 4 unsigned integral word .u64 8 8 unsigned integral doubleword .f16 2 2 IEEE half precision .f32 4 4 IEEE single precision .f64 8 8 IEEE double precision 2.2. Aggregates and Unions Beyond the scalar types, PTX also supports nativevector types of these scalar types, with both its vector syntax and its bytearray syntax. For scalar types with a size no greater than four bytes, vector types with 1, 2, 3, and 4 elements exist for all other types, only 1 and 2 element vector types exist. All aggregates and unions can be supported in PTX with its bytearray syntax. The following are the sizeandalignment rules for all aggregates and unions. For a nonnativevector type, an entire aggregate or union is aligned on the same boundary as its most strictly aligned member. This rule is not followed if the alignments are defined by the input language. For example, in OpenCL builtin vector data types have their alignment set to the size of the builtin data type in bytes. For a native vector type discussed at the start of this section the alignment is defined as follows. For the definitions below, the native vector has n elements and has an element type t. For a vector with an odd number of elements, its alignment is the same as its member alignoft. For a vector with an even number of elements, its alignment is set to number of elements times the alignment of its member nalignoft. Each member is assigned to the lowest available offset with the appropriate alignment. This may require internal padding, depending on the previous member. The size of an aggregate or union, if necessary, is increased to make it a multiple of the alignment of the aggregate or union. This may require tail padding, depending on the last member. 2.3. Bit Fields C structure and union definitions may have bit fields that define integral objects with a specified number of bits. Bit Field Type Width w Range signed char 1 to 8 2 w1 to 2 w1 1 unsigned char 1 to 8 0 to 2 w 1 signed short 1 to 16 2 w1 to 2 w1 1 unsigned short 1 to 16 0 to 2 w 1 signed int 1 to 32 2 w1 to 2 w1 1 unsigned int 1 to 32 0 to 2 w 1 signed long long 1 to 64 2 w1 to 2 w1 1 unsigned long long 1 to 64 0 to 2 w 1 Current GPUs only support littleendian memory, so the below assumes littleendian layout. The following are rules that apply to bit fields. Plain bit fields neither signed nor unsigned is specified are treated as signed. When no type is provided e.g., signed 6 is specified, the type defaults to int. Bit fields obey the same size and alignment rules as other structure and union members, with the following modifications. Bit fields are allocated in memory from right to left least to more significant for little endian. A bit field must entirely reside in a storage unit appropriate for its declared type. A bit field should never cross its unit boundary. Bit fields may share a storage unit with other structure and union members, including members that are not bit fields, as long as there is enough space within the storage unit. Unnamed bit fields do not affect the alignment of a structure or union. Zerolength bit fields force the alignment of the following member of a structure to the next alignment boundary corresponding to the bitfield type. An unnamed, zerolength bit field will not force the external alignment of the structure to that boundary. If an"
Miscellaneous,Inline PTX Assembly,https://docs.nvidia.com/cuda/inline-ptx-assembly/index.html,"Inline PTX Assembly in CUDA 1. Using Inline PTX Assembly in CUDA v12.5 PDF Archive Inline PTX Assembly in CUDA The reference guide for inlining PTX parallel thread execution assembly statements into CUDA. 1. Using Inline PTX Assembly in CUDA The NVIDIA CUDA programming environment provides a parallel thread execution PTX instruction set architecture ISA for using the GPU as a dataparallel computing device. For more information on the PTX ISA, refer to the latest version of the PTX ISA reference document . This application note describes how to inline PTX assembly language statements into CUDA code. 1.1. Assembler ASM Statements Assembler statements, asm , provide a way to insert arbitrary PTX code into your CUDA program. A simple example is asm membar.gl This inserts a PTX membar.gl into your generated PTX code at the point of the asm statement. 1.1.1. Parameters An asm statement becomes more complicated, and more useful, when we pass values in and out of the asm. The basic syntax is as follows asm templatestring constraint output constraint input where you can have multiple input or output operands separated by commas. The template string contains PTX instructions with references to the operands. Multiple PTX instructions can be given by separating them with semicolons. A simple example is as follows asm add.s32 0, 1, 2 r i r j , r k Each n in the template string is an index into the following list of operands, in text order. So 0 refers to the first operand, 1 to the second operand, and so on. Since the output operands are always listed ahead of the input operands, they are assigned the smallest indices. This example is conceptually equivalent to the following add . s32 i , j , k Note that the numbered references in the string can be in arbitrary order. The following is equivalent to the above example asm add.s32 0, 2, 1 r i r k , r j You can also repeat a reference, e.g. asm add.s32 0, 1, 1 r i r k is conceptually add . s32 i , k , k If there is no input operand, you can drop the final colon, e.g. asm mov.s32 0, 2 r i If there is no output operand, the colon separators are adjacent, e.g. asm mov.s32 r1, 0 r i If you want the in a ptx instruction, then you should escape it with double , e.g. asm mov.u32 0, clock r x The above was simplified to explain the ordering of the string references. In reality, the operand values are passed via whatever mechanism the constraint specifies. The full list of constraints will be explained later, but the r constraint refers to a 32bit integer register. So the earlier example asm statement asm add.s32 0, 1, 2 r i r j , r k produces the following code sequence in the output generated by the compiler ld . s32 r1 , j ld . s32 r2 , k add . s32 r3 , r1 , r2 st . s32 i , r3 This is where the distinction between input and output operands becomes important. The input operands are loaded into registers before the asm statement, then the result register is stored to the output operand. The modifier in r specifies that the register is written to. There is also available a modifier that specifies the register is both read and written, e.g. asm add.s32 0, 0, 1 r i r j Multiple instructions can be combined into a single asm statement basically, anything legal can be put into the asm string. Multiple instructions can be split across multiple lines by making use of CCs implicit string concatenation. Both C style line end comments and classical Cstyle comments can be interspersed with these strings. To generate readable output in the PTX intermediate file it is best practice to terminate each instruction string except the last one with nt. For example, a cube routine could be written as device int cube int x int y asm .reg .u32 t1 nt temp reg t1 mul.lo.u32 t1, 1, 1 nt t1 x x mul.lo.u32 0, t1, 1 y t1 x r y r x return y If an output operand is conditionally updated by the asm instructions, then the modifier should be used. There is an implicit use of the output operand in such a case. For example, device int cond int x int y 0 asm nt .reg .pred p nt setp.eq.s32 p, 1, 34 nt x 34? p mov.s32 0, 1 nt set y to 1 if true conceptually y x34?1y r y r x return y 1.1.2. Constraints There is a separate constraint letter for each PTX register type h . u16 reg r . u32 reg l . u64 reg f . f32 reg d . f64 reg Example asm cvt.f32.s64 0, 1 f x l y generates ld . s64 rd1 , y cvt . f32 . s64 f1 , rd1 st . f32 x , f1 The constraint n may be used for immediate integer operands with a known value. Example asm add.u32 0, 0, 1 r x n 42 generates add . u32 r1 , r1 , 42 The constraint C can be used for operand of type array of const char, where the array contents are known at compile time. It is intended to allow customization of PTX instruction modes based on compile time computation see examples. Here is the specification for the C constraint C constant expression The constantexpression is evaluated during compilation and shall generate the address of a variable V , where V has static storage duration . V has type array of const char. V is constantinitialized . If V is a static class member, then V s initializing declaration is the declaration within the class. During translation, the compiler will replace a reference to the operand within the Assembler Template with the contents of V s initializer, except for the last trailing"
CUDA Runtime API,CUDA Runtime API,https://docs.nvidia.com/cuda/cuda-runtime-api/index.html,1. Difference between the driver and runtime APIs 2. API synchronization behavior 3. Stream synchronization behavior 4. Graph object thread safety 5. Rules for version mixing 6. Modules 6.1. Device Management 6.2. Device Management DEPRECATED 6.3. Thread Management DEPRECATED 6.4. Error Handling 6.5. Stream Management 6.6. Event Management 6.7. External Resource Interoperability 6.8. Execution Control 6.9. Execution Control DEPRECATED 6.10. Occupancy 6.11. Memory Management 6.12. Memory Management DEPRECATED 6.13. Stream Ordered Memory Allocator 6.14. Unified Addressing 6.15. Peer Device Memory Access 6.16. OpenGL Interoperability 6.17. OpenGL Interoperability DEPRECATED 6.18. Direct3D 9 Interoperability 6.19. Direct3D 9 Interoperability DEPRECATED 6.20. Direct3D 10 Interoperability 6.21. Direct3D 10 Interoperability DEPRECATED 6.22. Direct3D 11 Interoperability 6.23. Direct3D 11 Interoperability DEPRECATED 6.24. VDPAU Interoperability 6.25. EGL Interoperability 6.26. Graphics Interoperability 6.27. Texture Object Management 6.28. Surface Object Management 6.29. Version Management 6.30. Graph Management 6.31. Driver Entry Point Access 6.32. C API Routines 6.33. Interactions with the CUDA Driver API 6.34. Profiler Control 6.35. Data types used by CUDA Runtime 7. Data Structures 7.1. cudaOccupancyB2DHelper 7.2. cudaAccessPolicyWindow 7.3. cudaArrayMemoryRequirements 7.4. cudaArraySparseProperties 7.5. cudaAsyncNotificationInfot 7.6. cudaChannelFormatDesc 7.7. cudaChildGraphNodeParams 7.8. cudaConditionalNodeParams 7.9. cudaDeviceProp 7.10. cudaEglFrame 7.11. cudaEglPlaneDesc 7.12. cudaEventRecordNodeParams 7.13. cudaEventWaitNodeParams 7.14. cudaExtent 7.15. cudaExternalMemoryBufferDesc 7.16. cudaExternalMemoryHandleDesc 7.17. cudaExternalMemoryMipmappedArrayDesc 7.18. cudaExternalSemaphoreHandleDesc 7.19. cudaExternalSemaphoreSignalNodeParams 7.20. cudaExternalSemaphoreSignalNodeParamsV2 7.21. cudaExternalSemaphoreSignalParams 7.22. cudaExternalSemaphoreSignalParamsv1 7.23. cudaExternalSemaphoreWaitNodeParams 7.24. cudaExternalSemaphoreWaitNodeParamsV2 7.25. cudaExternalSemaphoreWaitParams 7.26. cudaExternalSemaphoreWaitParamsv1 7.27. cudaFuncAttributes 7.28. cudaGraphEdgeData 7.29. cudaGraphExecUpdateResultInfo 7.30. cudaGraphInstantiateParams 7.31. cudaGraphKernelNodeUpdate 7.32. cudaGraphNodeParams 7.33. cudaHostNodeParams 7.34. cudaHostNodeParamsV2 7.35. cudaIpcEventHandlet 7.36. cudaIpcMemHandlet 7.37. cudaKernelNodeParams 7.38. cudaKernelNodeParamsV2 7.39. cudaLaunchAttribute 7.40. cudaLaunchAttributeValue 7.41. cudaLaunchConfigt 7.42. cudaLaunchMemSyncDomainMap 7.43. cudaLaunchParams 7.44. cudaMemAccessDesc 7.45. cudaMemAllocNodeParams 7.46. cudaMemAllocNodeParamsV2 7.47. cudaMemcpy3DParms 7.48. cudaMemcpy3DPeerParms 7.49. cudaMemcpyNodeParams 7.50. cudaMemFreeNodeParams 7.51. cudaMemLocation 7.52. cudaMemPoolProps 7.53. cudaMemPoolPtrExportData 7.54. cudaMemsetParams 7.55. cudaMemsetParamsV2 7.56. cudaPitchedPtr 7.57. cudaPointerAttributes 7.58. cudaPos 7.59. cudaResourceDesc 7.60. cudaResourceViewDesc 7.61. cudaTextureDesc 7.62. CUuuidst 8. Data Fields 9. Deprecated List
CUDA Driver API,CUDA Driver API,https://docs.nvidia.com/cuda/cuda-driver-api/index.html,1. Difference between the driver and runtime APIs 2. API synchronization behavior 3. Stream synchronization behavior 4. Graph object thread safety 5. Rules for version mixing 6. Modules 6.1. Data types used by CUDA driver 6.2. Error Handling 6.3. Initialization 6.4. Version Management 6.5. Device Management 6.6. Device Management DEPRECATED 6.7. Primary Context Management 6.8. Context Management 6.9. Context Management DEPRECATED 6.10. Module Management 6.11. Module Management DEPRECATED 6.12. Library Management 6.13. Memory Management 6.14. Virtual Memory Management 6.15. Stream Ordered Memory Allocator 6.16. Multicast Object Management 6.17. Unified Addressing 6.18. Stream Management 6.19. Event Management 6.20. External Resource Interoperability 6.21. Stream Memory Operations 6.22. Execution Control 6.23. Execution Control DEPRECATED 6.24. Graph Management 6.25. Occupancy 6.26. Texture Reference Management DEPRECATED 6.27. Surface Reference Management DEPRECATED 6.28. Texture Object Management 6.29. Surface Object Management 6.30. Tensor Map Object Managment 6.31. Peer Context Memory Access 6.32. Graphics Interoperability 6.33. Driver Entry Point Access 6.34. Coredump Attributes Control API 6.35. Green Contexts 6.36. Profiler Control DEPRECATED 6.37. Profiler Control 6.38. OpenGL Interoperability 6.38.1. OpenGL Interoperability DEPRECATED 6.39. Direct3D 9 Interoperability 6.39.1. Direct3D 9 Interoperability DEPRECATED 6.40. Direct3D 10 Interoperability 6.40.1. Direct3D 10 Interoperability DEPRECATED 6.41. Direct3D 11 Interoperability 6.41.1. Direct3D 11 Interoperability DEPRECATED 6.42. VDPAU Interoperability 6.43. EGL Interoperability 7. Data Structures 7.1. CUaccessPolicyWindowv1 7.2. CUarrayMapInfov1 7.3. CUasyncNotificationInfo 7.4. CUctxCigParam 7.5. CUctxCreateParams 7.6. CUDAARRAY3DDESCRIPTORv2 7.7. CUDAARRAYDESCRIPTORv2 7.8. CUDAARRAYMEMORYREQUIREMENTSv1 7.9. CUDAARRAYSPARSEPROPERTIESv1 7.10. 7.11. CUDACHILDGRAPHNODEPARAMS 7.12. CUDACONDITIONALNODEPARAMS 7.13. CUDAEVENTRECORDNODEPARAMS 7.14. CUDAEVENTWAITNODEPARAMS 7.15. CUDAEXTSEMSIGNALNODEPARAMSv1 7.16. CUDAEXTSEMSIGNALNODEPARAMSv2 7.17. CUDAEXTSEMWAITNODEPARAMSv1 7.18. CUDAEXTSEMWAITNODEPARAMSv2 7.19. CUDAEXTERNALMEMORYBUFFERDESCv1 7.20. CUDAEXTERNALMEMORYHANDLEDESCv1 7.21. CUDAEXTERNALMEMORYMIPMAPPEDARRAYDESCv1 7.22. CUDAEXTERNALSEMAPHOREHANDLEDESCv1 7.23. CUDAEXTERNALSEMAPHORESIGNALPARAMSv1 7.24. CUDAEXTERNALSEMAPHOREWAITPARAMSv1 7.25. CUDAGRAPHINSTANTIATEPARAMS 7.26. CUDAHOSTNODEPARAMSv1 7.27. CUDAHOSTNODEPARAMSv2 7.28. CUDAKERNELNODEPARAMSv1 7.29. CUDAKERNELNODEPARAMSv2 7.30. CUDAKERNELNODEPARAMSv3 7.31. CUDALAUNCHPARAMSv1 7.32. CUDAMEMALLOCNODEPARAMSv1 7.33. CUDAMEMALLOCNODEPARAMSv2 7.34. CUDAMEMFREENODEPARAMS 7.35. CUDAMEMCPY2Dv2 7.36. CUDAMEMCPY3DPEERv1 7.37. CUDAMEMCPY3Dv2 7.38. CUDAMEMCPYNODEPARAMS 7.39. CUDAMEMSETNODEPARAMSv1 7.40. CUDAMEMSETNODEPARAMSv2 7.41. CUDAPOINTERATTRIBUTEP2PTOKENSv1 7.42. CUDARESOURCEDESCv1 7.43. CUDARESOURCEVIEWDESCv1 7.44. CUDATEXTUREDESCv1 7.45. CUdevpropv1 7.46. CUdevResource 7.47. CUdevSmResource 7.48. CUeglFramev1 7.49. CUexecAffinityParamv1 7.50. CUexecAffinitySmCountv1 7.51. CUgraphEdgeData 7.52. CUgraphExecUpdateResultInfov1 7.53. CUgraphNodeParams 7.54. CUipcEventHandlev1 7.55. CUipcMemHandlev1 7.56. CUlaunchAttribute 7.57. CUlaunchAttributeValue 7.58. CUlaunchConfig 7.59. CUlaunchMemSyncDomainMap 7.60. CUmemAccessDescv1 7.61. CUmemAllocationPropv1 7.62. CUmemFabricHandlev1 7.63. CUmemLocationv1 7.64. CUmemPoolPropsv1 7.65. CUmemPoolPtrExportDatav1 7.66. CUmulticastObjectPropv1 7.67. CUstreamBatchMemOpParamsv1 7.68. CUtensorMap 8. Data Fields 9. Deprecated List
CUDA Math API,CUDA Math API,https://docs.nvidia.com/cuda/cuda-math-api/index.html,"CUDA Math API Reference Manual CUDA Math API Reference Manual v12.5 PDF Archive CUDA Math API Reference Manual CUDA mathematical functions are always available in device code. Host implementations of the common mathematical functions are mapped in a platformspecific way to standard math library functions, provided by the host compiler and respective host libm where available. Some functions, not available with the host compilers, are implemented in crtmathfunctions.hpp header file. For example, see erfinv . Other, less common functions, like rhypot , cylbesseli0 are only available in device code. CUDA Math device functions are nothrow for wellformed CUDA programs. Note that many floatingpoint and integer functions names are overloaded for different argument types. For example, the log function has the following prototypes double log double x float log float x float logf float x Note also that due to implementation constraints, certain math functions from std namespace may be callable in device code even via explicitly qualified std names. However, such use is discouraged, since this capability is unsupported, unverified, undocumented, not portable, and may change without notice. 1. FP8 Intrinsics 2. Half Precision Intrinsics 3. Bfloat16 Precision Intrinsics 4. Single Precision Mathematical Functions 5. Single Precision Intrinsics 6. Double Precision Mathematical Functions 7. Double Precision Intrinsics 8. Type Casting Intrinsics 9. Integer Mathematical Functions 10. Integer Intrinsics 11. SIMD Intrinsics 12. Structs 13. Notices Privacy Policy Manage My Privacy Do Not Sell or Share My Data Terms of Service Accessibility Corporate Policies Product Security Contact Copyright 20072024, NVIDIA Corporation affiliates. All rights reserved. Last updated on Jul 1, 2024."
cuBLAS,cuBLAS,https://docs.nvidia.com/cuda/cublas/index.html,"cuBLAS 1. Introduction v12.5 PDF Archive cuBLAS The API Reference guide for cuBLAS, the CUDA Basic Linear Algebra Subroutine library. 1. Introduction The cuBLAS library is an implementation of BLAS Basic Linear Algebra Subprograms on top of the NVIDIACUDA runtime. It allows the user to access the computational resources of NVIDIA Graphics Processing Unit GPU. The cuBLAS Library exposes four sets of APIs The cuBLAS API , which is simply called cuBLAS API in this document starting with CUDA 6.0, The cuBLASXt API starting with CUDA 6.0, and The cuBLASLt API starting with CUDA 10.1 The cuBLASDx API not shipped with the CUDA Toolkit To use the cuBLAS API, the application must allocate the required matrices and vectors in the GPU memory space, fill them with data, call the sequence of desired cuBLAS functions, and then upload the results from the GPU memory space back to the host. The cuBLAS API also provides helper functions for writing and retrieving data from the GPU. To use the cuBLASXt API, the application may have the data on the Host or any of the devices involved in the computation, and the Library will take care of dispatching the operation to, and transferring the data to, one or multiple GPUs present in the system, depending on the user request. The cuBLASLt is a lightweight library dedicated to GEneral Matrixtomatrix Multiply GEMM operations with a new flexible API. This library adds flexibility in matrix data layouts, input types, compute types, and also in choosing the algorithmic implementations and heuristics through parameter programmability. After a set of options for the intended GEMM operation are identified by the user, these options can be used repeatedly for different inputs. This is analogous to how cuFFT and FFTW first create a plan and reuse for same size and type FFTs with different input data. 1.1. Data Layout For maximum compatibility with existing Fortran environments, the cuBLAS library uses columnmajor storage, and 1based indexing. Since C and C use rowmajor storage, applications written in these languages can not use the native array semantics for twodimensional arrays. Instead, macros or inline functions should be defined to implement matrices on top of onedimensional arrays. For Fortran code ported to C in mechanical fashion, one may chose to retain 1based indexing to avoid the need to transform loops. In this case, the array index of a matrix element in row i and column j can be computed via the following macro define IDX2Fi,j,ld j1ldi1 Here, ld refers to the leading dimension of the matrix, which in the case of columnmajor storage is the number of rows of the allocated matrix even if only a submatrix of it is being used. For natively written C and C code, one would most likely choose 0based indexing, in which case the array index of a matrix element in row i and column j can be computed via the following macro define IDX2Ci,j,ld jldi 1.2. New and Legacy cuBLAS API Starting with version 4.0, the cuBLAS Library provides a new API, in addition to the existing legacy API. This section discusses why a new API is provided, the advantages of using it, and the differences with the existing legacy API. Warning The legacy cuBLAS API is deprecated and will be removed in future release. The new cuBLAS library API can be used by including the header file cublasv2.h . It has the following features that the legacy cuBLAS API does not have The handle to the cuBLAS library context is initialized using the function and is explicitly passed to every subsequent library function call. This allows the user to have more control over the library setup when using multiple host threads and multiple GPUs. This also allows the cuBLAS APIs to be reentrant. The scalars alpha and beta can be passed by reference on the host or the device, instead of only being allowed to be passed by value on the host. This change allows library functions to execute asynchronously using streams even when alpha and beta are generated by a previous kernel. When a library routine returns a scalar result, it can be returned by reference on the host or the device, instead of only being allowed to be returned by value only on the host. This change allows library routines to be called asynchronously when the scalar result is generated and returned by reference on the device resulting in maximum parallelism. The error status cublasStatust is returned by all cuBLAS library function calls. This change facilitates debugging and simplifies software development. Note that cublasStatus was renamed cublasStatust to be more consistent with other types in the cuBLAS library. The cublasAlloc and cublasFree functions have been deprecated. This change removes these unnecessary wrappers around cudaMalloc and cudaFree , respectively. The function cublasSetKernelStream was renamed cublasSetStream to be more consistent with the other CUDA libraries. The legacy cuBLAS API, explained in more detail in Using the cuBLAS Legacy API , can be used by including the header file cublas.h . Since the legacy API is identical to the previously released cuBLAS library API, existing applications will work out of the box and automatically use this legacy API without any source code changes. The current and the legacy cuBLAS APIs cannot be used simultaneously in a single translation unit including both cublas.h and cublasv2.h header files will lead to compilation errors due to incompatible symbol redeclarations. In general, new applications should not use the legacy cuBLAS API, and existing applications should convert to using the new API if it requires sophisticated and optimal stream parallelism, or if it calls cuBLAS routines concurrently from multiple threads. For the rest of the document, the new cuBLAS Library API will simply be referred to as the cuBLAS Library API. As mentioned earlier the interfaces to the legacy and the cuBLAS library APIs are the header file cublas.h and cublasv2.h , respectively. In addition, applications using the cuBLAS library need to link against The DSO cublas.so for Linux, The DLL cublas.dll for"
Miscellaneous,cuDLA API,https://docs.nvidia.com/cuda/cudla-api/index.html,Here is a list of all modules Data types used by cuDLA driver cuDLA API
Miscellaneous,NVBLAS,https://docs.nvidia.com/cuda/nvblas/index.html,"NVBLAS 1. Introduction v12.5 PDF Archive NVBLAS The User guide for NVBLAS, dropin BLAS replacement, multiGPUs accelerated 1. Introduction The NVBLAS Library is a GPUaccelerated Libary that implements BLAS Basic Linear Algebra Subprograms. It can accelerate most BLAS Level3 routines by dynamically routing BLAS calls to one or more NVIDIA GPUs present in the system, when the charateristics of the call make it speed up on a GPU. 2. NVBLAS Overview The NVBLAS Library is built on top of the cuBLAS Library using only the CUBLASXT API refer to the CUBLASXT API section of the cuBLAS Documentation for more details. NVBLAS also requires the presence of a CPU BLAS lirbary on the system. Currently NVBLAS intercepts only compute intensive BLAS Level3 calls see table below. Depending on the charateristics of those BLAS calls, NVBLAS will redirect the calls to the GPUs present in the system or to CPU. That decision is based on a simple heuristic that estimates if the BLAS call will execute for long enough to amortize the PCI transfers of the input and output data to the GPU. Because NVBLAS does not support all standard BLAS routines, it might be necessary to associate it with an existing full BLAS Library. Please refer to the Usage section for more details. 3. GPU Accelerated Routines NVBLAS offloads only the computeintensive BLAS3 routines which have the best potential for acceleration on GPUs. The following table shows the currently supported routines Routine Types Operation gemm S,D,C,Z Multiplication of 2 matrices syrk S,D,C,Z Symmetric rankk update herk C,Z Hermitian rankk update syr2k S,D,C,Z Symmetric rank2k update her2k C,Z Hermitian rank2k update trsm S,D,C,Z Triangular solve with multiple righthand sides trmm S,D,C,Z Triangular matrixmatrix multiplication symm S,D,C,Z Symmetric matrixmatrix multiplication hemm C,Z Hermitian matrixmatrix multiplication 4. BLAS Symbols Interception Standard BLAS Library implementations usually expose multiple symbols for the same routines. Lets say func is a BLAS routine name, func orand func are usually defined as extern symbols. Some BLAS Libraries might also expose some symbols with a proprietary appended prefix. NVBLAS intercepts only the symbols func and func . The user needs to make sure that the application intended to be GPUaccelerated by NVBLAS actually calls those defined symbols. Any other symbols will not be intercepted and the original BLAS routine will be executed for those cases. 5. Device Memory Support Starting with Release 8.0, data can be located on any GPU device, even on GPU devices that are not configured to be part of the computation. When any of the data is located on a GPU, the computation will be exclusively done on GPU whatever the size of the problem. Also, this feature has to be used with caution the user has to be sure that the BLAS call will indeed be intercepted by NVBLAS, otherwise it will result in a crash when the CPU BLAS tries to execute it. 6. Security Precaution Because the NVBLAS Library relies on a symbols interception mechanism, it is essential to make sure it has not been compromised. In that regard, NVBLAS should never be used from a process running at elevated privileges, such as Administrator on Windows or root on Linux. 7. Configuration Because NVBLAS is a dropin replacement of BLAS, it must be configured through an ASCII text file that describes how many and which GPUs can participate in the intercepted BLAS calls. The configuration file is parsed at the time of the loading of the library. The format of the configuration file is based on keywords optionally followed by one or more userdefined parameters. At most one keyword per line is allowed. Blank lines or lines beginning with the character are ignored. 7.1. NVBLASCONFIGFILE Environment Variable The location and name of the configuration file must be defined by the environment variable NVBLASCONFIGFILE . By default, if NVBLASCONFIGFILE is not defined, NVBLAS will try to open the file nvblas.conf in the current directory. For a safe use of NVBLAS, the configuration file should have have restricted write permissions. 7.2. Configuration Keywords The configuration keywords syntax is described in the following subsections. 7.2.1. NVBLASLOGFILE This keyword defines the file where NVBLAS should print status and error messages. By default, if not defined, the standard error output file eg. stderr will be used. It is advised to define this keyword early in the configuration to capture errors in parsing that file itself. 7.2.2. NVBLASTRACELOGENABLED When this keyword is defined, every intercepted BLAS calls will be logged into the NVBLASLOGFILE. This feature, even though intrusive, can be useful for debugging purposes. 7.2.3. NVBLASCPUBLASLIB This keyword defines the CPU BLAS dynamic library file for example, .so file on Linux or .dll on Windows that NVBLAS should open to find the CPU BLAS symbols definitions. This keyword must be defined for NVBLAS to work. Because CPU Blas libraries are often composed of multiple files, even though this keyword is set to the full path to the main file of the CPU library, it might still be necessary to define the right path to find the rest of the library files in the environment of your system. On Linux, this can be done by setting the environment variable LDLIBRARYPATH whereas on Windows, this can be done by setting the environment variable PATH . For a safe use of NVBLAS, the following precautions are strongly advised The CPU BLAS Library should be located where ordinary users do not have write permissions. The path specified should be absolute, not relative. 7.2.4. NVBLASGPULIST This keyword defines the list of GPUs that should participate in the computation of the intercepted BLAS calls. If not defined, only GPU device 0 is used, since that is normally the most computecapable GPU installed in the system. This keyword can be set to a list of device numbers separated by blank characters. Also the following wildcard keywords are also accepted for simplicity Keyword Meaning ALL All computecapable GPUs detected on the system will be used by NVBLAS ALL0 GPU device 0, AND all others GPUs detected that"
Miscellaneous,nvJPEG,https://docs.nvidia.com/cuda/nvjpeg/index.html,"nvJPEG 1. Introduction v12.5 PDF Archive nvJPEG A GPU accelerated JPEG codec library. 1. Introduction 1.1. nvJPEG Decoder The nvJPEG library provides highperformance, GPU accelerated JPEG decoding functionality for image formats commonly used in deep learning and hyperscale multimedia applications. The library offers single and batched JPEG decoding capabilities which efficiently utilize the available GPU resources for optimum performance and the flexibility for users to manage the memory allocation needed for decoding. The nvJPEG library enables the following functions use the JPEG image data stream as input retrieve the width and height of the image from the data stream, and use this retrieved information to manage the GPU memory allocation and the decoding. A dedicated API is provided for retrieving the image information from the raw JPEG image data stream. Note Throughout this document, the terms CPU and Host are used synonymously. Similarly, the terms GPU and Device are synonymous. The nvJPEG library supports the following JPEG options Baseline and Progressive JPEG decodingencoding 8 bits per pixel Huffman bitstream decoding Upto 4 channel JPEG bitstreams 8 and 16bit quantization tables The following chroma subsampling for the 3 color channels Y, Cb, Cr Y, U, V 444 422 420 440 411 410 Features Hybrid decoding using both the CPU i.e., host and the GPU i.e., device. Hardware acceleration for baseline JPEG decode on supported platforms . Input to the library is in the host memory, and the output is in the GPU memory. Single image and batched image decoding. Single phase and multiple phases decoding. Color space conversion. Userprovided memory manager for the device and pinned host memory allocations. 1.2. nvJPEG Encoder The encoding functions of the nvJPEG library perform GPUaccelerated compression of users image data to the JPEG bitstream. User can provide input data in a number of formats and colorspaces, and control the encoding process with parameters. Encoding functionality will allocate temporary buffers using userprovided memory allocator. Before calling the encoding functions the user should perform a few prerequisite steps using the helper functions described in nvJPEG Encoder Helper API Reference . 1.3. Thread Safety Not all nvJPEG types are thread safe. When using decoder APIs across multiple threads, the following decoder types should be instantiated separately for each thread nvjpegJpegStreamt , nvjpegJpegStatet , nvjpegBufferDevicet , nvjpegBufferPinnedt When using encoder APIs across multiple threads, nvjpegEncoderStatet should be instantiated separately for each thread. For userprovided allocators inputs to nvJPEGCreateEx , the user needs to ensure thread safety. 1.4. MultiGPU support The nvJPEG states and handles are bound to the device that was set as current during their creation. Using these states and handles with another device set as current is undefined. The user is responsible of keeping track of the current device. 1.5. Hardware Acceleration Hardware accelerated JPEG decode is available on the following GPUs A100, A30, H100. Platforms which support hardware accelerated JPEG decode Windows Linux x8664, PowerPC, ARM64 2. JPEG Decoding 2.1. Using JPEG Decoding The nvJPEG library provides functions for both the decoding of a single image, and batched decoding of multiple images. 2.1.1. Single Image Decoding For singleimage decoding you provide the data size and a pointer to the file data, and the decoded image is placed in the output buffer. To use the nvJPEG library, start by calling the helper functions for initialization. Create nvJPEG library handle with one of the helper functions nvjpegCreateSimple or nvjpegCreateEx . Create JPEG state with the helper function nvjpegJpegStateCreate . See nvJPEG Type Declarations and nvjpegJpegStateCreate . The following helper functions are available in the nvJPEG library nvjpegStatust nvjpegGetPropertylibraryPropertyType type, int value DEPRECATED nvjpegStatust nvjpegCreatenvjpegBackendt backend, nvjpegHandlet handle , nvjpegdevallocator allocator nvjpegStatust nvjpegCreateSimplenvjpegHandlet handle nvjpegStatust nvjpegCreateExnvjpegBackendt backend, nvjpegDevAllocatort devallocator, nvjpegPinnedAllocatort pinnedallocator, unsigned int flags, nvjpegHandlet handle nvjpegStatust nvjpegDestroynvjpegHandlet handle nvjpegStatust nvjpegJpegStateCreatenvjpegHandlet handle, nvjpegJpegStatet jpeghandle nvjpegStatust nvjpegJpegStateDestroynvjpegJpegState handle Other helper functions such as nvjpegSet and nvjpegGet can be used to configure the library functionality on perhandle basis. Refer to the helper API reference for more details. Retrieve the width and height information from the JPEGencoded image by using the nvjpegGetImageInfo function. Below is the signature of nvjpegGetImageInfo function nvjpegStatust nvjpegGetImageInfo nvjpegHandlet handle , const unsigned char data , sizet length , int nComponents , nvjpegChromaSubsamplingt subsampling , int widths , int heights For each image to be decoded, pass the JPEG data pointer and data length to the above function. The nvjpegGetImageInfo function is thread safe. One of the outputs of the above nvjpegGetImageInfo function is nvjpegChromaSubsamplingt . This parameter is an enum type, and its enumerator list is composed of the chroma subsampling property retrieved from the JPEG image. See nvJPEG Chroma Subsampling . Use the nvjpegDecode function in the nvJPEG library to decode this single JPEG image. See the signature of this function below nvjpegStatust nvjpegDecode nvjpegHandlet handle , nvjpegJpegStatet jpeghandle , const unsigned char data , sizet length , nvjpegOutputFormatt outputformat , nvjpegImaget destination , cudaStreamt stream In the above nvjpegDecode function, the parameters nvjpegOutputFormatt , nvjpegImaget , and cudaStreamt can be used to set the output behavior of the nvjpegDecode function. You provide the cudaStreamt parameter to indicate the stream to which your asynchronous tasks are submitted. The nvjpegOutputFormatt parameter The nvjpegOutputFormatt parameter can be set to one of the outputformat settings below outputformat Meaning NVJPEGOUTPUTUNCHANGED Return the decoded image planar format. NVJPEGOUTPUTRGB Convert to planar RGB. NVJPEGOUTPUTBGR Convert to planar BGR. NVJPEGOUTPUTRGBI Convert to interleaved RGB. NVJPEGOUTPUTBGRI Convert to interleaved BGR. NVJPEGOUTPUTY Return the Y component only. NVJPEGOUTPUTYUV Return in the YUV planar format. NVJPEGOUTPUTUNCHANGEDIU16 Return the decoded image interleaved format. For example, if outputformat is set to NVJPEGOUTPUTY or NVJPEGOUTPUTRGBI , or NVJPEGOUTPUTBGRI then the output is written only to channel0 of nvjpegImaget , and the other channels are not touched. Alternately, in the case of planar output, the data is written to the corresponding channels of the nvjpegImaget destination structure. Finally, in the case of grayscale JPEG and RGB output, the luminance is used to create the grayscale RGB. The below table explains the combinations of the output formats and the number of"
cuFFT,cuFFT,https://docs.nvidia.com/cuda/cufft/index.html,"cuFFT 1. Introduction v12.5 PDF Archive cuFFT API Reference The API reference guide for cuFFT, the CUDA Fast Fourier Transform library. 1. Introduction This document describes cuFFT, the NVIDIA CUDA Fast Fourier Transform FFT product. It consists of two separate libraries cuFFT and cuFFTW. The cuFFT library is designed to provide high performance on NVIDIA GPUs. The cuFFTW library is provided as a porting tool to enable users of FFTW to start using NVIDIA GPUs with a minimum amount of effort. The FFT is a divideandconquer algorithm for efficiently computing discrete Fourier transforms of complex or realvalued data sets. It is one of the most important and widely used numerical algorithms in computational physics and general signal processing. The cuFFT library provides a simple interface for computing FFTs on an NVIDIA GPU, which allows users to quickly leverage the floatingpoint power and parallelism of the GPU in a highly optimized and tested FFT library. The cuFFT product supports a wide range of FFT inputs and options efficiently on NVIDIA GPUs. This version of the cuFFT library supports the following features Algorithms highly optimized for input sizes that can be written in the form 2a times 3b times 5c times 7d . In general the smaller the prime factor, the better the performance, i.e., powers of two are fastest. An Oleft nlog n right algorithm for every input data size Halfprecision 16bit floating point, singleprecision 32bit floating point and doubleprecision 64bit floating point. Transforms of lower precision have higher performance. Complex and realvalued input and output. Real valued input or output require less computations and data than complex values and often have faster time to solution. Types supported are C2C Complex input to complex output R2C Real input to complex output C2R Symmetric complex input to real output 1D, 2D and 3D transforms Execution of multiple 1D, 2D and 3D transforms simultaneously. These batched transforms have higher performance than single transforms. Inplace and outofplace transforms Arbitrary intra and interdimension element strides strided layout FFTW compatible data layout Execution of transforms across multiple GPUs Streamed execution, enabling asynchronous computation and data movement The cuFFTW library provides the FFTW3 API to facilitate porting of existing FFTW applications. Please note that starting from CUDA 11.0, the minimum supported GPU architecture is SM35. See Deprecated Functionality . 2. Using the cuFFT API This chapter provides a general overview of the cuFFT library API. For more complete information on specific functions, see cuFFT API Reference . Users are encouraged to read this chapter before continuing with more detailed descriptions. The Discrete Fourier transform DFT maps a complexvalued vector xk time domain into its frequency domain representation given by Xk sumlimitsn 0N 1xne2pi ifracknN where Xk is a complexvalued vector of the same size. This is known as a forward DFT. If the sign on the exponent of e is changed to be positive, the transform is an inverse transform. Depending on N , different algorithms are deployed for the best performance. The cuFFT API is modeled after FFTW , which is one of the most popular and efficient CPUbased FFT libraries. cuFFT provides a simple configuration mechanism called a plan that uses internal building blocks to optimize the transform for the given configuration and the particular GPU hardware selected. Then, when the execution function is called, the actual transform takes place following the plan of execution. The advantage of this approach is that once the user creates a plan, the library retains whatever state is needed to execute the plan multiple times without recalculation of the configuration. This model works well for cuFFT because different kinds of FFTs require different thread configurations and GPU resources, and the plan interface provides a simple way of reusing configurations. Computing a number BATCH of onedimensional DFTs of size NX using cuFFT will typically look like this define NX 256 define BATCH 10 define RANK 1 ... cufftHandle plan cufftComplex data ... cudaMalloc void data , sizeof cufftComplex NX BATCH cufftPlanMany plan , RANK , NX , iembed , istride , idist , oembed , ostride , odist , CUFFTC2C , BATCH ... cufftExecC2C plan , data , data , CUFFTFORWARD cudaDeviceSynchronize ... cufftDestroy plan cudaFree data 2.1. Accessing cuFFT The cuFFT and cuFFTW libraries are available as shared libraries. They consist of compiled programs ready for users to incorporate into applications with the compiler and linker. cuFFT can be downloaded from httpsdeveloper.nvidia.comcufft . By selecting Download CUDA Production Release users are all able to install the package containing the CUDA Toolkit, SDK code samples and development drivers. The CUDA Toolkit contains cuFFT and the samples include simplecuFFT . The Linux release for simplecuFFT assumes that the root install directory is usrlocalcuda and that the locations of the products are contained there as follows. Modify the Makefile as appropriate for your system. Product Location and name Include file nvcc compiler binnvcc cuFFT library lib, lib64libcufft.so inccufft.h cuFFT library with Xt functionality lib, lib64libcufft.so inccufftXt.h cuFFTW library lib, lib64libcufftw.so inccufftw.h The most common case is for developers to modify an existing CUDA routine for example, filename.cu to call cuFFT routines. In this case the include file cufft.h or cufftXt.h should be inserted into filename.cu file and the library included in the link line. A single compile and link line might appear as usrlocalcudabinnvcc options filename.cu Iusrlocalcudainc Lusrlocalcudalib lcufft Of course there will typically be many compile lines and the compiler g may be used for linking so long as the library path is set correctly. Users of the FFTW interface see FFTW Interface to cuFFT should include cufftw.h and link with both cuFFT and cuFFTW libraries. Functions in the cuFFT and cuFFTW library assume that the data is in GPU visible memory. This means any memory allocated by cudaMalloc , cudaMallocHost and cudaMallocManaged or registered with cudaHostRegister can be used as input, output or plan work area with cuFFT and cuFFTW functions. For the best performance input data, output data and plan work area should reside in device memory. cuFFTW library"
cuRAND,cuRAND,https://docs.nvidia.com/cuda/curand/index.html,"The API reference guide for cuRAND, the CUDA random number generation library."
cuSPARSE,cuSPARSE,https://docs.nvidia.com/cuda/cusparse/index.html,"cuSPARSE 1. Introduction v12.5 PDF Archive cuSPARSE The API reference guide for cuSPARSE, the CUDA sparse matrix library. 1. Introduction The cuSPARSE library contains a set of GPUaccelerated basic linear algebra subroutines used for handling sparse matrices that perform significantly faster than CPUonly alternatives. Depending on the specific operation, the library targets matrices with sparsity ratios in the range between 7099.9. It is implemented on top of the NVIDIA CUDA runtime which is part of the CUDA Toolkit and is designed to be called from C and C. see also cuSPARSELt A HighPerformance CUDA Library for Sparse MatrixMatrix Multiplication cuSPARSE Release Notes cudatoolkitreleasenotes cuSPARSE GitHub Samples CUDALibrarySamples Nvidia Developer Forum GPUAccelerated Libraries Provide Feedback MathLibsFeedback nvidia . com Recent cuSPARSEcuSPARSELt Blog Posts and GTC presentations Exploiting NVIDIA Ampere Structured Sparsity with cuSPARSELt Accelerating Matrix Multiplication with Block Sparse Format and NVIDIA Tensor Cores JustInTime LinkTime Optimization Adoption in cuSPARSEcuFFT Use Case Overview Structured Sparsity in the NVIDIA Ampere Architecture and Applications in Search Engines Making the Most of Structured Sparsity in the NVIDIA Ampere Architecture The library routines provide the following functionalities Operations between a sparse vector and a dense vector sum, dot product, scatter, gather Operations between a dense matrix and a sparse vector multiplication Operations between a sparse matrix and a dense vector multiplication, triangular solver, tridiagonal solver, pentadiagonal solver Operations between a sparse matrix and a dense matrix multiplication, triangular solver, tridiagonal solver, pentadiagonal solver Operations between a sparse matrix and a sparse matrix sum, multiplication Operations between dense matrices with output a sparse matrix multiplication Sparse matrix preconditioners Incomplete Cholesky Factorization level 0, Incomplete LU Factorization level 0 Reordering and Conversion operations between different sparse matrix storage formats 1.1. Library Organization and Features The cuSPARSE library is organized in two set of APIs The Legacy APIs , inspired by the Sparse BLAS standard, provide a limited set of functionalities and will not be improved in future releases , even if standard maintenance is still ensured. Some routines in this category could be deprecated and removed in the shortterm. A replacement will be provided for the most important of them during the deprecation process. The Generic APIs provide the standard interface layer of cuSPARSE . They allow computing the most common sparse linear algebra operations, such as sparse matrixvector SpMV and sparse matrixmatrix multiplication SpMM, in a flexible way. The new APIs have the following capabilities and features Set matrix data layouts , number of batches , and storage formats for example, CSR, COO, and so on. Set inputoutputcompute data types. This also allows mixed datatype computation . Set types of sparse vectormatrix indices e.g. 32bit, 64bit. Choose the algorithm for the computation. Guarantee external device memory for internal operations. Provide extensive consistency checks across input matrices and vectors. This includes the validation of sizes, data types, layout, allowed operations, etc. Provide constant descriptors for vector and matrix inputs to support constsafe interface and guarantee that the APIs do not modify their inputs. 1.2. Static Library Support Starting with CUDA 6.5, the cuSPARSE library is also delivered in a static form as libcusparsestatic.a on Linux. For example, to compile a small application using cuSPARSE against the dynamic library , the following command can be used nvcc mycusparseapp . cu lcusparse o mycusparseapp Whereas to compile against the static library , the following command has to be used nvcc mycusparseapp . cu lcusparsestatic o mycusparseapp It is also possible to use the native Host C compiler. Depending on the Host Operating system, some additional libraries like pthread or dl might be needed on the linking line. The following command on Linux is suggested gcc mycusparseapp . c lcusparsestatic lcudartstatic lpthread ldl I cuda toolkit path include L cuda toolkit path lib64 o mycusparseapp Note that in the latter case, the library cuda is not needed. The CUDA Runtime will try to open explicitly the cuda library if needed. In the case of a system which does not have the CUDA driver installed, this allows the application to gracefully manage this issue and potentially run if a CPUonly path is available. 1.3. Library Dependencies Starting with CUDA 12.0, cuSPARSE will depend on nvJitLink library for JIT JustInTime LTO LinkTimeOptimization capabilities refer to the cusparseSpMMOp APIs for more information. If the user links to the dynamic library , the environment variables for loading the libraries at runtime such as LDLIBRARYPATH on Linux and PATH on Windows must include the path where libnvjitlink.so is located. If it is in the same directory as cuSPARSE, the user doesnt need to take any action. If linking to the static library , the user needs to link with lnvjitlink and set the environment variables for loading the libraries at compiletime LIBRARYPATHPATH accordingly. 2. Using the cuSPARSE API This chapter describes how to use the cuSPARSE library API. It is not a reference for the cuSPARSE API data types and functions that is provided in subsequent chapters. 2.1. APIs Usage Notes The cuSPARSE library allows developers to access the computational resources of the NVIDIA graphics processing unit GPU. The cuSPARSE APIs assume that input and output data vectors and matrices reside in GPU device memory . The input and output scalars e.g. alpha and beta can be passed by reference on the host or the device, instead of only being allowed to be passed by value on the host. This allows library functions to execute asynchronously using streams even when they are generated by a previous kernel resulting in maximum parallelism. The handle to the cuSPARSE library context is initialized using the function and is explicitly passed to every subsequent library function call. This allows the user to have more control over the library setup when using multiple host threads and multiple GPUs. The error status cusparseStatust is returned by all cuSPARSE library function calls. It is the responsibility of the developer to allocate memory and to copy data between GPU memory and CPU memory using standard CUDA runtime API routines, such as cudaMalloc ,"
Miscellaneous,NPP,https://docs.nvidia.com/cuda/npp/index.html,"npp NVIDIA 2D Image and Signal Processing Performance Primitives NPP v23.05 PDF Archive NVIDIA 2D Image and Signal Processing Performance Primitives NPP Indices and Search Index Search Page Contents What is NPP ? Files Header Files Library Files Library Organization Supported NVIDIA Hardware General Conventions Memory Management Scratch Buffer and Host Pointer Function Naming Integer Result Scaling Rounding Modes Rounding Mode Parameter Image Processing Conventions Function Naming Image Data Line Step Parameter Names for Image Data Passing SourceImage Data SourceImage Pointer SourceBatchImages Pointer SourcePlanarImage Pointer Array SourcePlanarImage Pointer SourceImage Line Step SourcePlanarImage Line Step Array SourcePlanarImage Line Step Passing DestinationImage Data DestinationImage Pointer DestinationBatchImages Pointer DestinationPlanarImage Pointer Array DestinationPlanarImage Pointer DestinationImage Line Step DestinationPlanarImage Line Step Passing InPlace Image Data InPlace Image Pointer InPlaceImage Line Step Passing MaskImage Data MaskImage Pointer MaskImage Line Step Passing ChannelofInterest Data ChannelofInterest Number Image Data Alignment Requirements Image Data Related Error Codes RegionOfInterest ROI ROI Related Error Codes Masked Operation ChannelofInterest API SelectChannel SourceImage Pointer SelectChannel SourceImage SelectChannel DestinationImage Pointer SourceImage Sampling PointWise Operations Neighborhood Operations MaskSize Parameter AnchorPoint Parameter Sampling Beyond Image Boundaries Signal Processing Conventions Signal Data Parameter Names for Signal Data Source Signal Pointer Destination Signal Pointer InPlace Signal Pointer Signal Data Alignment Requirements Signal Data Related Error Codes Signal Length Length Related Error Codes Data Types, Structs, Enums, and Constants Image Arithmetic And Logical Operations Arithmetic Operations Arithmetic Operations AddC MulC MulCScale SubC DivC AbsDiffC Add AddSquare AddProduct AddWeighted Mul MulScale Sub Div DivRound Abs AbsDiff Sqr Sqrt Ln Exp Logical Operations Logical Operations AndC OrC XorC RShiftC LShiftC And Or Xor Not Image Alpha Composition Operations AlphaCompC AlphaComp Image Color Conversion Functions Color Processing Functions Color To Gray Conversion Color Debayer Color Gamma Correction Complement Color Key ColorTwist ColorTwistBatch ColorLUT ColorLUTLinear ColorLUTCubic ColorLUTTrilinear ColorLUTPalette Color Sampling Format Conversion Functions YCbCr420ToYCbCr411 YCbCr422ToYCbCr422 YCbCr422ToYCrCb422 YCbCr422ToCbYCr422 CbYCr422ToYCbCr411 YCbCr422ToYCbCr420 YCrCb420ToYCbCr422 YCbCr422ToYCrCb420 YCbCr422ToYCbCr411 YCrCb422ToYCbCr422 YCrCb422ToYCbCr420 YCrCb422ToYCbCr411 CbYCr422ToYCbCr422 CbYCr422ToYCbCr420 CbYCr422ToYCrCb420 YCbCr420ToYCbCr420 YCbCr420ToYCbCr422 YCbCr420ToCbYCr422 YCbCr420ToYCrCb420 YCrCb420ToCbYCr422 YCrCb420ToYCbYCr420 YCrCb420ToYCbYCr411 YCbCr411ToYCbCr411 YCbCr411ToYCbCr422 YCbCr411ToYCrCb422 YCbCr411ToYCbCr420 YCbCr411ToYCrCb420 NV12ToYUV420 Color Model Conversion Functions RGBToYUV BGRToYUV YUVToRGB YUVToRGBBatch YUVToRGBBatchAdvanced YUVToBGR YUVToBGRBatch YUVToBGRBatchAdvanced RGBToYUV422 YUV422ToRGB YUV422ToRGBBatch YUV422ToRGBBatchAdvanced YUV422ToBGRBatch YUV422ToBGRBatchAdvanced RGBToYUV420 YUV420ToRGB YUV420ToRGBBatch YUV420ToRGBBatchAdvanced NV12ToRGB NV21ToRGB BGRToYUV420 YUV420ToBGR YUV420ToBGRBatch YUV420ToBGRBatchAdvanced NV12ToBGR NV21ToBGR RGBToYCbCr YCbCrToRGB YCbCrToRGBBatch YCbCrToRGBBatchAdvanced YCbCrToBGR YCbCrToBGRBatch YCbCrToBGRBatchAdvanced YCbCrToBGR709CSC RGBToYCbCr422 YCbCr422ToRGB YCbCr422ToRGBBatch YCbCr422ToRGBBatchAdvanced RGBToYCrCb422 YCrCb422ToRGB YCbCr422ToBGR YCbCr422ToBGRBatch YCbCr422ToBGRBatchAdvanced RGBToCbYCr422 CbYCr422ToRGB BGRToCbYCr422 BGRToCbYCr422 709HDTV CbYCr422ToBGR CbYCr422ToBGR 709HDTV RGBToYCbCr420 YCbCr420ToRGB YCbCr420ToRGBBatch YCbCr420ToRGBBatchAdvanced RGBToYCrCb420 YCrCb420ToRGB BGRToYCbCr420 BGRToYCbCr420 709CSC BGRToYCbCr420 709HDTV BGRToYCrCb420 709CSC YCbCr420ToBGR YCbCr420ToBGRBatch YCbCr420ToBGRBatchAdvanced YCbCr420ToBGR 709CSC YCbCr420ToBGR 709HDTV BGRToYCrCb420 BGRToYCbCr411 BGRToYCbCr YCbCr411ToBGR YCbCr411ToRGB RGBToXYZ XYZToRGB RGBToLUV LUVToRGB BGRToLab LabToBGR RGBToYCC YCCToRGB YCCKToCMYKJPEG CMYKOrYCCKJPEGToRGB YCCKJPEGOrCMYKToBGR RGBToHLS HLSToRGB BGRToHLS HLSToBGR RBGToHSV HSVToRGB JPEG Color Conversion Image Data Exchange And Initialization Functions Set Common parameters for nppiSet functions Masked Set Common parameters for nppiSetCXM functions Channel Set Common parameters for nppiSetCXC functions Copy Common parameters for nppiCopy functions Masked Copy Common parameters for nppiCopyCXM functions Channel Copy Common parameters for nppiCopyCXC functions Extract Channel Copy Common parameters for nppiCopyCXC1 functions Insert Channel Copy Common parameters for nppiCopyC1CX functions Packed To Planar Channel Copy Common parameters for nppiCopyCXPX functions Planar To Packed Channel Copy Common parameters for nppiCopyPXCX functions Copy Constant Border Common parameters for nppiCopyConstBorder functions Copy Replicate Border Common parameters for nppiCopyReplicateBorder functions Copy Wrap Border Common parameters for nppiCopyWrapBorder functions Copy SubPixel Common parameters for nppiCopySubPix functions Convert Bit Depth Convert To Increased Bit Depth Common parameters for nppiConvert to increased bit depth functions Convert To Decreased Bit Depth Common parameters for nppiConvert to decreased bit depth functions Scale Bit Depth Scale To Higher Bit Depth Common parameters for nppiScale to higher bit depth functions Scale To Lower Bit Depth Common parameters for nppiScale to lower bit depth functions Duplicate Channel Common parameters for nppiDup functions Transpose Common parameters for nppiTranspose functions Swap Channels Image Filtering Functions Image 1D Linear Filters 1DLinearFilter Image Filter Column FilterColumn Image Filter Column Border FilterColumnBorder Image Filter Column 32f FilterColumn32f Image Filter Column Border 32f FilterColumnBorder32f Image Filter Row FilterRow Image Filter Row Border FilterRowBorder Image Filter Row 32f FilterRow32f Image Filter Row Border 32f FilterRowBorder32f Image Filter 1D Window Sum 1D Window Sum Image Filter 1D Window Column Sum 1D Window Column Sum Image Filter 1D Window Row Sum 1D Window Row Sum Image Filter 1D Window Sum Border 1D Window Sum with Border Control Image Filter 1D Window Column Sum Border 1D Window Column Sum Border Image Filter 1D Window Row Sum Border 1D Window Row Sum Border Image Convolution Convolution Image Filter Filter Image Filter 32f Filter32f Image Filter Border FilterBorder Image Filter Border 32f FilterBorder32f 2D Fixed Linear Filters 2D Fixed Linear Filters Image Filter Box FilterBox Image Filter Box Border FilterBoxBorder Image Filter Box Border Advanced FilterBoxBorderAdvanced Image Filter Threshold Adaptive Box Border FilterThresholdAdaptiveBoxBorder Rank Filters Rank Filters Image Filter Max FilterMax Image Filter Max Border FilterMaxBorder Image Filter Min FilterMin Image Filter Min Border FilterMinBorder Image Filter Median FilterMedian Image Filter Median Border FilterMedianBorder Fixed Filters Fixed Filters Image Filter Prewitt FilterPrewitt Image Filter Prewitt Border FilterPrewittBorder Image Filter Scharr FilterScharr Image Filter Scharr Border FilterScharrBorder Image Filter Sobel FilterSobel Image Filter Sobel Border FilterSobelBorder Image Filter Roberts FilterRoberts Image Filter Roberts Border FilterRobertsBorder Image Filter Laplace FilterLaplace Image Filter Laplace Border FilterLaplaceBorder Image Filter Gauss FilterGauss Image Filter Gauss Advanced FilterGaussAdvanced Image Filter Gauss Border FilterGaussBorder Image Filter Advanced Gauss Border FilterGaussAdvancedBorder Image Filter Gauss Pyramid Layer Down Border FilterGaussPyramidLayerDownBorder Image Filter Gauss Pyramid Layer Up Border FilterGaussPyramidLayerUpBorder Image Filter Bilateral Gauss Border FilterBilateralGaussBorder Image Filter High Pass FilterHighPass Image Filter High Pass Border FilterHighPassBorder Image Filter Low Pass FilterLowPass Image Filter Low Pass Border FilterLowPassBorder Image Filter Sharpen FilterSharpen Image Filter Sharpen Border FilterSharpenBorder Image Filter Unsharp Border FilterUnsharpBorder Image Filter Wiener Border FilterWienerBorder Image Filter Gradient Vector Prewitt Border GradientVectorPrewittBorder Image Filter Gradient Vector Scharr Border GradientVectorScharrBorder Image Filter Gradient Vector Sobel Border GradientVectorSobelBorder Computer Vision Filtering Functions Computer Vision Image Filter Distance Transform FilterDistanceTransform Image Filter Harris Corners Border FilterHarrisCornersBorder Image Filter Hough Line FilterHoughLine Image Filter Histogram Of Oriented Gradients Border HistogramOfOrientedGradientsBorder Image Filter Flood Fill FloodFill Flood Fill"
Miscellaneous,nvJitLink,https://docs.nvidia.com/cuda/nvjitlink/index.html,"nvJitLink 1. Introduction v12.5 Archive nvJitLink The User guide to nvJitLink library. 1. Introduction The JIT Link APIs are a set of APIs which can be used at runtime to link together GPU devide code. The APIs accept inputs in multiple formats, either host objects, host libraries, fatbins, device cubins, PTX, or LTOIR. The output is a linked cubin that can be loaded by cuModuleLoadData and cuModuleLoadDataEx of the CUDA Driver API. Link Time Optimization can also be performed when given LTOIR or higher level formats that include LTOIR. If an input does not contain GPU assembly code, it is first compiled and then linked. The functionality in this library is similar to the cuLink APIs in the CUDA Driver, with the following advantages Support for Link Time Optimization Allow users to use runtime linking with the latest Toolkit version that is supported as part of CUDA Toolkit release. This support may not be available in the CUDA Driver APIs if the application is running with an older driver installed in the system. Refer to CUDA Compatibility for more details. The clients get fine grain control and can specify lowlevel compiler options during linking. 2. Getting Started 2.1. System Requirements The JIT Link library requires the following system configuration POSIX threads support for nonWindows platform. GPU Any GPU with CUDA Compute Capability 3.5 or higher. CUDA Toolkit and Driver. 2.2. Installation The JIT Link library is part of the CUDA Toolkit release and the components are organized as follows in the CUDA toolkit installation directory On Windows includenvJitLink.h libx64nvJitLink.dll libx64nvJitLinkstatic.lib docpdfnvJitLinkUserGuide.pdf On Linux includenvJitLink.h lib64libnvJitLink.so lib64libnvJitLinkstatic.a docpdfnvJitLinkUserGuide.pdf 3. User Interface This chapter presents the JIT Link APIs. Basic usage of the API is explained in Basic Usage . Error codes Linking Supported Link Options 3.1. Error codes Enumerations nvJitLinkResult The enumerated type nvJitLinkResult defines API call result codes. 3.1.1. Enumerations enum nvJitLinkResult The enumerated type nvJitLinkResult defines API call result codes. nvJitLink APIs return nvJitLinkResult codes to indicate the result. Values enumerator NVJITLINKSUCCESS enumerator NVJITLINKERRORUNRECOGNIZEDOPTION enumerator NVJITLINKERRORMISSINGARCH enumerator NVJITLINKERRORINVALIDINPUT enumerator NVJITLINKERRORPTXCOMPILE enumerator NVJITLINKERRORNVVMCOMPILE enumerator NVJITLINKERRORINTERNAL enumerator NVJITLINKERRORTHREADPOOL enumerator NVJITLINKERRORUNRECOGNIZEDINPUT 3.2. Linking Enumerations nvJitLinkInputType The enumerated type nvJitLinkInputType defines the kind of inputs that can be passed to nvJitLinkAdd APIs. Functions nvJitLinkResult nvJitLinkAddData nvJitLinkHandle handle, nvJitLinkInputType inputType, const void data, sizet size, const char name nvJitLinkAddData adds data image to the link. nvJitLinkResult nvJitLinkAddFile nvJitLinkHandle handle, nvJitLinkInputType inputType, const char fileName nvJitLinkAddFile reads data from file and links it in. nvJitLinkResult nvJitLinkComplete nvJitLinkHandle handle nvJitLinkComplete does the actual link. nvJitLinkResult nvJitLinkCreate nvJitLinkHandle handle, uint32t numOptions, const char options nvJitLinkCreate creates an instance of nvJitLinkHandle with the given input options, and sets the output parameter handle . nvJitLinkResult nvJitLinkDestroy nvJitLinkHandle handle nvJitLinkDestroy frees the memory associated with the given handle and sets it to NULL. nvJitLinkResult nvJitLinkGetErrorLog nvJitLinkHandle handle, char log nvJitLinkGetErrorLog puts any error messages in the log. nvJitLinkResult nvJitLinkGetErrorLogSize nvJitLinkHandle handle, sizet size nvJitLinkGetErrorLogSize gets the size of the error log. nvJitLinkResult nvJitLinkGetInfoLog nvJitLinkHandle handle, char log nvJitLinkGetInfoLog puts any info messages in the log. nvJitLinkResult nvJitLinkGetInfoLogSize nvJitLinkHandle handle, sizet size nvJitLinkGetInfoLogSize gets the size of the info log. nvJitLinkResult nvJitLinkGetLinkedCubin nvJitLinkHandle handle, void cubin nvJitLinkGetLinkedCubin gets the linked cubin. nvJitLinkResult nvJitLinkGetLinkedCubinSize nvJitLinkHandle handle, sizet size nvJitLinkGetLinkedCubinSize gets the size of the linked cubin. nvJitLinkResult nvJitLinkGetLinkedPtx nvJitLinkHandle handle, char ptx nvJitLinkGetLinkedPtx gets the linked ptx. nvJitLinkResult nvJitLinkGetLinkedPtxSize nvJitLinkHandle handle, sizet size nvJitLinkGetLinkedPtxSize gets the size of the linked ptx. nvJitLinkResult nvJitLinkVersion unsigned int major, unsigned int minor nvJitLinkVersion returns the current version of nvJitLink. Typedefs nvJitLinkHandle nvJitLinkHandle is the unit of linking, and an opaque handle for a program. 3.2.1. Enumerations enum nvJitLinkInputType The enumerated type nvJitLinkInputType defines the kind of inputs that can be passed to nvJitLinkAdd APIs. Values enumerator NVJITLINKINPUTNONE enumerator NVJITLINKINPUTCUBIN enumerator NVJITLINKINPUTPTX enumerator NVJITLINKINPUTLTOIR enumerator NVJITLINKINPUTFATBIN enumerator NVJITLINKINPUTOBJECT enumerator NVJITLINKINPUTLIBRARY enumerator NVJITLINKINPUTANY 3.2.2. Functions static inline nvJitLinkResult nvJitLinkAddData nvJitLinkHandle handle , nvJitLinkInputType inputType , const void data , sizet size , const char name nvJitLinkAddData adds data image to the link. Parameters handle in nvJitLink handle. inputType in kind of input. data in pointer to data image in memory. size in size of the data. name in name of input object. Returns NVJITLINKSUCCESS NVJITLINKERRORINVALIDINPUT NVJITLINKERRORINTERNAL static inline nvJitLinkResult nvJitLinkAddFile nvJitLinkHandle handle , nvJitLinkInputType inputType , const char fileName nvJitLinkAddFile reads data from file and links it in. Parameters handle in nvJitLink handle. inputType in kind of input. fileName in name of file. Returns NVJITLINKSUCCESS NVJITLINKERRORINVALIDINPUT NVJITLINKERRORINTERNAL static inline nvJitLinkResult nvJitLinkComplete nvJitLinkHandle handle nvJitLinkComplete does the actual link. Parameters handle in nvJitLink handle. Returns NVJITLINKSUCCESS NVJITLINKERRORINVALIDINPUT NVJITLINKERRORINTERNAL static inline nvJitLinkResult nvJitLinkCreate nvJitLinkHandle handle , uint32t numOptions , const char options nvJitLinkCreate creates an instance of nvJitLinkHandle with the given input options, and sets the output parameter handle . It supports options listed in Supported Link Options . See also nvJitLinkDestroy Parameters handle out Address of nvJitLink handle. numOptions in Number of options passed. options in Array of size numOptions of option strings. Returns NVJITLINKSUCCESS NVJITLINKERRORUNRECOGNIZEDOPTION NVJITLINKERRORMISSINGARCH NVJITLINKERRORINVALIDINPUT NVJITLINKERRORINTERNAL static inline nvJitLinkResult nvJitLinkDestroy nvJitLinkHandle handle nvJitLinkDestroy frees the memory associated with the given handle and sets it to NULL. See also nvJitLinkCreate Parameters handle in Address of nvJitLink handle. Returns NVJITLINKSUCCESS NVJITLINKERRORINVALIDINPUT NVJITLINKERRORINTERNAL static inline nvJitLinkResult nvJitLinkGetErrorLog nvJitLinkHandle handle , char log nvJitLinkGetErrorLog puts any error messages in the log. User is responsible for allocating enough space to hold the log . See also nvJitLinkGetErrorLogSize Parameters handle in nvJitLink handle. log out The error log. Returns NVJITLINKSUCCESS NVJITLINKERRORINVALIDINPUT NVJITLINKERRORINTERNAL static inline nvJitLinkResult nvJitLinkGetErrorLogSize nvJitLinkHandle handle , sizet size nvJitLinkGetErrorLogSize gets the size of the error log. See also nvJitLinkGetErrorLog Parameters handle in nvJitLink handle. size out Size of the error log. Returns NVJITLINKSUCCESS NVJITLINKERRORINVALIDINPUT NVJITLINKERRORINTERNAL static inline nvJitLinkResult nvJitLinkGetInfoLog nvJitLinkHandle handle , char log nvJitLinkGetInfoLog puts any info messages in the log. User is responsible for allocating enough space to hold the log . See also nvJitLinkGetInfoLogSize Parameters handle in nvJitLink handle. log out The info log. Returns NVJITLINKSUCCESS NVJITLINKERRORINVALIDINPUT NVJITLINKERRORINTERNAL static inline nvJitLinkResult nvJitLinkGetInfoLogSize nvJitLinkHandle handle ,"
Miscellaneous,nvFatbin,https://docs.nvidia.com/cuda/nvfatbin/index.html,"nvFatbin 1. Introduction v12.5 Archive nvFatbin The User guide to nvFatbin library. 1. Introduction The Fatbin Creator APIs are a set of APIs which can be used at runtime to combine multiple CUDA objects into one CUDA fat binary fatbin. The APIs accept inputs in multiple formats, either device cubins, PTX, or LTOIR. The output is a fatbin that can be loaded by cuModuleLoadData of the CUDA Driver API. The functionality in this library is similar to the fatbinary offline tool in the CUDA toolkit, with the following advantages Support for runtime fatbin creation. The clients get fine grain control over the input process. Supports direct input from memory, rather than requiring inputs be written to files. 2. Getting Started 2.1. System Requirements The Fatbin Creator library requires no special system configuration. It does not require a GPU. 2.2. Installation The Fatbin Creator library is part of the CUDA Toolkit release and the components are organized as follows in the CUDA toolkit installation directory On Windows includenvFatbin.h libx64nvFatbin.dll libx64nvFatbinstatic.lib docpdfnvFatbinUserGuide.pdf On Linux includenvFatbin.h lib64libnvfatbin.so lib64libnvfatbinstatic.a docpdfnvFatbinUserGuide.pdf 3. User Interface This chapter presents the Fatbin Creator APIs. Basic usage of the API is explained in Basic Usage . Error codes Creation Supported Options 3.1. Error codes Enumerations nvFatbinResult The enumerated type nvFatbinResult defines API call result codes. Functions const char nvFatbinGetErrorString nvFatbinResult result nvFatbinGetErrorString returns an error description string for each error code. 3.1.1. Enumerations enum nvFatbinResult The enumerated type nvFatbinResult defines API call result codes. nvFatbin APIs return nvFatbinResult codes to indicate the result. Values enumerator NVFATBINSUCCESS enumerator NVFATBINERRORINTERNAL enumerator NVFATBINERRORELFARCHMISMATCH enumerator NVFATBINERRORELFSIZEMISMATCH enumerator NVFATBINERRORMISSINGPTXVERSION enumerator NVFATBINERRORNULLPOINTER enumerator NVFATBINERRORCOMPRESSIONFAILED enumerator NVFATBINERRORCOMPRESSEDSIZEEXCEEDED enumerator NVFATBINERRORUNRECOGNIZEDOPTION enumerator NVFATBINERRORINVALIDARCH enumerator NVFATBINERRORINVALIDNVVM enumerator NVFATBINERROREMPTYINPUT enumerator NVFATBINERRORMISSINGPTXARCH enumerator NVFATBINERRORPTXARCHMISMATCH enumerator NVFATBINERRORMISSINGFATBIN enumerator NVFATBINERRORINVALIDINDEX enumerator NVFATBINERRORIDENTIFIERREUSE 3.1.2. Functions const char nvFatbinGetErrorString nvFatbinResult result nvFatbinGetErrorString returns an error description string for each error code. Parameters result in error code Returns nullptr, if result is NVFATBINSUCCESS a string, if result is not NVFATBINSUCCESS 3.2. Fatbinary Creation Functions nvFatbinResult nvFatbinAddCubin nvFatbinHandle handle, const void code, sizet size, const char arch, const char identifier nvFatbinAddCubin adds a CUDA binary to the fatbinary. nvFatbinResult nvFatbinAddIndex nvFatbinHandle handle, const void code, sizet size, const char identifier nvFatbinAddIndex adds an index file to the fatbinary. nvFatbinResult nvFatbinAddLTOIR nvFatbinHandle handle, const void code, sizet size, const char arch, const char identifier, const char optionsCmdLine nvFatbinAddLTOIR adds LTOIR to the fatbinary. nvFatbinResult nvFatbinAddPTX nvFatbinHandle handle, const char code, sizet size, const char arch, const char identifier, const char optionsCmdLine nvFatbinAddPTX adds PTX to the fatbinary. nvFatbinResult nvFatbinAddReloc nvFatbinHandle handle, const void code, sizet size nvFatbinAddReloc adds relocatable PTX entries from a host object to the fatbinary. nvFatbinResult nvFatbinCreate nvFatbinHandle handleindirect, const char options, sizet optionsCount nvFatbinCreate creates a new handle nvFatbinResult nvFatbinDestroy nvFatbinHandle handleindirect nvFatbinDestroy destroys the handle. nvFatbinResult nvFatbinGet nvFatbinHandle handle, void buffer nvFatbinGet returns the completed fatbinary. nvFatbinResult nvFatbinSize nvFatbinHandle handle, sizet size nvFatbinSize returns the fatbinarys size. nvFatbinResult nvFatbinVersion unsigned int major, unsigned int minor nvFatbinVersion returns the current version of nvFatbin Typedefs nvFatbinHandle nvFatbinHandle is the unit of fatbin creation, and an opaque handle for a program. 3.2.1. Functions nvFatbinResult nvFatbinAddCubin nvFatbinHandle handle , const void code , sizet size , const char arch , const char identifier nvFatbinAddCubin adds a CUDA binary to the fatbinary. User is responsible for making sure all strings are wellformed. Parameters handle in nvFatbin handle. code in The cubin. size in The size of the cubin. arch in The architecture that this cubin is for. identifier in Name of the cubin, useful when extracting the fatbin with tools like cuobjdump. Returns NVFATBINSUCCESS NVFATBINERRORINVALIDARCH NVFATBINERRORELFARCHMISMATCH NVFATBINERRORELFSIZEMISMATCH NVFATBINERRORCOMPRESSIONFAILED, NVFATBINERRORUNRECOGNIZEDOPTION NVFATBINERRORCOMPRESSEDSIZEEXCEEDED NVFATBINERROREMPTYINPUT NVFATBINERRORINTERNAL nvFatbinResult nvFatbinAddIndex nvFatbinHandle handle , const void code , sizet size , const char identifier nvFatbinAddIndex adds an index file to the fatbinary. User is responsible for making sure all strings are wellformed. Parameters handle in nvFatbin handle. code in The index. size in The size of the index. identifier in Name of the index, useful when extracting the fatbin with tools like cuobjdump. Returns NVFATBINSUCCESS NVFATBINERRORINVALIDINDEX NVFATBINERRORCOMPRESSIONFAILED, NVFATBINERRORUNRECOGNIZEDOPTION NVFATBINERRORCOMPRESSEDSIZEEXCEEDED NVFATBINERROREMPTYINPUT NVFATBINERRORINTERNAL nvFatbinResult nvFatbinAddLTOIR nvFatbinHandle handle , const void code , sizet size , const char arch , const char identifier , const char optionsCmdLine nvFatbinAddLTOIR adds LTOIR to the fatbinary. User is responsible for making sure all strings are wellformed. Parameters handle in nvFatbin handle. code in The LTOIR code. size in The size of the LTOIR code. arch in The architecture that this LTOIR is for. identifier in Name of the LTOIR, useful when extracting the fatbin with tools like cuobjdump. optionsCmdLine in Options used during JIT compilation. Returns NVFATBINSUCCESS NVFATBINERRORNULLPOINTER NVFATBINERRORINVALIDARCH NVFATBINERRORCOMPRESSIONFAILED, NVFATBINERRORUNRECOGNIZEDOPTION NVFATBINERRORCOMPRESSEDSIZEEXCEEDED NVFATBINERROREMPTYINPUT NVFATBINERRORINTERNAL nvFatbinResult nvFatbinAddPTX nvFatbinHandle handle , const char code , sizet size , const char arch , const char identifier , const char optionsCmdLine nvFatbinAddPTX adds PTX to the fatbinary. User is responsible for making sure all string are wellformed. The size should be inclusive of the terminating null character 0. If the final character is not 0, one will be added automatically, but in doing so, the code will be copied if it hasnt already been copied. Parameters handle in nvFatbin handle. code in The PTX code. size in The size of the PTX code. arch in The architecture that this PTX is for. identifier in Name of the PTX, useful when extracting the fatbin with tools like cuobjdump. optionsCmdLine in Options used during JIT compilation. Returns NVFATBINSUCCESS NVFATBINERRORNULLPOINTER NVFATBINERRORINVALIDARCH NVFATBINERRORPTXARCHMISMATCH NVFATBINERRORCOMPRESSIONFAILED, NVFATBINERRORUNRECOGNIZEDOPTION NVFATBINERRORCOMPRESSEDSIZEEXCEEDED NVFATBINERROREMPTYINPUT NVFATBINERRORMISSINGPTXVERSION NVFATBINERRORMISSINGPTXARCH NVFATBINERRORINTERNAL nvFatbinResult nvFatbinAddReloc nvFatbinHandle handle , const void code , sizet size nvFatbinAddReloc adds relocatable PTX entries from a host object to the fatbinary. Note that each relocatable ptx source must have a unique identifier the identifiers are taken from the objects entries. This is enforced as only one entry per sm of each unique identifier. Note also that handle options are ignored for this operation. Instead, the host objects options are copied over from each of its entries. Parameters handle in nvFatbin handle. code in The host object image."
Miscellaneous,NVRTC (Runtime Compilation),https://docs.nvidia.com/cuda/nvrtc/index.html,"NVRTC 1. Introduction v12.5 PDF Archive nvrtc The User guide for the NVRTC library. 1. Introduction NVRTC is a runtime compilation library for CUDA C. It accepts CUDA C source code in character string form and creates handles that can be used to obtain the PTX. The PTX string generated by NVRTC can be loaded by cuModuleLoadData and cuModuleLoadDataEx , and linked with other modules by using the nvJitLink library or using cuLinkAddData of the CUDA Driver API. This facility can often provide optimizations and performance not possible in a purely offline static compilation. In the absence of NVRTC or any runtime compilation support in CUDA, users needed to spawn a separate process to execute nvcc at runtime if they wished to implement runtime compilation in their applications or libraries, and, unfortunately, this approach has the following drawbacks The compilation overhead tends to be higher than necessary. End users are required to install nvcc and related tools which make it complicated to distribute applications that use runtime compilation. NVRTC addresses these issues by providing a library interface that eliminates overhead associated with spawning separate processes, disk IO,and so on, while keeping application deployment simple. 2. Getting Started 2.1. System Requirements NVRTC requires the following system configuration Operating System Linux x8664, Linux ppc64le, Linux aarch64 or Windows x8664. GPU Any GPU with CUDA Compute Capability 2.0 or higher. CUDA Toolkit and Driver. 2.2. Installation NVRTC is part of the CUDA Toolkit release and the components are organized as follows in the CUDA toolkit installation directory On Windows includenvrtc.h binnvrtc64Major Release VersionMinor Release Version0.dll binnvrtcbuiltins64Major Release VersionMinor Release Version.dll libx64nvrtc.lib libx64nvrtcstatic.lib libx64nvrtcbuiltinsstatic.lib docpdfNVRTCUserGuide.pdf On Linux includenvrtc.h lib64libnvrtc.so lib64libnvrtc.so.Major Release Version.Minor Release Version lib64libnvrtc.so.Major Release Version.Minor Release Version.build version lib64libnvrtcbuiltins.so lib64libnvrtcbuiltins.so.Major Release Version.Minor Release Version lib64libnvrtcbuiltins.so.Major Release Version.Minor Release Version.build version lib64libnvrtcstatic.a lib64libnvrtcbuiltinsstatic.a docpdfNVRTCUserGuide.pdf 3. User Interface This chapter presents the API of NVRTC. Basic usage of the API is explained in Basic Usage . Error Handling General Information Query Compilation Supported Compile Options Host Helper 3.1. Error Handling NVRTC defines the following enumeration type and function for API call error handling. Enumerations nvrtcResult The enumerated type nvrtcResult defines API call result codes. Functions const char nvrtcGetErrorString nvrtcResult result nvrtcGetErrorString is a helper function that returns a string describing the given nvrtcResult code, e.g., NVRTCSUCCESS to NVRTCSUCCESS . 3.1.1. Enumerations enum nvrtcResult The enumerated type nvrtcResult defines API call result codes. NVRTC API functions return nvrtcResult to indicate the call result. Values enumerator NVRTCSUCCESS enumerator NVRTCERROROUTOFMEMORY enumerator NVRTCERRORPROGRAMCREATIONFAILURE enumerator NVRTCERRORINVALIDINPUT enumerator NVRTCERRORINVALIDPROGRAM enumerator NVRTCERRORINVALIDOPTION enumerator NVRTCERRORCOMPILATION enumerator NVRTCERRORBUILTINOPERATIONFAILURE enumerator NVRTCERRORNONAMEEXPRESSIONSAFTERCOMPILATION enumerator NVRTCERRORNOLOWEREDNAMESBEFORECOMPILATION enumerator NVRTCERRORNAMEEXPRESSIONNOTVALID enumerator NVRTCERRORINTERNALERROR enumerator NVRTCERRORTIMEFILEWRITEFAILED 3.1.2. Functions const char nvrtcGetErrorString nvrtcResult result nvrtcGetErrorString is a helper function that returns a string describing the given nvrtcResult code, e.g., NVRTCSUCCESS to NVRTCSUCCESS . For unrecognized enumeration values, it returns NVRTCERROR unknown . Parameters result in CUDA Runtime Compilation API result code. Returns Message string for the given nvrtcResult code. 3.2. General Information Query NVRTC defines the following function for general information query. Functions nvrtcResult nvrtcGetNumSupportedArchs int numArchs nvrtcGetNumSupportedArchs sets the output parameter numArchs with the number of architectures supported by NVRTC. nvrtcResult nvrtcGetSupportedArchs int supportedArchs nvrtcGetSupportedArchs populates the array passed via the output parameter supportedArchs with the architectures supported by NVRTC. nvrtcResult nvrtcVersion int major, int minor nvrtcVersion sets the output parameters major and minor with the CUDA Runtime Compilation version number. 3.2.1. Functions nvrtcResult nvrtcGetNumSupportedArchs int numArchs nvrtcGetNumSupportedArchs sets the output parameter numArchs with the number of architectures supported by NVRTC. This can then be used to pass an array to nvrtcGetSupportedArchs to get the supported architectures. see nvrtcGetSupportedArchs Parameters numArchs out number of supported architectures. Returns NVRTCSUCCESS NVRTCERRORINVALIDINPUT nvrtcResult nvrtcGetSupportedArchs int supportedArchs nvrtcGetSupportedArchs populates the array passed via the output parameter supportedArchs with the architectures supported by NVRTC. The array is sorted in the ascending order. The size of the array to be passed can be determined using nvrtcGetNumSupportedArchs . see nvrtcGetNumSupportedArchs Parameters supportedArchs out sorted array of supported architectures. Returns NVRTCSUCCESS NVRTCERRORINVALIDINPUT nvrtcResult nvrtcVersion int major , int minor nvrtcVersion sets the output parameters major and minor with the CUDA Runtime Compilation version number. Parameters major out CUDA Runtime Compilation major version number. minor out CUDA Runtime Compilation minor version number. Returns NVRTCSUCCESS NVRTCERRORINVALIDINPUT 3.3. Compilation NVRTC defines the following type and functions for actual compilation. Functions nvrtcResult nvrtcAddNameExpression nvrtcProgram prog, const char const nameexpression nvrtcAddNameExpression notes the given name expression denoting the address of a global function or device constant variable. nvrtcResult nvrtcCompileProgram nvrtcProgram prog, int numOptions, const char const options nvrtcCompileProgram compiles the given program. nvrtcResult nvrtcCreateProgram nvrtcProgram prog, const char src, const char name, int numHeaders, const char const headers, const char const includeNames nvrtcCreateProgram creates an instance of nvrtcProgram with the given input parameters, and sets the output parameter prog with it. nvrtcResult nvrtcDestroyProgram nvrtcProgram prog nvrtcDestroyProgram destroys the given program. nvrtcResult nvrtcGetCUBIN nvrtcProgram prog, char cubin nvrtcGetCUBIN stores the cubin generated by the previous compilation of prog in the memory pointed by cubin . nvrtcResult nvrtcGetCUBINSize nvrtcProgram prog, sizet cubinSizeRet nvrtcGetCUBINSize sets the value of cubinSizeRet with the size of the cubin generated by the previous compilation of prog . nvrtcResult nvrtcGetLTOIR nvrtcProgram prog, char LTOIR nvrtcGetLTOIR stores the LTO IR generated by the previous compilation of prog in the memory pointed by LTOIR . nvrtcResult nvrtcGetLTOIRSize nvrtcProgram prog, sizet LTOIRSizeRet nvrtcGetLTOIRSize sets the value of LTOIRSizeRet with the size of the LTO IR generated by the previous compilation of prog . nvrtcResult nvrtcGetLoweredName nvrtcProgram prog, const char const nameexpression, const char loweredname nvrtcGetLoweredName extracts the lowered mangled name for a global function or device constant variable, and updates loweredname to point to it. nvrtcResult nvrtcGetNVVM nvrtcProgram prog, char nvvm DEPRECATION NOTICE This function will be removed in a future release. nvrtcResult nvrtcGetNVVMSize nvrtcProgram prog, sizet nvvmSizeRet DEPRECATION NOTICE This function will be removed in a future release. nvrtcResult nvrtcGetOptiXIR nvrtcProgram prog, char optixir nvrtcGetOptiXIR stores the OptiX IR generated by the previous compilation of prog in the memory pointed by optixir . nvrtcResult"
Miscellaneous,cuSOLVER,https://docs.nvidia.com/cuda/cusolver/index.html,"cuSOLVER 1. Introduction v12.5 PDF Archive cuSOLVER API Reference The API reference guide for cuSOLVER, a GPU accelerated library for decompositions and linear system solutions for both dense and sparse matrices. 1. Introduction The cuSolver library is a highlevel package based on the cuBLAS and cuSPARSE libraries. It consists of two modules corresponding to two sets of API The cuSolver API on a single GPU The cuSolverMG API on a single node multiGPU Each of these can be used independently or in concert with other toolkit libraries. To simplify the notation, cuSolver denotes single GPU API and cuSolverMg denotes multiGPU API. The intent of cuSolver is to provide useful LAPACKlike features, such as common matrix factorization and triangular solve routines for dense matrices, a sparse leastsquares solver and an eigenvalue solver. In addition cuSolver provides a new refactorization library useful for solving sequences of matrices with a shared sparsity pattern. cuSolver combines three separate components under a single umbrella. The first part of cuSolver is called cuSolverDN, and deals with dense matrix factorization and solve routines such as LU, QR, SVD and LDLT, as well as useful utilities such as matrix and vector permutations. Next, cuSolverSP provides a new set of sparse routines based on a sparse QR factorization. Not all matrices have a good sparsity pattern for parallelism in factorization, so the cuSolverSP library also provides a CPU path to handle those sequentiallike matrices. For those matrices with abundant parallelism, the GPU path will deliver higher performance. The library is designed to be called from C and C. The final part is cuSolverRF, a sparse refactorization package that can provide very good performance when solving a sequence of matrices where only the coefficients are changed but the sparsity pattern remains the same. The GPU path of the cuSolver library assumes data is already in the device memory. It is the responsibility of the developer to allocate memory and to copy data between GPU memory and CPU memory using standard CUDA runtime API routines, such as cudaMalloc , cudaFree , cudaMemcpy , and cudaMemcpyAsync . cuSolverMg is GPUaccelerated ScaLAPACK. By now, cuSolverMg supports 1D column block cyclic layout and provides symmetric eigenvalue solver. Note The cuSolver library requires hardware with a CUDA Compute Capability CC of 5.0 or higher. Please see the CUDA C Programming Guide for a list of the Compute Capabilities corresponding to all NVIDIA GPUs. 1.1. cuSolverDN Dense LAPACK The cuSolverDN library was designed to solve dense linear systems of the form Ax b where the coefficient matrix Ain Rnxn , righthandside vector bin Rn and solution vector xin Rn The cuSolverDN library provides QR factorization and LU with partial pivoting to handle a general matrix A , which may be nonsymmetric. Cholesky factorization is also provided for symmetricHermitian matrices. For symmetric indefinite matrices, we provide BunchKaufman LDL factorization. The cuSolverDN library also provides a helpful bidiagonalization routine and singular value decomposition SVD. The cuSolverDN library targets computationallyintensive and popular routines in LAPACK, and provides an API compatible with LAPACK. The user can accelerate these timeconsuming routines with cuSolverDN and keep others in LAPACK without a major change to existing code. 1.2. cuSolverSP Sparse LAPACK The cuSolverSP library was mainly designed to a solve sparse linear system Ax b and the leastsquares problem x argminAz b where sparse matrix Ain Rmxn , righthandside vector bin Rm and solution vector xin Rn . For a linear system, we require mn . The core algorithm is based on sparse QR factorization. The matrix A is accepted in CSR format. If matrix A is symmetricHermitian, the user has to provide a full matrix, ie fill missing lower or upper part. If matrix A is symmetric positive definite and the user only needs to solve Ax b , Cholesky factorization can work and the user only needs to provide the lower triangular part of A . On top of the linear and leastsquares solvers, the cuSolverSP library provides a simple eigenvalue solver based on shiftinverse power method, and a function to count the number of eigenvalues contained in a box in the complex plane. 1.3. cuSolverRF Refactorization The cuSolverRF library was designed to accelerate solution of sets of linear systems by fast refactorization when given new coefficients in the same sparsity pattern Aixi fi where a sequence of coefficient matrices Aiin Rnxn , righthandsides fiin Rn and solutions xiin Rn are given for i1,...,k . The cuSolverRF library is applicable when the sparsity pattern of the coefficient matrices Ai as well as the reordering to minimize fillin and the pivoting used during the LU factorization remain the same across these linear systems. In that case, the first linear system i1 requires a full LU factorization, while the subsequent linear systems i2,...,k require only the LU refactorization. The later can be performed using the cuSolverRF library. Notice that because the sparsity pattern of the coefficient matrices, the reordering and pivoting remain the same, the sparsity pattern of the resulting triangular factors Li and Ui also remains the same. Therefore, the real difference between the full LU factorization and LU refactorization is that the required memory is known ahead of time. 1.4. Naming Conventions The cuSolverDN library provides two different APIs legacy and generic . The functions in the legacy API are available for data types float , double , cuComplex , and cuDoubleComplex . The naming convention for the legacy API is as follows cusolverDn t operation where t can be S , D , C , Z , or X , corresponding to the data types float , double , cuComplex , cuDoubleComplex , and the generic type, respectively. operation can be Cholesky factorization potrf , LU with partial pivoting getrf , QR factorization geqrf and BunchKaufman factorization sytrf . The functions in the generic API provide a single entry point for each routine and support for 64bit integers to define matrix and vector dimensions. The naming convention for the generic API is dataagnostic and is as follows cusolverDn operation where"
Miscellaneous,PTX Compiler APIs,https://docs.nvidia.com/cuda/ptx-compiler-api/index.html,"PTX Compiler API 1. Introduction v12.5 Archive PTX Compiler APIs The User guide to PTX Compiler APIs. 1. Introduction The PTX Compiler APIs are a set of APIs which can be used to compile a PTX program into GPU assembly code. The APIs accept PTX programs in character string form and create handles to the compiler that can be used to obtain the GPU assembly code. The GPU assembly code string generated by the APIs can be loaded by cuModuleLoadData and cuModuleLoadDataEx , and linked with other modules by cuLinkAddData or nvJitLinkAddData API from nvjitlink of the CUDA Driver API. The main use cases for these PTX Compiler APIs are With CUDA driver APIs, compilation and loading are tied together. PTX Compiler APIs decouple the two operations. This allows applications to perform early compilation and caching of the GPU assembly code. PTX Compiler APIs allow users to use runtime compilation for the latest PTX version that is supported as part of CUDA Toolkit release. This support may not be available in the PTX JIT compiler present in the CUDA Driver if the application is running with an older driver installed in the system. Refer to CUDA Compatibility for more details. With PTX Compiler APIs, clients can implement a custom caching mechanism with the compiled GPU assembly. With CUDA driver, there is no control over caching of the JIT compilation results. The clients get fine grain control and can specify the compiler options during compilation. 2. Getting Started 2.1. System Requirements PTX Compiler library requires the following system configuration POSIX threads support for nonWindows platform. GPU Any GPU with CUDA Compute Capability 5.0 or higher. CUDA Toolkit and Driver. 2.2. Installation PTX Compiler library is part of the CUDA Toolkit release and the components are organized as follows in the CUDA toolkit installation directory On Windows includenvPTXCompiler.h libx64nvptxcompilerstatic.lib docpdfPTXCompilerAPIUserGuide.pdf On Linux includenvPTXCompiler.h lib64libnvptxcompilerstatic.a docpdfPTXCompilerAPIUserGuide.pdf 3. Thread Safety All PTX Compiler API functions are thread safe and may be invoked by multiple threads concurrently. 4. User Interface This chapter presents the PTX Compiler APIs. Basic usage of the API is explained in Basic Usage. PTXcompiler handle Error codes API Versioning Compilation APIs 4.1. PTXCompiler Handle Typedefs nvPTXCompilerHandle nvPTXCompilerHandle represents a handle to the PTX Compiler. 4.1.1. Typedefs typedef struct nvPTXCompiler nvPTXCompilerHandle nvPTXCompilerHandle represents a handle to the PTX Compiler. To compile a PTX program string, an instance of nvPTXCompiler must be created and the handle to it must be obtained using the API nvPTXCompilerCreate . Then the compilation can be done using the API nvPTXCompilerCompile . 4.2. Error codes Enumerations nvPTXCompileResult The nvPTXCompiler APIs return the nvPTXCompileResult codes to indicate the call result. 4.2.1. Enumerations enum nvPTXCompileResult The nvPTXCompiler APIs return the nvPTXCompileResult codes to indicate the call result. Values enumerator NVPTXCOMPILESUCCESS enumerator NVPTXCOMPILEERRORINVALIDCOMPILERHANDLE enumerator NVPTXCOMPILEERRORINVALIDINPUT enumerator NVPTXCOMPILEERRORCOMPILATIONFAILURE enumerator NVPTXCOMPILEERRORINTERNAL enumerator NVPTXCOMPILEERROROUTOFMEMORY enumerator NVPTXCOMPILEERRORCOMPILERINVOCATIONINCOMPLETE enumerator NVPTXCOMPILEERRORUNSUPPORTEDPTXVERSION enumerator NVPTXCOMPILEERRORUNSUPPORTEDDEVSIDESYNC 4.3. API Versioning The PTX compiler APIs are versioned so that any new features or API changes can be done by bumping up the API version. Functions nvPTXCompileResult nvPTXCompilerGetVersion unsigned int major, unsigned int minor Queries the current major and minor version of PTX Compiler APIs being used. 4.3.1. Functions nvPTXCompileResult nvPTXCompilerGetVersion unsigned int major , unsigned int minor Queries the current major and minor version of PTX Compiler APIs being used. Note The version of PTX Compiler APIs follows the CUDA Toolkit versioning. The PTX ISA version supported by a PTX Compiler API version is listed here . Parameters major out Major version of the PTX Compiler APIs minor out Minor version of the PTX Compiler APIs Returns NVPTXCOMPILESUCCESS NVPTXCOMPILEERRORINTERNAL 4.4. Compilation APIs Functions nvPTXCompileResult nvPTXCompilerCompile nvPTXCompilerHandle compiler, int numCompileOptions, const char const compileOptions Compile a PTX program with the given compiler options. nvPTXCompileResult nvPTXCompilerCreate nvPTXCompilerHandle compiler, sizet ptxCodeLen, const char ptxCode Obtains the handle to an instance of the PTX compiler initialized with the given PTX program ptxCode . nvPTXCompileResult nvPTXCompilerDestroy nvPTXCompilerHandle compiler Destroys and cleans the already created PTX compiler. nvPTXCompileResult nvPTXCompilerGetCompiledProgram nvPTXCompilerHandle compiler, void binaryImage Obtains the image of the compiled program. nvPTXCompileResult nvPTXCompilerGetCompiledProgramSize nvPTXCompilerHandle compiler, sizet binaryImageSize Obtains the size of the image of the compiled program. nvPTXCompileResult nvPTXCompilerGetErrorLog nvPTXCompilerHandle compiler, char errorLog Query the error message that was seen previously for the handle. nvPTXCompileResult nvPTXCompilerGetErrorLogSize nvPTXCompilerHandle compiler, sizet errorLogSize Query the size of the error message that was seen previously for the handle. nvPTXCompileResult nvPTXCompilerGetInfoLog nvPTXCompilerHandle compiler, char infoLog Query the information message that was seen previously for the handle. nvPTXCompileResult nvPTXCompilerGetInfoLogSize nvPTXCompilerHandle compiler, sizet infoLogSize Query the size of the information message that was seen previously for the handle. 4.4.1. Functions nvPTXCompileResult nvPTXCompilerCompile nvPTXCompilerHandle compiler , int numCompileOptions , const char const compileOptions Compile a PTX program with the given compiler options. Note 8212gpuname arch is a mandatory option. Parameters compiler inout A handle to PTX compiler initialized with the PTX program which is to be compiled. The compiled program can be accessed using the handle numCompileOptions in Length of the array compileOptions compileOptions in Compiler options with which compilation should be done. The compiler options string is a null terminated character array. A valid list of compiler options is at link . Returns NVPTXCOMPILESUCCESS NVPTXCOMPILEERROROUTOFMEMORY NVPTXCOMPILEERRORINTERNAL NVPTXCOMPILEERRORINVALIDPROGRAMHANDLE NVPTXCOMPILEERRORCOMPILATIONFAILURE NVPTXCOMPILEERRORUNSUPPORTEDPTXVERSION NVPTXCOMPILEERRORUNSUPPORTEDDEVSIDESYNC nvPTXCompileResult nvPTXCompilerCreate nvPTXCompilerHandle compiler , sizet ptxCodeLen , const char ptxCode Obtains the handle to an instance of the PTX compiler initialized with the given PTX program ptxCode . Parameters compiler out Returns a handle to PTX compiler initialized with the PTX program ptxCode ptxCodeLen in Size of the PTX program ptxCode passed as string ptxCode in The PTX program which is to be compiled passed as string. Returns NVPTXCOMPILESUCCESS NVPTXCOMPILEERROROUTOFMEMORY NVPTXCOMPILEERRORINTERNAL nvPTXCompileResult nvPTXCompilerDestroy nvPTXCompilerHandle compiler Destroys and cleans the already created PTX compiler. Parameters compiler in A handle to the PTX compiler which is to be destroyed Returns NVPTXCOMPILESUCCESS NVPTXCOMPILEERROROUTOFMEMORY NVPTXCOMPILEERRORINTERNAL NVPTXCOMPILEERRORINVALIDPROGRAMHANDLE nvPTXCompileResult nvPTXCompilerGetCompiledProgram nvPTXCompilerHandle compiler , void binaryImage Obtains the image of the compiled program. Note nvPTXCompilerCompile API should be invoked for the handle before calling this API. Otherwise, NVPTXCOMPILEERRORCOMPILERINVOCATIONINCOMPLETE is returned. Parameters compiler"
Miscellaneous,CUDA Demo Suite,https://docs.nvidia.com/cuda/demo-suite/index.html,"CUDA Demo Suite 1. Introduction v12.5 PDF Archive CUDA Demo Suite The reference guide for the CUDA Demo Suite. 1. Introduction The CUDA Demo Suite contains prebuilt applications which use CUDA. These applications demonstrate the capabilities and details of NVIDIA GPUs. 2. Demos Below are the demos within the demo suite. 2.1. deviceQuery This application enumerates the properties of the CUDA devices present in the system and displays them in a human readable format. 2.2. vectorAdd This application is a very basic demo that implements element by element vector addition. 2.3. bandwidthTest This application provides the memcopy bandwidth of the GPU and memcpy bandwidth across PCIe. This application is capable of measuring device to device copy bandwidth, host to device copy bandwidth for pageable and pagelocked memory, and device to host copy bandwidth for pageable and pagelocked memory. Arguments Usage bandwidthTest OPTION... Test the bandwidth for device to host, host to device, and device to device transfers Example measure the bandwidth of device to host pinned memory copies in the range 1024 Bytes to 102400 Bytes in 1024 Byte increments .bandwidthTest memorypinned moderange start1024 end102400 increment1024 dtoh Options Explanation help Display this help menu csv Print results as a CSV devicedeviceno all 0,1,2,,n Specify the device device to be used compute cumulative bandwidth on all the devices Specify any particular device to be used memoryMEMMODE pageable pinned Specify which memory mode to use pageable memory nonpageable system memory modeMODE quick range shmoo Specify the mode to use performs a quick measurement measures a userspecified range of values performs an intense shmoo of a large range of values htod Measure host to device transfers dtoh Measure device to host transfers dtod Measure device to device transfers wc Allocate pinned memory as writecombined cputiming Force CPUbased timing always Range Mode options startSIZE endSIZE incrementSIZE Starting transfer size in bytes Ending transfer size in bytes Increment size in bytes 2.4. busGrind Provides detailed statistics about peertopeer memory bandwidth amongst GPUs present in the system as well as pinned, unpinned memory bandwidth. Arguments Options Explanation h print usage p 0,1 enable or disable pinned memory tests default on u 0,1 enable or disable unpinned memory tests default off e 0,1 enable or disable p2p enabled memory tests default off d 0,1 enable or disable p2p disabled memory tests default off a enable all tests n disable all tests Order of parameters matters. Examples .BusGrind n p 1 e 1 Run all pinned and P2P tests .BusGrind n u 1 Runs only unpinned tests .BusGrind a Runs all tests pinned, unpinned, p2p enabled, p2p disabled 2.5. nbody This demo does an efficient allpairs simulation of a gravitational nbody simulation in CUDA. It scales the nbody simulation across multiple GPUs in a single PC if available. Adding numbodiesnumofbodies to the command line will allow users to set of bodies for simulation. Adding numdevicesN to the command line option will cause the sample to use N devices if available for simulation. In this mode, the position and velocity data for all bodies are read from system memory using zero copy rather than from device memory. For a small number of devices 4 or fewer and a large enough number of bodies, bandwidth is not a bottleneck so we can achieve strong scaling across these devices. Arguments Options Explanation fullscreen run nbody simulation in fullscreen mode fp64 use double precision floating point values for simulation hostmem stores simulation data in host memory benchmark run benchmark to measure performance numbodiesN number of bodies 1 to run in simulation deviced where d0,1,2. for the CUDA device to use numdevicesi where inumber of CUDA devices 0 to use for simulation compare compares simulation results running once on the default GPU and once on the CPU cpu run nbody simulation on the CPU tipsyfile.bin load a tipsy model file for simulation 2.6. oceanFFT This is a graphical demo which simulates an ocean height field using the CUFFT library, and renders the result using OpenGL. The following keys can be used to control the output Keys Function w Toggle wireframe 2.7. randomFog This is a graphical demo which does pseudo and quasi random numbers visualization produced by CURAND. On creation, randomFog generates 200,000 random coordinates in spherical coordinate space radius, angle rho, angle theta with curands XORWOW algorithm. The coordinates are normalized for a uniform distribution through the sphere. The X axis is drawn with blue in the negative direction and yellow positive. The Y axis is drawn with green in the negative direction and magenta positive. The Z axis is drawn with red in the negative direction and cyan positive. The following keys can be used to control the output Keys Function s Generate new set of random nos and display as spherical coordinates Sphere e Generate new set of random nos and display on a spherical surface shEll b Generate new set of random nos and display as cartesian coordinates cuBeBox p Generate new set of random nos and display on a cartesian plane Plane i, l, j Rotate the negative Zaxis up, right, down and left respectively a Toggle autorotation t Toggle 10x zoom z Toggle axes display x Select XORWOW generator default c Select Sobol generator v Select scrambled Sobol generator r Reset XORWOW i.e. reset to initial seed and regenerate Increment the number of Sobol dimensions and regenerate Reset the number of Sobol dimensions to 1 and regenerate Increment the number of displayed points by 8,000 max. 200,000 Decrement the number of displayed points by 8,000 down to min. 8000 qESC Quit the application. 3. Notices 3.1. Notice This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation NVIDIA makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of"
Miscellaneous,CUDA on WSL,https://docs.nvidia.com/cuda/wsl-user-guide/index.html,"CUDA on WSL 1. NVIDIA GPU Accelerated Computing on WSL 2 v12.5 PDF Archive CUDA on WSL User Guide The guide for using NVIDIA CUDA on Windows Subsystem for Linux. 1. NVIDIA GPU Accelerated Computing on WSL 2 WSL or Windows Subsystem for Linux is a Windows feature that enables users to run native Linux applications, containers and commandline tools directly on Windows 11 and later OS builds. CUDA support in this user guide is specifically for WSL 2, which is the second generation of WSL that offers the following benefits Linux applications can run as is in WSL 2. WSL 2 is characteristically a VM with a Linux WSL Kernel in it that provides full compatibility with mainstream Linux kernel allowing support for native Linux applications including popular Linux distros. Faster file system support and thats more performant. WSL 2 is tightly integrated with the Microsoft Windows operating system, which allows it to run Linux applications alongside and even interop with other Windows desktop and modern store apps. For the rest of this user guide, WSL and WSL 2 may be used interchangeably. Typically, developers working across both Linux and Windows environments have a very disruptive workflow. They either have to Use different systems for Linux and Windows, or Dual Boot i.e. install Linux and Windows in separate partitions on the same or different hard disks on the system and boot to the OS of choice. In both cases, developers have to stop all the work and then switch the system or reboot. Also this has historically restricted the development of seamless, well integrated tools and software systems across two dominant ecosystems. WSL enables users to have a seamless transition across the two environments without the need for a resource intensive traditional virtual machine and to improve productivity and develop using tools and integrate their workflow. More importantly WSL 2 enables applications that were hitherto only available on Linux to be available on Windows. WSL 2 support for GPU allows for these applications to benefit from GPU accelerated computing and expands the domain of applications that can be developed on WSL 2. With NVIDIA CUDA support for WSL 2, developers can leverage NVIDIA GPU accelerated computing technology for data science, machine learning and inference on Windows through WSL. GPU acceleration also serves to bring down the performance overhead of running an application inside a WSL like environment close to nearnative by being able to pipeline more parallel work on the GPU with less CPU intervention. NVIDIA driver support for WSL 2 includes not only CUDA but also DirectX and Direct ML support. For some helpful examples, see httpsdocs.microsoft.comenuswindowswin32direct3d12gputensorflowwsl . WSL 2 is a key enabler in making GPU acceleration to be seamlessly shared between Windows and Linux applications on the same system a reality. This offers flexibility and versatility while also serving to open up GPU accelerated computing by making it more accessible. Figure 1. Illustration of the possibilities with NVIDIA CUDA software stack on WSL 2 This document describes a workflow for getting started with running CUDA applications or containers in a WSL 2 environment. 1.1. NVIDIA Compute Software Support on WSL 2 This table captures the readiness and suggested software versions for NVIDIA software stack for WSL 2. Package Suggested Versions Installation NVIDIA Windows Driver x86 Use the latest Windows x86 production driver. R495 and later windows will have CUDA support for WSL 2. NVIDIASMI will have a Limited Feature Set on WSL 2. Legacy CUDA IPC APIs are support from R510. Windows x86 drivers can be directly downloaded from httpswww.nvidia.comDownloadindex.aspx for WSL 2 support on Pascal or later GPUs. Docker support Supported. NVIDIA Container Toolkit Minimum versions v2.6.0 with libnvidiacontainer 1.5.1 CLI and Docker Desktop Supported. Refer to httpsdocs.nvidia.comaienterprisedeploymentguidevmware0.1.0docker.html . CUDA Toolkit and CUDA Developer Tools Preview Support Compute Sanitizer Pascal and later Nsight Systems CLI, and CUPTI Trace Volta and later Developer tools Debuggers Pascal and later Using driver r535 Developer tools Profilers Volta and later Using Windows 10 OS build 19044 with driver r545 or using Windows 11 with driver r525 Latest Linux CUDA toolkit package WSLUbuntu from 12.x releases can be downloaded from httpsdeveloper.nvidia.comcudadownloads . RAPIDS 22.04 or later 1.10 Experimental Support for single GPU. httpsdocs.rapids.ainoticesrgn0024 NCCL 2.12 or later 1.4 Refer to the NCCL Installation guide for Linux x86 . 2. Getting Started with CUDA on WSL 2 To get started with running CUDA on WSL, complete these steps in order 2.1. Step 1 Install NVIDIA Driver for GPU Support Install NVIDIA GeForce Game Ready or NVIDIA RTX Quadro Windows 11 display driver on your system with a compatible GeForce or NVIDIA RTXQuadro card from httpswww.nvidia.comDownloadindex.aspx . Refer to the system requirements in the Appendix. Note This is the only driver you need to install. Do not install any Linux display driver in WSL. 2.2. Step 2 Install WSL 2 Launch your preferred Windows Terminal Command Prompt Powershell and install WSL wsl.exe install Ensure you have the latest WSL kernel wsl.exe update 2.3. Step 3 Set Up a Linux Development Environment From a Windows terminal, enter WSL C wsl.exe The default distro is Ubuntu. To update the distro to your favorite distro from the command line and to review other WSL commands, refer to the following resources httpsdocs.microsoft.comenuswindowswslinstall httpsdocs.microsoft.comenuswindowswslbasiccommands From this point you should be able to run any existing Linux application which requires CUDA. Do not install any driver within the WSL environment. For building a CUDA application, you will need CUDA Toolkit. Read the next section for further information. 3. CUDA Support for WSL 2 The latest NVIDIA Windows GPU Driver will fully support WSL 2. With CUDA support in the driver, existing applications compiled elsewhere on a Linux system for the same target GPU can run unmodified within the WSL environment. To compile new CUDA applications, a CUDA Toolkit for Linux x86 is needed. CUDA Toolkit support for WSL is still in preview stage as developer tools such as profilers are not available yet. However, CUDA application development is"
Miscellaneous,CUDA on EFLOW,https://docs.nvidia.com/cuda/eflow-users-guide/index.html,"EFLOW Users Guide 1. EFLOW Users Guide v12.5 PDF Archive EFLOW Users Guide Describes how CUDA and NVIDIA GPU accelerated cloud native applications can be deployed on EFLOW enabled Windows devices. 1. EFLOW Users Guide 1.1. Introduction Azure IoT Edge For Linux on Windows, otherwise referred to as EFLOW, is a Microsoft Technology for the deployment of Linux AI containers on Windows Edge devices. This document details how NVIDIA CUDA and NVIDIA GPU accelerated cloud native applications can be deployed on such EFLOWenabled Windows devices. EFLOW has the following components The Windows host OS with virtualization enabled A Linux virtual machine IoT Edge Runtime IoT Edge Modules, or otherwise any dockercompatible containerized application runs on mobycontainerd GPUaccelerated IoT Edge Modules support for GeForce RTX GPUs is based on the GPU Paravirtualization that was foundational to CUDA on Windows Subsystem on Linux. So CUDA and compute support for EFLOW comes by virtue of existing CUDA support on WSL 2. CUDA on WSL 2 boosted the productivity of CUDA developers by enabling them to build, develop, and containerize GPU accelerated NVIDIA AIML Linux applications on Windows desktop computers before deployment on Linux instances on the cloud. But EFLOW is aimed at deployment for AI at the edge. A containerized NVIDIA GPU accelerated Linux application that is either hosted on Azure IoT Hub or NGC registry can be seamlessly deployed at the edge such as a retail service center or hospitals. These edge deployments are typically IT managed devices entrenched with Windows devices for manageability but the advent of AIML use cases in this space seek the convergence for Linux and Windows applications not only to coexist but also seamlessly communicate on the same device. Because CUDA support on EFLOW is predominantly based on WSL 2, refer to the Software Support, Limitations and Known Issues sections in the CUDA on WSL 2 document to stay abreast of the scope of NVIDIA software support available on EFLOW as well. Any additional prerequisites for EFLOW are covered in this document. The following sections details installation of EFLOW, prerequisites for outofthebox CUDA support, followed by sample instructions for running an existing GPU accelerated container on EFLOW. 1.2. Setup and Installation Follow the Microsoft EFLOW documentation page for various installation options suiting your needs For uptodate installation instructions, visit httpaka.msAzEFLOWinstall . For details on the EFLOW PowerShell API, visit httpaka.msAzEFLOWPowerShell . For quick setup, we have included the steps for installation through Powershell in the following sections. 1.2.1. Driver Installation On the target Windows device, first install an NVIDIA GeForce or NVIDIA RTX GPU Windows driver that is compatible with the NVIDIA GPU on your device. EFLOW VM supports deploying containerized CUDA applications and hence only the driver must be installed on the host system. CUDA Toolkit cannot be installed directly within EFLOW. NVIDIAprovided CUDA containers from the NGC registry can be deployed directly. If you are preparing a CUDA docker container, ensure that the necessary toolchains are installed. Because EFLOW is based on WSL, the restrictions of the software stack for a hybrid Linux on Windows environment apply, and not all of the NVIDIA software stack is supported. Refer to the users guide of the SDK that you are interested in to determine support. 1.2.2. Installation of EFLOW In an elevated powershell prompt perform the following Enable HyperV. EnableWindowsOptionalFeature Online FeatureName MicrosoftHyperV All Path Online True RestartNeeded False Set execution policy and verify. SetExecutionPolicy ExecutionPolicy AllSigned Force GetExecutionPolicy AllSigned Download and install EFLOW. msiPath io.PathCombineenvTEMP, AzureIoTEdge.msi ProgressPreference SilentlyContinue InvokeWebRequest httpsaka.msAzEFLOWMSI14LTSX64 OutFile msiPath StartProcess Wait msiexec ArgumentList i,io.PathCombineenvTEMP, AzureIoTEdge.msi,qn Determine host OS configuration. GetEflowHostConfiguration formatlist FreePhysicalMemoryInMB 35502 NumberOfLogicalProcessors 64, 64 DiskInfo DriveC FreeSizeInGB798 GpuInfo Count1 SupportedPassthroughTypesSystem.Object NameNVIDIA RTX A2000 Deploy EFLOW. Deploying EFLOW will set up the EFLOW runtime and virtual machine. By default, EFLOW only reserves 1024MB of system memory for use for the workloads and that is insufficient to support GPU accelerated configurations. For GPU acceleration, you will have to reserve system memory explicitly at EFLOW deployment otherwise there will not be sufficient system memory for your containerized applications to run. In order to prevent out of memory errors, reserve memory explicitly as required see example below. Refer to command line argument options available for deploying EFLOW in the official documentation for more details. 1.2.3. Prerequisites for CUDA Support x86 64bit support only. GeForce RTX GPU products. Windows 1011 Pro, Enterprise, IoT Enterprise Windows 10 users must use the November 2021 update build 19044.1620 or higher. DeployEflow only allocates 1024 MB memory by default, set it to a larger value to prevent OOM issue, check MS documents for more details at httpslearn.microsoft.comenusazureiotedgereferenceiotedgeforlinuxonwindowsfunctionsdeployeflow . Other prerequisites specific to the platform also apply. Refer to httpslearn.microsoft.comenusazureiotedgegpuacceleration?viewiotedge1.4 . 1.3. Connecting to the EFLOW VM GetEflowVmAddr 10132022 114116 Querying IP and MAC addresses from virtual machine IPP11490EFLOW Virtual machine MAC 00155db240c7 Virtual machine IP 172.24.14.242 retrieved directly from virtual machine 00155db240c7 172.24.14.242 ConnectEflowVm 1.4. Running nvidiasmi 1.5. Running GPUaccelerated Containers Let us run an Nbody simulation containerized CUDA sample from NGC, but this time inside EFLOW. iotedgeuserIPP11490EFLOW sudo docker run gpus all env NVIDIADISABLEREQUIRE1 nvcr.ionvidiak8scudasamplenbody nbody gpu benchmark Unable to find image nvcr.ionvidiak8scudasamplenbody locally nbody Pulling from nvidiak8scudasample 22c5ef60a68e Pull complete 1939e4248814 Pull complete 548afb82c856 Pull complete a424d45fd86f Pull complete 207b64ab7ce6 Pull complete f65423f1b49b Pull complete 2b60900a3ea5 Pull complete e9bff09d04df Pull complete edc14edf1b04 Pull complete 1f37f461c076 Pull complete 9026fb14bf88 Pull complete Digest sha25659261e419d6d48a772aad5bb213f9f1588fcdb042b115ceb7166c89a51f03363 Status Downloaded newer image for nvcr.ionvidiak8scudasamplenbody Run nbody benchmark numbodiesnumBodies to measure performance. fullscreen run nbody simulation in fullscreen mode fp64 use double precision floating point values for simulation hostmem stores simulation data in host memory benchmark run benchmark to measure performance numbodiesN number of bodies 1 to run in simulation deviced where d0,1,2.... for the CUDA device to use numdevicesi where inumber of CUDA devices 0 to use for simulation compare compares simulation results running once on the default GPU and once on the CPU cpu run nbody simulation on the CPU tipsyfile.bin load a tipsy model file for simulation NOTE The CUDA Samples are"
Miscellaneous,Debugger API,https://docs.nvidia.com/cuda/debugger-api/index.html,The API reference guide for the CUDA debugger.
Miscellaneous,GPUDirect RDMA,https://docs.nvidia.com/cuda/gpudirect-rdma/index.html,"GPUDirect RDMA 1. Overview v12.5 PDF Archive Developing a Linux Kernel Module using GPUDirect RDMA The API reference guide for enabling GPUDirect RDMA connections to NVIDIA GPUs. 1. Overview GPUDirect RDMA is a technology introduced in Keplerclass GPUs and CUDA 5.0 that enables a direct path for data exchange between the GPU and a thirdparty peer device using standard features of PCI Express. Examples of thirdparty devices are network interfaces, video acquisition devices, storage adapters. GPUDirect RDMA is available on both Tesla and Quadro GPUs. A number of limitations can apply, the most important being that the two devices must share the same upstream PCI Express root complex. Some of the limitations depend on the platform used and could be lifted in currentfuture products. A few straightforward changes must be made to device drivers to enable this functionality with a wide range of hardware devices. This document introduces the technology and describes the steps necessary to enable an GPUDirect RDMA connection to NVIDIA GPUs on Linux. GPUDirect RDMA within the Linux Device Driver Model 1.1. How GPUDirect RDMA Works When setting up GPUDirect RDMA communication between two peers, all physical addresses are the same from the PCI Express devices point of view. Within this physical address space are linear windows called PCI BARs. Each device has six BAR registers at most, so it can have up to six active 32bit BAR regions. 64bit BARs consume two BAR registers. The PCI Express device issues reads and writes to a peer devices BAR addresses in the same way that they are issued to system memory. Traditionally, resources like BAR windows are mapped to user or kernel address space using the CPUs MMU as memory mapped IO MMIO addresses. However, because current operating systems dont have sufficient mechanisms for exchanging MMIO regions between drivers, the NVIDIA kernel driver exports functions to perform the necessary address translations and mappings. To add GPUDirect RDMA support to a device driver, a small amount of address mapping code within the kernel driver must be modified. This code typically resides near existing calls to getuserpages . The APIs and control flow involved with GPUDirect RDMA are very similar to those used with standard DMA transfers. See Supported Systems and PCI BAR sizes for more hardware details. 1.2. Standard DMA Transfer First, we outline a standard DMA Transfer initiated from userspace. In this scenario, the following components are present Userspace program Userspace communication library Kernel driver for the device interested in doing DMA transfers The general sequence is as follows The userspace program requests a transfer via the userspace communication library. This operation takes a pointer to data a virtual address and a size in bytes. The communication library must make sure the memory region corresponding to the virtual address and size is ready for the transfer. If this is not the case already, it has to be handled by the kernel driver next step. The kernel driver receives the virtual address and size from the userspace communication library. It then asks the kernel to translate the virtual address range to a list of physical pages and make sure they are ready to be transferred to or from. We will refer to this operation as pinning the memory. The kernel driver uses the list of pages to program the physical devices DMA engines. The communication library initiates the transfer. After the transfer is done, the communication library should eventually clean up any resources used to pin the memory. We will refer to this operation as unpinning the memory. 1.3. GPUDirect RDMA Transfers For the communication to support GPUDirect RDMA transfers some changes to the sequence above have to be introduced. First of all, two new components are present Userspace CUDA library NVIDIA kernel driver As described in Basics of UVA CUDA Memory Management , programs using the CUDA library have their address space split between GPU and CPU virtual addresses, and the communication library has to implement two separate paths for them. The userspace CUDA library provides a function that lets the communication library distinguish between CPU and GPU addresses. Moreover, for GPU addresses it returns additional metadata that is required to uniquely identify the GPU memory represented by the address. See Userspace API for details. The difference between the paths for CPU and GPU addresses is in how the memory is pinned and unpinned. For CPU memory this is handled by builtin Linux Kernel functions getuserpages and putpage . However, in the GPU memory case the pinning and unpinning has to be handled by functions provided by the NVIDIA Kernel driver. See Pinning GPU memory and Unpinning GPU memory for details. Some hardware caveats are explained in Supported Systems and PCI BAR sizes . 1.4. Changes in CUDA 6.0 In this section we briefly list the changes that are available in CUDA 6.0 CUDA peertopeer tokens are no longer mandatory. For memory buffers owned by the calling process which is typical tokens can be replaced by zero 0 in the kernelmode function nvidiap2pgetpages . This new feature is meant to make it easier for existing third party software stacks to adopt RDMA for GPUDirect. As a consequence of the change above, a new API cuPointerSetAttribute has been introduced. This API must be used to register any buffer for which no peertopeer tokens are used. It is necessary to ensure correct synchronization behavior of the CUDA API when operation on memory which may be read by RDMA for GPUDirect. Failing to use it in these cases may cause data corruption. See changes in Tokens Usage . cuPointerGetAttribute has been extended to return a globally unique numeric identifier, which in turn can be used by lowerlevel libraries to detect buffer reallocations happening in userlevel code see Userspace API . It provides an alternative method to detect reallocations when intercepting CUDA allocation and deallocation APIs is not possible. The kernelmode memory pinning feature has been extended to work in combination with MultiProcess Service MPS. Caveats as of CUDA 6.0 CUDA Unified"
Miscellaneous,vGPU,https://docs.nvidia.com/cuda/vGPU/index.html,"vGPUs and CUDA 1. Overview v12.5 PDF Archive vGPUs and CUDA vGPUs that support CUDA This page describes the support for CUDA on NVIDIA virtual GPU software. 1. Overview For more information on NVIDIA virtual GPU software, visit httpsdocs.nvidia.comgridindex.html . 1.1. vGPU The following vGPU releases support CUDA CUDA Toolkit Version vGPU Software Releases CUDA 12.5 Not supported CUDA 12.4 Update 1 12.4.1 17.x releases CUDA 12.4 17.x releases CUDA 12.3 Update 2 12.3.2 Not supported CUDA 12.3 Update 1 12.3.1 Not supported CUDA 12.3 Not supported CUDA 12.2 Update 1 12.2.1 16.x releases CUDA 12.2 16.x releases CUDA 12.1 Update 1 12.1.1 15.x releases CUDA 12.1 15.x releases CUDA 12.0 Update 1 12.0.1 15.x releases CUDA 12.0 15.x releases CUDA 11.8 14.x releases CUDA 11.7 Update 1 11.7.1 14.x releases CUDA 11.7 14.x releases CUDA 11.6 Update 1 11.6.1 14.x releases CUDA 11.6 14.x releases CUDA 11.5 Update 2 11.5.2 14.x releases CUDA 11.5 Update 1 11.5.1 14.x releases CUDA 11.5 14.x releases CUDA 11.4 Update 4 11.4.4 13.x releases CUDA 11.4 Update 3 11.4.3 13.x releases CUDA 11.4 Update 2 11.4.2 13.x releases CUDA 11.4 Update 1 11.4.1 13.x releases CUDA 11.4 13.x releases CUDA 11.3 Update 1 11.3.1 Not supported CUDA 11.3 Not supported CUDA 11.2 Update 1 11.2.1 Not supported CUDA 11.2 12.x releases CUDA 11.1 Update 1 11.1.1 Not supported CUDA 11.1 Not supported CUDA 11.0 11.x releases CUDA 10.2 10.x releases CUDA 10.1 Update 2 10.1.243 Not supported CUDA 10.1 Update 1 9.x releases CUDA 10.1 general release 10.1.105 8.x releases CUDA 10.0.130 7.x releases CUDA 9.2 9.2.148 Update 1 Not supported CUDA 9.2 9.2.88 Not supported CUDA 9.1 9.1.85 6.x releases CUDA 9.0 9.0.176 5.x releases Some CUDA features might not be supported by your version of NVIDIA virtual GPU software. For details, follow the link in the table to the documentation for your version. 1.2. Notices 1.2.1. Notice This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation NVIDIA makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material defined below, code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer Terms of Sale. NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion andor use of NVIDIA products in such equipment or applications and therefore such inclusion andor use is at customers own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customers sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customers product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions andor requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to i the use of the NVIDIA product in any manner that is contrary to this document or ii customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding thirdparty products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS TOGETHER AND SEPARATELY, MATERIALS ARE BEING PROVIDED AS IS. NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE"
NVCC,NVCC,https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html,"NVIDIA CUDA Compiler Driver 1. Introduction v12.5 PDF Archive NVIDIA CUDA Compiler Driver NVCC The documentation for nvcc , the CUDA compiler driver. 1. Introduction 1.1. Overview 1.1.1. CUDA Programming Model The CUDA Toolkit targets a class of applications whose control part runs as a process on a general purpose computing device, and which use one or more NVIDIA GPUs as coprocessors for accelerating single program, multiple data SPMD parallel jobs. Such jobs are selfcontained, in the sense that they can be executed and completed by a batch of GPU threads entirely without intervention by the host process, thereby gaining optimal benefit from the parallel graphics hardware. The GPU code is implemented as a collection of functions in a language that is essentially C, but with some annotations for distinguishing them from the host code, plus annotations for distinguishing different types of data memory that exists on the GPU. Such functions may have parameters, and they can be called using a syntax that is very similar to regular C function calling, but slightly extended for being able to specify the matrix of GPU threads that must execute the called function. During its life time, the host process may dispatch many parallel GPU tasks. For more information on the CUDA programming model, consult the CUDA C Programming Guide . 1.1.2. CUDA Sources Source files for CUDA applications consist of a mixture of conventional C host code, plus GPU device functions. The CUDA compilation trajectory separates the device functions from the host code, compiles the device functions using the proprietary NVIDIA compilers and assembler, compiles the host code using a C host compiler that is available, and afterwards embeds the compiled GPU functions as fatbinary images in the host object file. In the linking stage, specific CUDA runtime libraries are added for supporting remote SPMD procedure calling and for providing explicit GPU manipulation such as allocation of GPU memory buffers and hostGPU data transfer. 1.1.3. Purpose of NVCC The compilation trajectory involves several splitting, compilation, preprocessing, and merging steps for each CUDA source file. It is the purpose of nvcc , the CUDA compiler driver, to hide the intricate details of CUDA compilation from developers. It accepts a range of conventional compiler options, such as for defining macros and includelibrary paths, and for steering the compilation process. All nonCUDA compilation steps are forwarded to a C host compiler that is supported by nvcc , and nvcc translates its options to appropriate host compiler command line options. 1.2. Supported Host Compilers A general purpose C host compiler is needed by nvcc in the following situations During nonCUDA phases except the run phase, because these phases will be forwarded by nvcc to this compiler. During CUDA phases, for several preprocessing stages and host code compilation see also The CUDA Compilation Trajectory . nvcc assumes that the host compiler is installed with the standard method designed by the compiler provider. If the host compiler installation is nonstandard, the user must make sure that the environment is set appropriately and use relevant nvcc compile options. The following documents provide detailed information about supported host compilers NVIDIA CUDA Installation Guide for Linux NVIDIA CUDA Installation Guide for Microsoft Windows On all platforms, the default host compiler executable gcc and g on Linux and cl.exe on Windows found in the current execution search path will be used, unless specified otherwise with appropriate options see File and Path Specifications . Note, nvcc does not support the compilation of file paths that exceed the maximum path length limitations of the host system. To support the compilation of long file paths, please refer to the documentation for your system. 2. Compilation Phases 2.1. NVCC Identification Macro nvcc predefines the following macros NVCC Defined when compiling CCCUDA source files. CUDACC Defined when compiling CUDA source files. CUDACCRDC Defined when compiling CUDA source files in relocatable device code mode see NVCC Options for Separate Compilation . CUDACCEWP Defined when compiling CUDA source files in extensible whole program mode see Options for Specifying Behavior of CompilerLinker . CUDACCDEBUG Defined when compiling CUDA source files in the devicedebug mode see Options for Specifying Behavior of CompilerLinker . CUDACCRELAXEDCONSTEXPR Defined when the exptrelaxedconstexpr flag is specified on the command line. Refer to the CUDA C Programming Guide for more details. CUDACCEXTENDEDLAMBDA Defined when the exptextendedlambda or extendedlambda flag is specified on the command line. Refer to the CUDA C Programming Guide for more details. CUDACCVERMAJOR Defined with the major version number of nvcc . CUDACCVERMINOR Defined with the minor version number of nvcc . CUDACCVERBUILD Defined with the build version number of nvcc . NVCCDIAGPRAGMASUPPORT Defined when the CUDA frontend compiler supports diagnostic control with the nvdiagsuppress , nvdiagerror , nvdiagwarning , nvdiagdefault , nvdiagonce , and nvdiagnostic pragmas. 2.2. NVCC Phases A compilation phase is a logical translation step that can be selected by command line options to nvcc . A single compilation phase can still be broken up by nvcc into smaller steps, but these smaller steps are just implementations of the phase they depend on seemingly arbitrary capabilities of the internal tools that nvcc uses, and all of these internals may change with a new release of the CUDA Toolkit. Hence, only compilation phases are stable across releases, and although nvcc provides options to display the compilation steps that it executes, these are for debugging purposes only and must not be copied and used in build scripts. nvcc phases are selected by a combination of command line options and input file name suffixes, and the execution of these phases may be modified by other command line options. In phase selection, the input file suffix defines the phase input, while the command line option defines the required output of the phase. The following paragraphs list the recognized file name suffixes and the supported compilation phases. A full explanation of the nvcc command line options can be found in NVCC Command Options . 2.3. Supported Input File Suffixes The following table defines how nvcc interprets"
CUDA GDB,CUDA-GDB,https://docs.nvidia.com/cuda/cuda-gdb/index.html,"CUDAGDB 1. Introduction v12.5 PDF Archive CUDAGDB The user manual for CUDAGDB, the NVIDIA tool for debugging CUDA applications on Linux and QNX systems. 1. Introduction This document introduces CUDAGDB, the NVIDIA CUDA debugger for Linux and QNX targets. 1.1. What is CUDAGDB? CUDAGDB is the NVIDIA tool for debugging CUDA applications running on Linux and QNX. CUDAGDB is an extension to GDB, the GNU Project debugger. The tool provides developers with a mechanism for debugging CUDA applications running on actual hardware. This enables developers to debug applications without the potential variations introduced by simulation and emulation environments. 1.2. Supported Features CUDAGDB is designed to present the user with a seamless debugging environment that allows simultaneous debugging of both GPU and CPU code within the same application. Just as programming in CUDA C is an extension to C programming, debugging with CUDAGDB is a natural extension to debugging with GDB. The existing GDB debugging features are inherently present for debugging the host code, and additional features have been provided to support debugging CUDA device code. CUDAGDB supports debugging CC and Fortran CUDA applications. Fortran debugging support is limited to 64bit Linux operating system. CUDAGDB allows the user to set breakpoints, to singlestep CUDA applications, and also to inspect and modify the memory and variables of any given thread running on the hardware. CUDAGDB supports debugging all CUDA applications, whether they use the CUDA driver API, the CUDA runtime API, or both. CUDAGDB supports debugging kernels that have been compiled for specific CUDA architectures, such as sm75 or sm80 , but also supports debugging kernels compiled at runtime, referred to as justintime compilation, or JIT compilation for short. 1.3. About This Document This document is the main documentation for CUDAGDB and is organized more as a user manual than a reference manual. The rest of the document will describe how to install and use CUDAGDB to debug CUDA kernels and how to use the new CUDA commands that have been added to GDB. Some walkthrough examples are also provided. It is assumed that the user already knows the basic GDB commands used to debug host applications. 2. Release Notes 12.5 Release Updated GDB version Moved from GDB 13.1 to 13.2. See GDB 13.2 changes Support removal notice Support for the macOS host client of CUDAGDB has been removed. Support for Android has been removed. Support for Python 3.6 and 3.7 has been removed. Features Multi build feature that supports native Python and TUI mode across all supported platforms. The cudagdb program is now a wrapper script that calls the appropriate cudagdb binary. If no supported Python or libncurses is detected, the wrapper will fallback to a cudagdb binary with Python and TUI support disabled. Added support for TUI mode. Added support for Python 3.10, 3.11, and 3.12. Added support for detecting and printing exceptions encountered in exited warps. This can occur when debugging an application with optimizations enabled. Added new gdbmi command equivalents for info cuda managed and info cuda line. Fixed Issues Fixed issue with printing reference parameter arguments to CUDA functions. Fixed issues resulting in crasheserrors when readingwriting fromto CUDA generic memory. Fixed issue where breakonlaunch breakpoints were missed for back to back launches of the same kernel. Fixed issue with incorrectly reporting breakpoint hit events as SIGTRAP when breakpoint is hit in divergent thread. Fixed crash on QNX when cudagdbserver packets arrive outoforder. Better error handling when encountering an error when reading CUDA disassembly. Better exit handling when resuming execution from a fatal CUDA exception. 12.4 Release Updated GDB version Moved from GDB 12.1 to 13.1. See GDB 13.1 changes Android deprecation notice Support for Android is deprecated. It will be dropped in an upcoming release. Python 3.6 and 3.7 deprecation notice Support for endoflife Python 3.6 and 3.7 versions is deprecated. It will be dropped in an upcoming release. Features Performance enhancement which reduces the number of overall CUDA Debugger API calls. Performance enhancement when loading large cubins with device functions using a large number of GPU registers. Performance enhancement when single stepping over warp wide barriers. Added support for printing values contained within constant banks from GPU core dumps. Fixed Issues Prevented shell expansion on cloned function names when disassembling. Fixed crash when setting a conditional breakpoint on an unknown symbol name. Fixed issue with setting a watchpoint on a global pointer. Fixed assertion in switchtothread1 during inferior teardown. Fixed attach failures encountered with newer Intel processors. Refactored the libpython layer to avoid unnecessary gdb code changes. 12.3 Release macOS host client deprecation notice Support for the macOS host client of CUDAGDB is deprecated. It will be dropped in an upcoming release. Features Added support for printing values contained within constant banks. New cudaconstbankbank, offset convenience function to obtain address of offset in constant bank. See Const Banks . Performance enhancements added which reduce overhead when running applications with many CUDA threads. Added support for CUDA function pointers. Fixed Issues Fixed issue when detaching from attached process that can result in a crash. Fixed thread ordering issues present with several info cuda commands. Added support for opening of GPU core dumps when no valid warps are present on the device. Added missing DWARF operators used by OptiX. Fixed issue with parsing CUDA Fortran pointer types. Fixed issue where CUDA Cluster coordinates were being displayed when no CUDA Cluster was present. 12.2 Release Features Enabled printing of extended error messages when a CUDA Debugger API error is encountered. Enabled support for debugging with Confidential Compute mode with devtools mode. See Confidential Computing Deployment Guide httpsdocs.nvidia.comconfidentialcomputingdeploymentguide.pdf for more details on how to enable the mode. Fixed Issues Fixed ?? appearing in backtrace in OptiX applications. Host shadow breakpoints are now handled correctly with CUDA Lazy Loading enabled. Fixed name mangling issue when debugging LLVM generated cubins. CUDA Cluster coordinates are now displayed correctly. Fixed issue with attaching to an application using CUDA Lazy Loading when debugging remotely with cudagdbserver. 12.1 Release CUDA Driver API added for controlling core"
Miscellaneous,Nsight Eclipse Plugins Installation Guide,https://docs.nvidia.com/cuda/nsightee-plugins-install-guide/index.html,"Nsight Eclipse Plugins Installation Guide 1. Introduction v12.5 PDF Archive Nsight Eclipse Plugins Installation Guide The user guide for installing Nsight Eclipse Plugins. 1. Introduction This guide provides the procedures to install the Nsight Eclipse Edition Plugins in users own eclipse environment. Nsight Eclipse Plugins offers fullfeatured IDE that provides an allinone integrated environment to edit, build, debug and profile CUDAC applications. 1.1. Install plugins using Eclipse IDE You can install Nsight Eclipse plugins in your own Eclipse environment or download and install Eclipse IDE for CC developers . Launch Eclipse and go to Help Install New Software.. menu. Click on the Add Button Enter name ex NsightEE in the Name field. Choose the the zip filecom.nvidia.cuda.repo.zip that contains the plugins using Archive button or Enter the full path of zip file. Nsight EE plugns zip file can be found in usrlocalcuda11.8nsighteeplugins directory. Click OK button Select Cuda Main Features option and go to next page. Accept the license agreement and click on Finish button to install the plugins. Click OK on the Security Warning dialog to ignore the warning message about unsigned content This warning message is displayed for all the plugins that are not signed by Eclipse.org. Restart eclipse when prompted. Nsight Eclipse plugins installation is now complete.Go to Help Installation Details.. Menu to verify the Cuda Developer Tools and Cuda Remote Launch plugins are installed 1.2. Uninstall plugins using Eclipse IDE Launch Eclipse and go to Help Installation Details menu. Select Cuda Developer Tools and Cuda Remote Launch options from the dialog Click on the Uninstall button. Click Finish button when asked to review and confirm. Restart eclipse when prompted. Nsight Eclipse plugins will be uninstalled after restarting eclipse. Go to Help Installation Details.. menu to verify. 1.3. Install Using Script To install or uninstall the Nsight Eclipse Plugins using the script, run the installation script provided in the bin directory of the toolkit. By default, it is located in usrlocalcuda11.8bin The usage of the script is as follows Usage .nsighteepluginsmanage.sh action eclipsedir action install or uninstall eclipsedir eclipse installation directory To install the Nsight Eclipse Plugins, run the following command usrlocalcuda11.8binnsighteepluginsmanage.sh install eclipsedir To uninstall the Nsight Eclipse Plugins, run the following command usrlocalcuda11.8binnsighteepluginsmanage.sh uninstall eclipsedir 2. Notices 2.1. Notice This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation NVIDIA makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material defined below, code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer Terms of Sale. NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion andor use of NVIDIA products in such equipment or applications and therefore such inclusion andor use is at customers own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customers sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customers product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions andor requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to i the use of the NVIDIA product in any manner that is contrary to this document or ii customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding thirdparty products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS TOGETHER AND SEPARATELY, MATERIALS ARE BEING PROVIDED AS IS. NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS,"
Miscellaneous,Nsight Eclipse Plugins Edition,https://docs.nvidia.com/cuda/nsight-eclipse-plugins-guide/index.html,"Nsight Eclipse Plugins Guide 1. Introduction v12.5 PDF Archive Nsight Eclipse Plugins Edition Getting Started Guide The user guide for using Nsight Eclipse Plugins Edition. 1. Introduction This guide introduces Nsight Eclipse Plugins Edition and provides instructions necessary to start using this tool. Nsight Eclipse is based on Eclipse CDT project. For a detailed description of Eclipse CDT features consult the integrated help CC Development User Guide available from inside Nsight through HelpHelp Contents menu. 1.1. About Nsight Eclipse Plugins Edition NVIDIA Nsight Eclipse Edition is a unified CPU plus GPU integrated development environment IDE for developing CUDA applications on Linux and Mac OS X for the x86, POWER and ARM platforms. It is designed to help developers on all stages of the software development process. Nsight Eclipse Plugins can be installed on vanilla Eclipse using the standard HelpInstall New Software.. Menu. The principal features are as follows Edit, build, debug and profile CUDAC applications CUDA aware source code editor syntax highlighting, code completion and inline help Graphical user interface for debugging heterogeneous applications Profiler integration Launch visual profiler as an external application with the CUDA application built in this IDE to easily identify performance bottlenecks For more information about Eclipse Platform, visit httpeclipse.org 2. Using Nsight Eclipse Edition 2.1. Installing Nsight Eclipse Edition Nsight Eclipse Plugins archive is part of the CUDA Toolkit. Nsight Eclipse Plugins archive can be installed using the Help Install New Software Menu on Eclipse 2.1.1. Installing CUDA Toolkit To install CUDA Toolkit Visit the NVIDIA CUDA Toolkit download page httpsdeveloper.nvidia.comcudadownloads Select appropriate operating system. Nsight Eclipse Edition is available in Mac OS X and Linux toolkit packages. Download and install the CUDA Driver. Download and install the CUDA Toolkit. Follow instructions to configure CUDA Driver and Toolkit on your system. 2.1.2. Configure CUDA Toolkit Path When Eclipse is first launched with Nsight Eclipse plugins in the new workspace, NVIDIA usage data collection dialog will be displayed as below. Click Yes to enable usage collection. This can be disabled later from the CUDA preference page. Usage data collection page To get started, CUDA Toolkit path must be configured in Eclipse with Nsight Plugins Open the Preferences page, Window Preferences. Go to CUDA toolkit section. Select the CUDA toolkit path to be used by Nsight. CUDA tookits that are installed in the default location will automatically appear. CUDA toolkit path can be also specified in the project properties page in order to use different toolkit for a project. Enable usage data collection if you wish to send usage data to NVIDIA. Click on the button to set cudagdb and Visual Profiler as the default launchers. For QNX When QNX is selected as Target OS, a dialog will be displayed to set the QNXHOST and QNXTARGET environment variables if they were not already set. QNXHOST environment variable identifies the directory that holds the hostrelated components QNXTARGET environment variable identifies the directory that holds the targetrelated components 2.2. Nsight Eclipse Main Window On the first run Eclipse will ask to pick a workspace location. The workspace is a folder where Nsight will store its settings, local files history and caches. An empty folder should be selected to avoid overwriting existing files. The main Nsight window will open after the workspace location is selected. The main window is divided into the following areas Editor displays source files that are opened for editing. Project Explorer displays project files Outline displays structure of the source file in the current editor. Problems displays errors and warnings detected by static code analysis in IDE or by a compiler during the build. Console displays make output during the build or output from the running application. 2.3. Creating a New Project From the main menu, open the new project wizard File New CUDA CC Project Specify the project name and project files location. Specify the project type like executable project. Specify the CUDA toolchain from the list of toolchains. Specify the project configurations on the next wizard page. Complete the wizard. The project will be shown in the Project Explorer view and source editor will be opened. Build the project by clicking on the hammer button on the main toolbar. Nsight main window after creating a new project 2.4. Importing CUDA Samples The CUDA samples are an optional component of the CUDA Toolkit installation. Nsight provides a mechanism to import these samples and work with them easily Note Samples that use the CUDA driver API suffixed with Drv are not supported by Nsight. From the main menu, open the new project wizard File New CUDA CC Project Specify the project name and project files location. Select Import CUDA Sample under Executable in the Project type tree. Select CUDA toolchain from the Toolchains option. location. On the next wizard page select project sample you want to import. Also select the target CPU architecture. Press Next Specify the project parameters on the next wizard page. Complete the wizard. The project will be shown in the Project Explorer view and source editor will be opened. Build the project by clicking on the hammer button on the main toolbar. 2.4.1. cuHook Sample cuHook sample builds both the library and the executable. cuHook sample should be imported as the makefile project using the following steps. From the main menu, open the new project wizard File New CUDA CC Project Select project type Makefile project and choose Empty Project Specify the project name and project files location. Complete the wizard. The project will be shown in the Project Explorer view. Right click on the project Import General File System On the next wizard page, select the location of cuHook sampleSamples7CUDALibrariescuHook Select all the source files and makefile and Finish the wizard Build the project by clicking on the hammer button on the main toolbar. To run the sample, from the main menu Run Run Configurations Select the executable Go to Environment tab New enter NameLDPRELOAD, Value.libcuhook.so.1 Run will execute the sample 2.5. Configure Build Settings To define build settings In the"
Miscellaneous,Profiler,https://docs.nvidia.com/cuda/profiler-users-guide/index.html,"Profiler 1. Preparing An Application For Profiling v12.5 PDF Archive Profiler Users Guide The user manual for NVIDIA profiling tools for optimizing performance of CUDA applications. Profiling Overview This document describes NVIDIA profiling tools that enable you to understand and optimize the performance of your CUDA, OpenACC or OpenMP applications. The Visual Profiler is a graphical profiling tool that displays a timeline of your applications CPU and GPU activity, and that includes an automated analysis engine to identify optimization opportunities. The nvprof profiling tool enables you to collect and view profiling data from the commandline. Note that Visual Profiler and nvprof will be deprecated in a future CUDA release. The NVIDIA Volta platform is the last architecture on which these tools are fully supported. It is recommended to use nextgeneration tools NVIDIA Nsight Systems for GPU and CPU sampling and tracing and NVIDIA Nsight Compute for GPU kernel profiling. Refer the Migrating to Nsight Tools from Visual Profiler and nvprof section for more details. Terminology An event is a countable activity, action, or occurrence on a device. It corresponds to a single hardware counter value which is collected during kernel execution. To see a list of all available events on a particular NVIDIA GPU, type nvprof queryevents . A metric is a characteristic of an application that is calculated from one or more event values. To see a list of all available metrics on a particular NVIDIA GPU, type nvprof querymetrics . You can also refer to the metrics reference . 1. Preparing An Application For Profiling The CUDA profiling tools do not require any application changes to enable profiling however, by making some simple modifications and additions, you can greatly increase the usability and effectiveness profiling. This section describes these modifications and how they can improve your profiling results. 1.1. Focused Profiling By default, the profiling tools collect profile data over the entire run of your application. But, as explained below, you typically only want to profile the regions of your application containing some or all of the performancecritical code. Limiting profiling to performancecritical regions reduces the amount of profile data that both you and the tools must process, and focuses attention on the code where optimization will result in the greatest performance gains. There are several common situations where profiling a region of the application is helpful. The application is a test harness that contains a CUDA implementation of all or part of your algorithm. The test harness initializes the data, invokes the CUDA functions to perform the algorithm, and then checks the results for correctness. Using a test harness is a common and productive way to quickly iterate and test algorithm changes. When profiling, you want to collect profile data for the CUDA functions implementing the algorithm, but not for the test harness code that initializes the data or checks the results. The application operates in phases, where a different set of algorithms is active in each phase. When the performance of each phase of the application can be optimized independently of the others, you want to profile each phase separately to focus your optimization efforts. The application contains algorithms that operate over a large number of iterations, but the performance of the algorithm does not vary significantly across those iterations. In this case you can collect profile data from a subset of the iterations. To limit profiling to a region of your application, CUDA provides functions to start and stop profile data collection. cudaProfilerStart is used to start profiling and cudaProfilerStop is used to stop profiling using the CUDA driver API, you get the same functionality with cuProfilerStart and cuProfilerStop . To use these functions you must include cudaprofilerapi.h or cudaProfiler.h for the driver API. When using the start and stop functions, you also need to instruct the profiling tool to disable profiling at the start of the application. For nvprof you do this with the profilefromstart off flag. For the Visual Profiler you use the Start execution with profiling enabled checkbox in the Settings View . 1.2. Marking Regions of CPU Activity The Visual Profiler can collect a trace of the CUDA function calls made by your application. The Visual Profiler shows these calls in the Timeline View , allowing you to see where each CPU thread in the application is invoking CUDA functions. To understand what the applications CPU threads are doing outside of CUDA function calls, you can use the NVIDIA Tools Extension API NVTX. When you add NVTX markers and ranges to your application, the Timeline View shows when your CPU threads are executing within those regions. nvprof also supports NVTX markers and ranges. Markers and ranges are shown in the API trace output in the timeline. In summary mode, each range is shown with CUDA activities associated with that range. 1.3. Naming CPU and CUDA Resources The Visual Profiler Timeline View shows default naming for CPU thread and GPU devices, context and streams. Using custom names for these resources can improve understanding of the application behavior, especially for CUDA applications that have many host threads, devices, contexts, or streams. You can use the NVIDIA Tools Extension API to assign custom names for your CPU and GPU resources. Your custom names will then be displayed in the Timeline View . nvprof also supports NVTX naming. Names of CUDA devices, contexts and streams are displayed in summary and trace mode. Thread names are displayed in summary mode. 1.4. Flush Profile Data To reduce profiling overhead, the profiling tools collect and record profile information into internal buffers. These buffers are then flushed asynchronously to disk with low priority to avoid perturbing application behavior. To avoid losing profile information that has not yet been flushed, the application being profiled should make sure, before exiting, that all GPU work is done using CUDA synchronization calls, and then call cudaProfilerStop or cuProfilerStop . Doing so forces buffered profile information on corresponding contexts to be flushed. If your CUDA application includes graphics that operate using a display or"
Miscellaneous,CUDA Binary Utilities,https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html,"cudabinaryutilities 1. Overview v12.5 PDF Archive CUDA Binary Utilities The application notes for cuobjdump, nvdisasm, cufilt, and nvprune. 1. Overview This document introduces cuobjdump , nvdisasm , cufilt and nvprune , four CUDA binary tools for Linux x86, ARM and P9, Windows, Mac OS and Android. 1.1. What is a CUDA Binary? A CUDA binary also referred to as cubin file is an ELFformatted file which consists of CUDA executable code sections as well as other sections containing symbols, relocators, debug info, etc. By default, the CUDA compiler driver nvcc embeds cubin files into the host executable file. But they can also be generated separately by using the cubin option of nvcc . cubin files are loaded at run time by the CUDA driver API. Note For more details on cubin files or the CUDA compilation trajectory, refer to NVIDIA CUDA Compiler Driver NVCC . 1.2. Differences between cuobjdump and nvdisasm CUDA provides two binary utilities for examining and disassembling cubin files and host executables cuobjdump and nvdisasm . Basically, cuobjdump accepts both cubin files and host binaries while nvdisasm only accepts cubin files but nvdisasm provides richer output options. Heres a quick comparison of the two tools Table 1. Comparison of cuobjdump and nvdisasm cuobjdump nvdisasm Disassemble cubin Yes Yes Extract ptx and extract and disassemble cubin from the following input files Host binaries Executables Object files Static libraries External fatbinary files Yes No Control flow analysis and output No Yes Advanced display options No Yes 1.3. Command Option Types and Notation This section of the document provides common details about the command line options for the following tools cuobjdump nvdisasm nvprune Each commandline option has a long name and a short name, which are interchangeable with each other. These two variants are distinguished by the number of hyphens that must precede the option name, i.e. long names must be preceded by two hyphens and short names must be preceded by a single hyphen. For example, I is the short name of includepath . Long options are intended for use in build scripts, where size of the option is less important than descriptive value and short options are intended for interactive use. The tools mentioned above recognize three types of command options boolean options, single value options and list options. Boolean options do not have an argument, they are either specified on a command line or not. Single value options must be specified at most once and list options may be repeated. Examples of each of these option types are, respectively Boolean option nvdisams printraw file Single value nvdisasm binary SM70 file List options cuobjdump function foo,bar,foobar file Single value options and list options must have arguments, which must follow the name of the option by either one or more spaces or an equals character. When a onecharacter short name such as I , l , and L is used, the value of the option may also immediately follow the option itself without being seperated by spaces or an equal character. The individual values of list options may be separated by commas in a single instance of the option or the option may be repeated, or any combination of these two cases. Hence, for the two sample options mentioned above that may take values, the following notations are legal o file ofile Idir1,dir2 Idir3 I dir4,dir5 For options taking a single value, if specified multiple times, the rightmost value in the command line will be considered for that option. In the below example, test.bin binary will be disassembled assuming SM75 as the architecture. nvdisasm.exe b SM70 b SM75 test.bin nvdisasm warning incompatible redefinition for option binary, the last value of this option was used For options taking a list of values, if specified multiple times, the values get appended to the list. If there are duplicate values specified, they are ignored. In the below example, functions foo and bar are considered as valid values for option function and the duplicate value foo is ignored. cuobjdump function foo function bar function foo sass test.cubin 2. cuobjdump cuobjdump extracts information from CUDA binary files both standalone and those embedded in host binaries and presents them in human readable format. The output of cuobjdump includes CUDA assembly code for each kernel, CUDA ELF section headers, string tables, relocators and other CUDA specific sections. It also extracts embedded ptx text from host binaries. For a list of CUDA assembly instruction set of each GPU architecture, see Instruction Set Reference . 2.1. Usage cuobjdump accepts a single input file each time its run. The basic usage is as following cuobjdump options file To disassemble a standalone cubin or cubins embedded in a host executable and show CUDA assembly of the kernels, use the following command cuobjdump sass input file To dump cuda elf sections in human readable format from a cubin file, use the following command cuobjdump elf cubin file To extract ptx text from a host binary, use the following command cuobjdump ptx host binary Heres a sample output of cuobjdump cuobjdump a.out sass ptx Fatbin elf code arch sm70 code version 1,7 producer cuda host linux compilesize 64bit identifier add.cu code for sm70 Function Z3addPiSS .headerflags EFCUDASM70 EFCUDAPTXSMEFCUDASM70 0000 IMAD.MOV.U32 R1, RZ, RZ, c0x00x28 0x00000a00ff017624 0x000fd000078e00ff 0010 !PT SHFL.IDX PT, RZ, RZ, RZ, RZ 0x000000fffffff389 0x000fe200000e00ff 0020 IMAD.MOV.U32 R2, RZ, RZ, c0x00x160 0x00005800ff027624 0x000fe200078e00ff 0030 MOV R3, c0x00x164 0x0000590000037a02 0x000fe20000000f00 0040 IMAD.MOV.U32 R4, RZ, RZ, c0x00x168 0x00005a00ff047624 0x000fe200078e00ff 0050 MOV R5, c0x00x16c 0x00005b0000057a02 0x000fcc0000000f00 0060 LDG.E.SYS R2, R2 0x0000000002027381 0x000ea800001ee900 0070 LDG.E.SYS R5, R4 0x0000000004057381 0x000ea200001ee900 0080 IMAD.MOV.U32 R6, RZ, RZ, c0x00x170 0x00005c00ff067624 0x000fe200078e00ff 0090 MOV R7, c0x00x174 0x00005d0000077a02 0x000fe40000000f00 00a0 IADD3 R9, R2, R5, RZ 0x0000000502097210 0x004fd00007ffe0ff 00b0 STG.E.SYS R6, R9 0x0000000906007386 0x000fe2000010e900 00c0 EXIT 0x000000000000794d 0x000fea0003800000 00d0 BRA 0xd0 0xfffffff000007947 0x000fc0000383ffff 00e0 NOP 0x0000000000007918 0x000fc00000000000 00f0 NOP 0x0000000000007918 0x000fc00000000000 ....................... Fatbin ptx code arch sm70 code version 7,0 producer cuda host linux compilesize 64bit compressed identifier add.cu .version 7.0 .target sm70 .addresssize 64 .visible .entry Z3addPiSS .param .u64 Z3addPiSSparam0, .param"
Miscellaneous,Floating Point and IEEE 754,https://docs.nvidia.com/cuda/floating-point/index.html,"Floating Point and IEEE 754 1. Introduction v12.5 PDF Archive Floating Point and IEEE 754 Compliance for NVIDIA GPUs White paper covering the most common issues related to NVIDIA GPUs. A number of issues related to floating point accuracy and compliance are a frequent source of confusion on both CPUs and GPUs. The purpose of this white paper is to discuss the most common issues related to NVIDIA GPUs and to supplement the documentation in the CUDA C Programming Guide. 1. Introduction Since the widespread adoption in 1985 of the IEEE Standard for Binary FloatingPoint Arithmetic IEEE 7541985 1 virtually all mainstream computing systems have implemented the standard, including NVIDIA with the CUDA architecture. IEEE 754 standardizes how arithmetic results should be approximated in floating point. Whenever working with inexact results, programming decisions can affect accuracy. It is important to consider many aspects of floating point behavior in order to achieve the highest performance with the precision required for any specific application. This is especially true in a heterogeneous computing environment where operations will be performed on different types of hardware. Understanding some of the intricacies of floating point and the specifics of how NVIDIA hardware handles floating point is obviously important to CUDA programmers striving to implement correct numerical algorithms. In addition, users of libraries such as cuBLAS and cuFFT will also find it informative to learn how NVIDIA handles floating point under the hood. We review some of the basic properties of floating point calculations in Chapter 2 . We also discuss the fused multiplyadd operator, which was added to the IEEE 754 standard in 2008 2 and is built into the hardware of NVIDIA GPUs. In Chapter 3 we work through an example of computing the dot product of two short vectors to illustrate how different choices of implementation affect the accuracy of the final result. In Chapter 4 we describe NVIDIA hardware versions and NVCC compiler options that affect floating point calculations. In Chapter 5 we consider some issues regarding the comparison of CPU and GPU results. Finally, in Chapter 6 we conclude with concrete recommendations to programmers that deal with numeric issues relating to floating point on the GPU. 2. Floating Point 2.1. Formats Floating point encodings and functionality are defined in the IEEE 754 Standard 2 last revised in 2008. Goldberg 5 gives a good introduction to floating point and many of the issues that arise. The standard mandates binary floating point data be encoded on three fields a one bit sign field, followed by exponent bits encoding the exponent offset by a numeric bias specific to each format, and bits encoding the significand or fraction. In order to ensure consistent computations across platforms and to exchange floating point data, IEEE 754 defines basic and interchange formats. The 32 and 64 bit basic binary floating point formats correspond to the C data types float and double . Their corresponding representations have the following bit lengths For numerical data representing finite values, the sign is either negative or positive, the exponent field encodes the exponent in base 2, and the fraction field encodes the significand without the most significant nonzero bit. For example, the value 192 equals 1 1 x 2 7 x 1.5, and can be represented as having a negative sign, an exponent of 7, and a fractional part .5. The exponents are biased by 127 and 1023, respectively, to allow exponents to extend from negative to positive. Hence the exponent 7 is represented by bit strings with values 134 for float and 1030 for double. The integral part of 1. is implicit in the fraction. Also, encodings to represent infinity and notanumber NaN data are reserved. The IEEE 754 Standard 2 describes floating point encodings in full. Given that the fraction field uses a limited number of bits, not all real numbers can be represented exactly. For example the mathematical value of the fraction 23 represented in binary is 0.10101010 which has an infinite number of bits after the binary point. The value 23 must be rounded first in order to be represented as a floating point number with limited precision. The rules for rounding and the rounding modes are specified in IEEE 754. The most frequently used is the roundtonearestoreven mode abbreviated as roundtonearest. The value 23 rounded in this mode is represented in binary as The sign is positive and the stored exponent value represents an exponent of 1. 2.2. Operations and Accuracy The IEEE 754 standard requires support for a handful of operations. These include the arithmetic operations add, subtract, multiply, divide, square root, fusedmultiplyadd, remainder, conversion operations, scaling, sign operations, and comparisons. The results of these operations are guaranteed to be the same for all implementations of the standard, for a given format and rounding mode. The rules and properties of mathematical arithmetic do not hold directly for floating point arithmetic because of floating points limited precision. For example, the table below shows single precision values A , B , and C , and the mathematical exact value of their sum computed using different associativity. beginmatrix A 21 times 1.00000000000000000000001 B 20 times 1.00000000000000000000001 C 23 times 1.00000000000000000000001 A B C 23 times 1.01100000000000000000001011 A B C 23 times 1.01100000000000000000001011 endmatrix Mathematically, A B C does equal A B C . Let rn x denote one rounding step on x . Performing these same computations in single precision floating point arithmetic in roundtonearest mode according to IEEE 754, we obtain beginmatrix A B 21 times 1.1000000000000000000000110000... textrnA B 21 times 1.10000000000000000000010 B C 23 times 1.0010000000000000000000100100... textrnB C 23 times 1.00100000000000000000001 A B C 23 times 1.0110000000000000000000101100... textrnleft textrnA B C right 23 times 1.01100000000000000000010 textrnleft A textrnB C right 23 times 1.01100000000000000000001 endmatrix For reference, the exact, mathematical results are computed as well in the table above. Not only are the results computed according to IEEE 754 different from the exact mathematical results, but also the results corresponding to the sum rnrnA B C and the sum"
Miscellaneous,Incomplete-LU and Cholesky Preconditioned Iterative Methods,https://docs.nvidia.com/cuda/incomplete-lu-cholesky/index.html,"IncompleteLU and Cholesky Preconditioned Iterative Methods 1. Introduction v12.5 PDF Archive IncompleteLU and Cholesky Preconditioned Iterative Methods Using cuSPARSE and cuBLAS White paper describing how to use the cuSPARSE and cuBLAS libraries to achieve a 2x speedup over CPU in the incompleteLU and Cholesky preconditioned iterative methods. 1. Introduction The solution of large sparse linear systems is an important problem in computational mechanics, atmospheric modeling, geophysics, biology, circuit simulation and many other applications in the field of computational science and engineering. In general, these linear systems can be solved using direct or preconditioned iterative methods. Although the direct methods are often more reliable, they usually have large memory requirements and do not scale well on massively parallel computer platforms. The iterative methods are more amenable to parallelism and therefore can be used to solve larger problems. Currently, the most popular iterative schemes belong to the Krylov subspace family of methods. They include BiConjugate Gradient Stabilized BiCGStab and Conjugate Gradient CG iterative methods for nonsymmetric and symmetric positive definite s.p.d. linear systems, respectively 2 , 11 . We describe these methods in more detail in the next section. In practice, we often use a variety of preconditioning techniques to improve the convergence of the iterative methods. In this white paper we focus on the incompleteLU and Cholesky preconditioning 11 , which is one of the most popular of these preconditioning techniques. It computes an incomplete factorization of the coefficient matrix and requires a solution of lower and upper triangular linear systems in every iteration of the iterative method. In order to implement the preconditioned BiCGStab and CG we use the sparse matrixvector multiplication 3 , 15 and the sparse triangular solve 8 , 16 implemented in the cuSPARSE library. We point out that the underlying implementation of these algorithms takes advantage of the CUDA parallel programming paradigm 5 , 9 , 13 , which allows us to explore the computational resources of the graphical processing unit GPU. In our numerical experiments the incomplete factorization is performed on the CPU host and the resulting lower and upper triangular factors are then transferred to the GPU device memory before starting the iterative method. However, the computation of the incomplete factorization could also be accelerated on the GPU. We point out that the parallelism available in these iterative methods depends highly on the sparsity pattern of the coefficient matrix at hand. In our numerical experiments the incompleteLU and Cholesky preconditioned iterative methods achieve on average more than 2x speedup using the cuSPARSE and cuBLAS libraries on the GPU over the MKL 17 implementation on the CPU. For example, the speedup for the preconditioned iterative methods with the incompleteLU and Cholesky factorization with 0 fillin ilu0 is shown in Figure 1 for matrices resulting from a variety of applications. It will be described in more detail in the last section. Speedup of the IncompleteLU Cholesky with 0 fillin Prec. Iterative Methods In the next sections we briefly describe the methods of interest and comment on the role played in them by the parallel sparse matrixvector multiplication and triangular solve algorithms. 2. Preconditioned Iterative Methods Let us consider the linear system Amathbfx mathbff where A in mathbbRn times n is a nonsingular coefficient matrix and mathbfx,mathbff in mathbbRn are the solution and righthandside vectors. In general, the iterative methods start with an initial guess and perform a series of steps that find more accurate approximations to the solution. There are two types of iterative methods i the stationary iterative methods, such as the splittingbased Jacobi and GaussSeidel GS, and ii the nonstationary iterative methods, such as the Krylov subspace family of methods, which includes CG and BiCGStab . As we mentioned earlier we focus on the latter in this white paper. The convergence of the iterative methods depends highly on the spectrum of the coefficient matrix and can be significantly improved using preconditioning. The preconditioning modifies the spectrum of the coefficient matrix of the linear system in order to reduce the number of iterative steps required for convergence. It often involves finding a preconditioning matrix M , such that M 1 is a good approximation of A 1 and the systems with M are relatively easy to solve. For the s.p.d. matrix A we can let M be its incompleteCholesky factorization, so that A approx M widetildeRTwidetildeR , where widetildeR is an upper triangular matrix. Let us assume that M is nonsingular, then widetildeR TAwidetildeR 1 is s.p.d. and instead of solving the linear system 1 , we can solve the preconditioned linear system left widetildeR TAwidetildeR 1 rightleft widetildeRmathbfx right widetildeR Tmathbff The pseudocode for the preconditioned CG iterative method is shown in Algorithm 1 . 2.1. Algorithm 1 Conjugate Gradient CG 1 textLetting initial guess be mathbfx0text, compute mathbfrleftarrowmathbff Amathbfx0 2 textbffor ileftarrow 1,2,...text until convergence textbfdo 3 quadquadtextSolve Mmathbfzleftarrowmathbfr vartriangleright textSparse lower and upper triangular solves 4 quadquadrhoileftarrowmathbfrTmathbfz 5 quadquadtextbfif i1textbf then 6 quadquadquadquadmathbfpleftarrowmathbfz 7 quadquadtextbfelse 8 quadquadquadquadbetaleftarrowfracrhoirhoi 1 9 quadquadquadquadmathbfpleftarrowmathbfz betamathbfp 10 quadquadtextbfend if 11 quadquadtextCompute mathbfqleftarrow Amathbfp vartriangleright textSparse matrixvector multiplication 12 quadquadalphaleftarrowfracrhoimathbfpTmathbfq 13 quadquadmathbfxleftarrowmathbfx alphamathbfp 14 quadquadmathbfrleftarrowmathbfr alphamathbfq 15 textbfend for Notice that in every iteration of the incompleteCholesky preconditioned CG iterative method we need to perform one sparse matrixvector multiplication and two triangular solves. The corresponding CG code using the cuSPARSE and cuBLAS libraries in C programming language is shown below. CG Code ASSUMPTIONS 1. The cuSPARSE and cuBLAS libraries have been initialized. 2. The appropriate memory has been allocated and set to zero. 3. The matrix A valA, csrRowPtrA, csrColIndA and the incomplete Cholesky upper triangular factor R valR, csrRowPtrR, csrColIndR have been computed and are present in the device GPU memory. create the info and analyse the lower and upper triangular factors cusparseCreateSolveAnalysisInfo inforRt cusparseCreateSolveAnalysisInfo inforR cusparseDcsrsvanalysis handle , CUSPARSEOPERATIONTRANSPOSE , n , descrR , valR , csrRowPtrR , csrColIndR , inforRt cusparseDcsrsvanalysis handle , CUSPARSEOPERATIONNONTRANSPOSE , n , descrR , valR , csrRowPtrR , csrColIndR , inforR 1 compute initial residual r f A x0 using"
Miscellaneous,CUDA for Tegra,https://docs.nvidia.com/cuda/cuda-for-tegra-appnote/index.html,"CUDA for Tegra 1. CUDA for Tegra v12.5 PDF Archive 1. CUDA for Tegra This application note provides an overview of NVIDIA Tegra memory architecture and considerations for porting code from a discrete GPU dGPU attached to an x86 system to the Tegra integrated GPU iGPU. It also discusses EGL interoperability. 2. Overview This document provides an overview of NVIDIA Tegra memory architecture and considerations for porting code from a discrete GPU dGPU attached to an x86 system to the Tegra integrated GPU iGPU. It also discusses EGL interoperability. This guide is for developers who are already familiar with programming in CUDA, and CC, and who want to develop applications for the Tegra SoC. Performance guidelines, best practices, terminology, and general information provided in the CUDA C Programming Guide and the CUDA C Best Practices Guide are applicable to all CUDAcapable GPU architectures, including Tegra devices. The CUDA C Programming Guide and the CUDA C Best Practices Guide are available at the following web sites CUDA C Programming Guide httpsdocs.nvidia.comcudacudacprogrammingguideindex.html CUDA C Best Practices Guide httpsdocs.nvidia.comcudacudacbestpracticesguideindex.html 3. Memory Management In Tegra devices, both the CPU Host and the iGPU share SoC DRAM memory. A dGPU with separate DRAM memory can be connected to the Tegra device over PCIe or NVLink. It is currently supported only on the NVIDIA DRIVE platform. An overview of a dGPUconnected Tegra memory system is shown in Figure 1 . dGPUconnected Tegra Memory System In Tegra, device memory, host memory, and unified memory are allocated on the same physical SoC DRAM. On a dGPU, device memory is allocated on the dGPU DRAM. The caching behavior in a Tegra system is different from that of an x86 system with a dGPU. The caching and accessing behavior of different memory types in a Tegra system is shown in Table 1 . Table 1. Characteristics of Different Memory Types in a Tegra System Memory Type CPU iGPU Tegraconnected dGPU Device memory Not directly accessible Cached Cached Pageable host memory Cached Not directly accessible Not directly accessible Pinned host memory Uncached where compute capability is less than 7.2. Cached where compute capability is greater than or equal to 7.2. Uncached Uncached Unified memory Cached Cached Not supported On Tegra, because device memory, host memory, and unified memory are allocated on the same physical SoC DRAM, duplicate memory allocations and data transfers can be avoided. 3.1. IO Coherency IO coherency also known as oneway coherency is a feature with which an IO device such as a GPU can read the latest updates in CPU caches. It removes the need to perform CPU cache management operations when the same physical memory is shared between CPU and GPU. The GPU cache management operations still need to be performed because the coherency is one way. Please note that the CUDA driver internally performs the GPU cache management operations when managed memory or interop memory is used. IO coherency is supported on Tegra devices starting with Xavier SOC. Applications should realize benefits from this HW feature without needing to make changes to the applications code see point 2 below. The following functionalities depend on IO coherency support cudaHostRegister cuMemHostRegister is supported only on platforms which are IO coherent. The host register support can be queried using the device attribute cudaDevAttrHostRegisterSupported CUDEVICEATTRIBUTEHOSTREGISTERSUPPORTED. CPU cache for pinned memory allocated using cudaMallocHost cuMemHostAlloc cuMemAllocHost is enabled only on platforms which are IO coherent. 3.2. Estimating Total Allocatable Device Memory on an Integrated GPU Device The cudaMemGetInfo API returns the snapshot of free and total amount of memory available for allocation for the GPU. The free memory could change if any other client allocate memory. The discrete GPU has the dedicated DRAM called VIDMEM which is separate from CPU memory. The snapshot of free memory in discrete GPU is returned by the cudaMemGetInfo API. The integrated GPU, on Tegra SoC, shares the DRAM with CPU and other the Tegra engines. The CPU can control the contents of DRAM and free DRAM memory by moving the contents of DMAR to SWAP area or vice versa. The cudaMemGetInfo API currently does not account for SWAP memory area. The cudaMemGetInfo API may return a smaller size than the actually allocatable memory since the CPU may be able to free up some DRAM region by moving pages to the SWAP area. In order to estimate the amount of allocatable device memory, CUDA application developers should consider following On Linux and Android platforms Device allocatable memory on Linux and Android depends mainly on the total and free sizes of swap space and main memory. The following points can help users to estimate the total amount of device allocatable memory in various situations Host allocated memory Total used physical memory Device allocated memory If Host allocated memory Free Swap Space then Device allocatable memory Total Physical Memory already allocated device memory If Host allocated memory Free Swap Space then Device allocatable memory Total Physical Memory Host allocated memory Free swap space Here, Device allocated memory is memory already allocated on the device. It can be obtained from the NvMapMemUsed field in procmeminfo or from the total field of syskerneldebugnvmapiovmmclients . Total used physical memory can be obtained using the free m command. The used field in row Mem represents this information. Total Physical memory is obtained from the MemTotal field in procmeminfo . Free swap space can be find by using the free m command. The free field in the Swap row represents this information. If the free command is not available, the same information can be obtained from procmeminfo as Total Used physical memory MemTotal MemFree Free swap space SwapFree On QNX platforms QNX does not use swap space, hence, cudaMemGetInfo.free will be a fair estimate of allocatable device memory as there is no swap space to move memory pages to swap area. 4. Porting Considerations CUDA applications originally developed for dGPUs attached to x86 systems may require modifications to perform efficiently on Tegra systems. This section describes the considerations for porting such applications"
Miscellaneous,libNVVM API,https://docs.nvidia.com/cuda/libnvvm-api/index.html,"libNVVM API v4.0 1. libNVVM API v12.5 Archive libNVVM API libNVVM API v4.0 Reference Manual 1. libNVVM API 1.1. Introduction libNVVM API provides an interface for generating PTX code from both binary and text NVVM IR inputs. Compatible input can be generated by tools and libraries that produce LLVM 7.0 IR and bitcode. Support for reading the text NVVM IR representation is deprecated and may be removed in a later release. 1.2. Thread Safety libNVVM API provides a threadsafe interface to libNVVM. Clients can take advantage of improved compilation speeds by spawning multiple compilation threads concurrently. 1.3. Module This chapter presents the API of the libNVVM library. Here is a list of all modules Error Handling General Information Query Compilation 2. Error Handling Enumerations nvvmResult NVVM API call result code. Functions const char nvvmGetErrorString nvvmResult result Get the message string for the given nvvmResult code. 2.1. Enumerations enum nvvmResult NVVM API call result code. Values enumerator NVVMSUCCESS enumerator NVVMERROROUTOFMEMORY enumerator NVVMERRORPROGRAMCREATIONFAILURE enumerator NVVMERRORIRVERSIONMISMATCH enumerator NVVMERRORINVALIDINPUT enumerator NVVMERRORINVALIDPROGRAM enumerator NVVMERRORINVALIDIR enumerator NVVMERRORINVALIDOPTION enumerator NVVMERRORNOMODULEINPROGRAM enumerator NVVMERRORCOMPILATION 2.2. Functions const char nvvmGetErrorString nvvmResult result Get the message string for the given nvvmResult code. Parameters result in NVVM API result code. Returns Message string for the given nvvmResult code. 3. General Information Query Functions nvvmResult nvvmIRVersion int majorIR, int minorIR, int majorDbg, int minorDbg Get the NVVM IR version. nvvmResult nvvmVersion int major, int minor Get the NVVM version. 3.1. Functions nvvmResult nvvmIRVersion int majorIR , int minorIR , int majorDbg , int minorDbg Get the NVVM IR version. Parameters majorIR out NVVM IR major version number. minorIR out NVVM IR minor version number. majorDbg out NVVM IR debug metadata major version number. minorDbg out NVVM IR debug metadata minor version number. Returns NVVMSUCCESS nvvmResult nvvmVersion int major , int minor Get the NVVM version. Parameters major out NVVM major version number. minor out NVVM minor version number. Returns NVVMSUCCESS 4. Compilation Functions nvvmResult nvvmAddModuleToProgram nvvmProgram prog, const char buffer, sizet size, const char name Add a module level NVVM IR to a program. nvvmResult nvvmCompileProgram nvvmProgram prog, int numOptions, const char options Compile the NVVM program. nvvmResult nvvmCreateProgram nvvmProgram prog Create a program, and set the value of its handle to prog . nvvmResult nvvmDestroyProgram nvvmProgram prog Destroy a program. nvvmResult nvvmGetCompiledResult nvvmProgram prog, char buffer Get the compiled result. nvvmResult nvvmGetCompiledResultSize nvvmProgram prog, sizet bufferSizeRet Get the size of the compiled result. nvvmResult nvvmGetProgramLog nvvmProgram prog, char buffer Get the CompilerVerifier Message. nvvmResult nvvmGetProgramLogSize nvvmProgram prog, sizet bufferSizeRet Get the Size of CompilerVerifier Message. nvvmResult nvvmLazyAddModuleToProgram nvvmProgram prog, const char buffer, sizet size, const char name Add a module level NVVM IR to a program. nvvmResult nvvmVerifyProgram nvvmProgram prog, int numOptions, const char options Verify the NVVM program. Typedefs nvvmProgram NVVM Program. 4.1. Functions nvvmResult nvvmAddModuleToProgram nvvmProgram prog , const char buffer , sizet size , const char name Add a module level NVVM IR to a program. The buffer should contain an NVVM IR module. The module should have NVVM IR either in the LLVM 7.0.1 bitcode representation or in the LLVM 7.0.1 text representation. Support for reading the text representation of NVVM IR is deprecated and may be removed in a later version. Parameters prog in NVVM program. buffer in NVVM IR module in the bitcode or text representation. size in Size of the NVVM IR module. name in Name of the NVVM IR module. If NULL, unnamed is used as the name. Returns NVVMSUCCESS NVVMERROROUTOFMEMORY NVVMERRORINVALIDINPUT NVVMERRORINVALIDPROGRAM nvvmResult nvvmCompileProgram nvvmProgram prog , int numOptions , const char options Compile the NVVM program. The NVVM IR modules in the program will be linked at the IR level. The linked IR program is compiled to PTX. The target datalayout in the linked IR program is used to determine the address size 32bit vs 64bit. The valid compiler options are g enable generation of full debugging information. Full debug support is only valid with opt0. Debug support requires the input module to utilize NVVM IR Debug Metadata. Line number line info only generation is also enabled via NVVM IR Debug Metadata, there is no specific libNVVM API flag for that case. opt 0 disable optimizations 3 default, enable optimizations arch compute50 compute52 default compute53 compute60 compute61 compute62 compute70 compute72 compute75 compute80 compute87 compute89 compute90 ftz 0 default, preserve denormal values, when performing singleprecision floatingpoint operations 1 flush denormal values to zero, when performing singleprecision floatingpoint operations precsqrt 0 use a faster approximation for singleprecision floatingpoint square root 1 default, use IEEE roundtonearest mode for singleprecision floatingpoint square root precdiv 0 use a faster approximation for singleprecision floatingpoint division and reciprocals 1 default, use IEEE roundtonearest mode for singleprecision floatingpoint division and reciprocals fma 0 disable FMA contraction 1 default, enable FMA contraction jumptabledensity0101 Specify the case density percentage in switch statements, and use it as a minimal threshold to determine whether jump tablebrx.idx instruction will be used to implement a switch statement. Default value is 101. The percentage ranges from 0 to 101 inclusively. genlto Generate LTO IR instead of PTX. Parameters prog in NVVM program. numOptions in Number of compiler options passed. options in Compiler options in the form of C string array. Returns NVVMSUCCESS NVVMERROROUTOFMEMORY NVVMERRORIRVERSIONMISMATCH NVVMERRORINVALIDPROGRAM NVVMERRORINVALIDOPTION NVVMERRORNOMODULEINPROGRAM NVVMERRORCOMPILATION nvvmResult nvvmCreateProgram nvvmProgram prog Create a program, and set the value of its handle to prog . See also nvvmDestroyProgram Parameters prog in NVVM program. Returns NVVMSUCCESS NVVMERROROUTOFMEMORY NVVMERRORINVALIDPROGRAM nvvmResult nvvmDestroyProgram nvvmProgram prog Destroy a program. See also nvvmCreateProgram Parameters prog in NVVM program. Returns NVVMSUCCESS NVVMERRORINVALIDPROGRAM nvvmResult nvvmGetCompiledResult nvvmProgram prog , char buffer Get the compiled result. The result is stored in the memory pointed to by buffer . Parameters prog in NVVM program. buffer out Compiled result. Returns NVVMSUCCESS NVVMERRORINVALIDPROGRAM nvvmResult nvvmGetCompiledResultSize nvvmProgram prog , sizet bufferSizeRet Get the size of the compiled result. Parameters prog in NVVM program. bufferSizeRet out Size of the compiled result including the trailing NULL. Returns NVVMSUCCESS NVVMERRORINVALIDPROGRAM nvvmResult nvvmGetProgramLog nvvmProgram prog , char buffer Get the CompilerVerifier Message. The NULL terminated"
Miscellaneous,libdevice Userâs Guide,https://docs.nvidia.com/cuda/libdevice-users-guide/index.html,Users guide to libdevice
Miscellaneous,NVVM IR,https://docs.nvidia.com/cuda/nvvm-ir-spec/index.html,"NVVM IR Specification 1. Introduction v12.5 PDF Archive NVVM IR Specification Reference guide to the NVVM compiler intermediate representation based on the LLVM IR. 1. Introduction NVVM IR is a compiler IR intermediate representation based on the LLVM IR. The NVVM IR is designed to represent GPU compute kernels for example, CUDA kernels. Highlevel language frontends, like the CUDA C compiler frontend, can generate NVVM IR. The NVVM compiler which is based on LLVM generates PTX code from NVVM IR. NVVM IR and NVVM compilers are mostly agnostic about the source language being used. The PTX codegen part of a NVVM compiler needs to know the source language because of the difference in DCI drivercompiler interface. NVVM IR is a binary format and is based on a subset of LLVM IR bitcode format. This document uses only humanreadable form to describe NVVM IR. Technically speaking, NVVM IR is LLVM IR with a set of rules, restrictions, and conventions, plus a set of supported intrinsic functions. A program specified in NVVM IR is always a legal LLVM program. A legal LLVM program may not be a legal NVVM program. There are three levels of support for NVVM IR. Supported The feature is fully supported. Most IR features should fall into this category. Accepted and ignored The NVVM compiler will accept this IR feature, but will ignore the required semantics. This applies to some IR features that do not have meaningful semantics on GPUs and that can be ignored. Calling convention markings are an example. Illegal, not supported The specified semantics is not supported, such as a fence instruction. Future versions of NVVM may either support or accept and ignore IRs that are illegal in the current version. This document describes version 2.0 of the NVVM IR and version 3.1 of the NVVM debug metadata see Source Level Debugging Support . The 2.0 version of NVVM IR is incompatible with the previous version 1.11. Linking of NVVM IR Version 1.11 with 2.0 will result in compiler error. The current NVVM IR is based on LLVM 7.0.1. For the complete semantics of the IR, readers of this document should check the official LLVM Language Reference Manual httpsreleases.llvm.org7.0.1docsLangRef.html . 2. Identifiers The name of a named global identifier must have the form azAZazAZ09 Note that it cannot contain the . character. llvm.nvvm. and nvvm. are reserved words. 3. High Level Structure 3.1. Linkage Types Supported private internal availableexternally linkonce weak common linkonceodr weakodr external Not supported appending externweak See NVVM ABI for PTX for details on how linkage types are translated to PTX. 3.2. Calling Conventions All LLVM calling convention markings are accepted and ignored. Functions and calls are generated according to the PTX calling convention. 3.2.1. Rules and Restrictions When an argument with width less than 32bit is passed, the zeroextsignext parameter attribute should be set. zeroext will be assumed if not set. When a value with width less than 32bit is returned, the zeroextsignext parameter attribute should be set. zeroext will be assumed if not set. Arguments of aggregate or vector types that are passed by value can be passed by pointer with the byval attribute set referred to as the bypointerbyval case below. The align attribute must be set if the type requires a nonnatural alignment natural alignment is the alignment inferred for the aggregate type according to the Data Layout section. If a function has an argument of aggregate or vector type that is passed by value directly and the type has a nonnatural alignment requirement, the alignment must be annotated by the global property annotation align , alignment, where alignment is a 32bit integer whose upper 16 bits represent the argument position starting from 1 and the lower 16 bits represent the alignment. If the return type of a function is an aggregate or a vector that has a nonnatural alignment, then the alignment requirement must be annotated by the global property annotation align , alignment, where the upper 16 bits is 0, and the lower 16 bits represent the alignment. It is not required to annotate a function with align , alignment otherwise. If annotated, the alignment must match the natural alignment or the align attribute in the bypointerbyval case. For an indirect call instruction of a function that has a nonnatural alignment for its return value or one of its arguments that is not expressed in alignment in the bypointerbyval case, the call instruction must have an attached metadata of kind callalign . The metadata contains a sequence of i32 fields each of which represents a nonnatural alignment requirement. The upper 16 bits of an i32 field represent the argument position 0 for return value, 1 for the first argument, and so on and the lower 16 bits represent the alignment. The i32 fields must be sorted in the increasing order. For example, call call struct . S fp1 struct . S byval align 8 arg1p , struct . S arg2 , ! callalign ! 10 ! 10 ! i32 8 , i32 520 It is not required to have an i32 metadata field for the other arguments or the return value otherwise. If presented, the alignment must match the natural alignment or the align attribute in the bypointerbyval case . It is not required to have a callalign metadata attached to a direct call instruction. If attached, the alignment must match the natural alignment or the alignment in the bypointerbyval case. The absence of the metadata in an indirect call instruction means using natural alignment or the align attribute in the bypointerbyval case. 3.3. Visibility Styles All stylesdefault, hidden, and protectedare accepted and ignored. 3.4. DLL Storage Classes Not supported. 3.5. Thread Local Storage Models Not supported. 3.6. Runtime Preemption Specifiers Not supported. 3.7. Structure Types Fully supported. 3.8. NonIntegral Pointer Type Not supported. 3.9. Comdats Not supported. 3.10. sourcefilename Accepted and ignored. 3.11. Global Variables A global variable, that is not an intrinsic global variable, may be optionally declared to reside in one of the"
